{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7b. Zero_NYC_TaxiFare_XGBoost_GP_EI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wHutsqZUcn"
      },
      "source": [
        "XGBoost Regression - 'real-world' example: NYC Taxi-Fare Predictor\n",
        "\n",
        "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7PwmXsgZO8D",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a74e4c91-e9eb-4896-f080-9bff767d68a6"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1d98ebab-d598-49c9-a0d2-fc4b477a69b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1d98ebab-d598-49c9-a0d2-fc4b477a69b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"conorc2006\",\"key\":\"c5c5a6382a7d50c022aab991694fc17f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwbJ6hjZltI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d632afde-57cc-4f33-e196-47b70d47b487"
      },
      "source": [
        "## Ensure the kaggle.json file is present:\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Aug 19 10:09 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Pu-UlWZovH"
      },
      "source": [
        "## Next, install the Kaggle API client:\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOQ4SE7Zuj3"
      },
      "source": [
        "## The Kaggle API Client expects this file to be ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcEztjCZxOn"
      },
      "source": [
        "## Permissions' change\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-u4Tmj7ZUD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd3ea71-5980-438c-f2ad-b1fc2f17755f"
      },
      "source": [
        "!kaggle competitions download -c new-york-city-taxi-fare-prediction"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading GCP-Coupons-Instructions.rtf to /content\n",
            "  0% 0.00/486 [00:00<?, ?B/s]\n",
            "100% 486/486 [00:00<00:00, 756kB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/960k [00:00<?, ?B/s]\n",
            "100% 960k/960k [00:00<00:00, 63.4MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/335k [00:00<?, ?B/s]\n",
            "100% 335k/335k [00:00<00:00, 107MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 99% 1.55G/1.56G [00:13<00:00, 106MB/s]\n",
            "100% 1.56G/1.56G [00:13<00:00, 121MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-0Pe1i4Z2R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2fd5ee6-f8a9-4329-ae17-ecbe7ba760ba"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading pyGPGO-0.5.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (2019.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: Theano-PyMC in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.1.2)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.11.2)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl->pyGPGO) (2021.3.0)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.13.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: fastprogress>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: cachetools>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.2.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (3.7.4.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.3.4)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: arviz>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from Theano-PyMC->pyGPGO) (3.0.12)\n",
            "Requirement already satisfied: setuptools>=38.4 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (21.0)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (3.2.2)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (1.5.7)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (0.18.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.7/dist-packages (from netcdf4->arviz>=0.11.0->pyMC3->pyGPGO) (1.5.0)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.5.1-py3-none-any.whl size=19880 sha256=613ba34f026e4f98bcb3cc30cd02dba09f38c95a3f9f7c9d4c7f100b17899ae1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/5d/0b/2160114e2f1b87791c51b66cf07f89831dbb6f49167950316f\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zDTf1naBsH"
      },
      "source": [
        "# Load some default Python modules:\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import time\n",
        "\n",
        "from matplotlib.pyplot import rc\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
        "rc('text', usetex=False)\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "from collections import OrderedDict\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.special import gamma\n",
        "from scipy.stats import norm, t\n",
        "from joblib import Parallel, delayed\n",
        "import itertools\n",
        "\n",
        "from pyGPGO.logger import EventLogger\n",
        "from pyGPGO.GPGO import GPGO\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
        "from pyGPGO.acquisition import Acquisition\n",
        "from pyGPGO.covfunc import squaredExponential\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from pandas_datareader import data\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXicekJhaE0P"
      },
      "source": [
        "# Read data in pandas dataframe:\n",
        "\n",
        "df_train =  pd.read_csv('/content/train.csv.zip', nrows = 1_000_000, parse_dates=[\"pickup_datetime\"])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0mDzt_cBmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5015cbd8-7f45-4c43-ee50-43f4070690b5"
      },
      "source": [
        "# List first rows:\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-06-15 17:26:21.0000001</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2009-06-15 17:26:21+00:00</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-05 16:52:16.0000002</td>\n",
              "      <td>16.9</td>\n",
              "      <td>2010-01-05 16:52:16+00:00</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-08-18 00:35:00.00000049</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2011-08-18 00:35:00+00:00</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-04-21 04:30:42.0000001</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2012-04-21 04:30:42+00:00</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-03-09 07:51:00.000000135</td>\n",
              "      <td>5.3</td>\n",
              "      <td>2010-03-09 07:51:00+00:00</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             key  ...  passenger_count\n",
              "0    2009-06-15 17:26:21.0000001  ...                1\n",
              "1    2010-01-05 16:52:16.0000002  ...                1\n",
              "2   2011-08-18 00:35:00.00000049  ...                2\n",
              "3    2012-04-21 04:30:42.0000001  ...                1\n",
              "4  2010-03-09 07:51:00.000000135  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9fZujMycFMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9388e433-d271-4846-9821-60b5ab6efe2b"
      },
      "source": [
        "# Format 'pickup_datetime' variable:\n",
        "\n",
        "df_train['pickup_datetime'] =  pd.to_datetime(df_train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
        "df_train['pickup_datetime'].head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   2009-06-15 17:26:21+00:00\n",
              "1   2010-01-05 16:52:16+00:00\n",
              "2   2011-08-18 00:35:00+00:00\n",
              "3   2012-04-21 04:30:42+00:00\n",
              "4   2010-03-09 07:51:00+00:00\n",
              "Name: pickup_datetime, dtype: datetime64[ns, UTC]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nReKu62HcVFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "014759e9-852b-4766-a348-bb1403f42ba4"
      },
      "source": [
        "df_train.sort_values(by = 'pickup_datetime').tail() ### June 2015 the final month\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>286276</th>\n",
              "      <td>2015-06-30 23:38:21.0000003</td>\n",
              "      <td>26.5</td>\n",
              "      <td>2015-06-30 23:38:21+00:00</td>\n",
              "      <td>-74.008385</td>\n",
              "      <td>40.711571</td>\n",
              "      <td>-73.884071</td>\n",
              "      <td>40.737385</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955575</th>\n",
              "      <td>2015-06-30 23:45:57.0000003</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015-06-30 23:45:57+00:00</td>\n",
              "      <td>-74.002342</td>\n",
              "      <td>40.739819</td>\n",
              "      <td>-74.005829</td>\n",
              "      <td>40.745239</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915826</th>\n",
              "      <td>2015-06-30 23:48:35.0000005</td>\n",
              "      <td>30.5</td>\n",
              "      <td>2015-06-30 23:48:35+00:00</td>\n",
              "      <td>-73.983826</td>\n",
              "      <td>40.729546</td>\n",
              "      <td>-73.927917</td>\n",
              "      <td>40.661186</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751350</th>\n",
              "      <td>2015-06-30 23:53:23.0000002</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2015-06-30 23:53:23+00:00</td>\n",
              "      <td>-73.978020</td>\n",
              "      <td>40.757439</td>\n",
              "      <td>-73.980705</td>\n",
              "      <td>40.753544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785182</th>\n",
              "      <td>2015-06-30 23:53:49.0000003</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2015-06-30 23:53:49+00:00</td>\n",
              "      <td>-73.959969</td>\n",
              "      <td>40.762405</td>\n",
              "      <td>-73.953064</td>\n",
              "      <td>40.782688</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                key  ...  passenger_count\n",
              "286276  2015-06-30 23:38:21.0000003  ...                5\n",
              "955575  2015-06-30 23:45:57.0000003  ...                1\n",
              "915826  2015-06-30 23:48:35.0000005  ...                2\n",
              "751350  2015-06-30 23:53:23.0000002  ...                1\n",
              "785182  2015-06-30 23:53:49.0000003  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9j9LnIfcXcX"
      },
      "source": [
        "# Add time variables:\n",
        "\n",
        "df_train['hour'] = df_train['pickup_datetime'].dt.hour\n",
        "df_train['weekday'] = df_train['pickup_datetime'].dt.weekday\n",
        "df_train['month'] = df_train['pickup_datetime'].dt.month\n",
        "df_train['year'] = df_train['pickup_datetime'].dt.year\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVyFZIVIcaj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2ddaf8a4-b560-43ff-dc2e-68bba13c28a9"
      },
      "source": [
        "df_train = df_train.drop(['pickup_datetime','key'], axis = 1)\n",
        "df_train.head()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.5</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.9</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.7</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.7</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.3</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "0          4.5        -73.844311        40.721319  ...        0      6  2009\n",
              "1         16.9        -74.016048        40.711303  ...        1      1  2010\n",
              "2          5.7        -73.982738        40.761270  ...        3      8  2011\n",
              "3          7.7        -73.987130        40.733143  ...        5      4  2012\n",
              "4          5.3        -73.968095        40.768008  ...        1      3  2010\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfm-KSqcdVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ff5329-4e62-4d2d-bef1-2ebc1ba7023e"
      },
      "source": [
        "# Remove negative fares and postive outliers:\n",
        "\n",
        "df_train = df_train[df_train.fare_amount>=0]\n",
        "df_train = df_train[df_train.fare_amount<=60]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTVDAD2KchTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1942b38-82d3-44b0-9551-ec9975006c17"
      },
      "source": [
        "# Remove missing data:\n",
        "\n",
        "df_train = df_train.dropna(how = 'any', axis = 'rows')\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYksJ2cclVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f122ff-7132-48f6-803c-1b37e54f7c3b"
      },
      "source": [
        "# June 2015 NYC taxi data (Wu et al, 2017):\n",
        "\n",
        "df_train = df_train[df_train.month==6]\n",
        "df_train = df_train[df_train.year==2015]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 11269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgSHPyYcnuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "e4b9e4f9-297e-42d0-8405-d73a7a9d45bf"
      },
      "source": [
        "# Histogram fare plot:\n",
        "\n",
        "df_train[df_train.fare_amount<15].fare_amount.hist(bins=100, figsize=(16,5), color = \"red\")\n",
        "plt.xlabel('$ US Dollars', weight = 'bold', family = 'Arial')\n",
        "plt.title('June 2015 Fares', weight = 'bold', family = 'Arial')\n",
        "plt.grid(b=None)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA58AAAFJCAYAAAAc6ZlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZRVZaEG8GcQRsJGEWJIWEmmNnoFkQ8rSExArtGXqIg2AeklM4PSLgVIZpZWal4hkNTKrkZZ1GjJNRPS1KwmKvASlIloH4QIMzqIAZM6zv2j5ay8IgPE5szg77fWrHXOPnuf99l7nRl5fPfep6y5ubk5AAAAUKAOpQ4AAADA3k/5BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTgDZjxIgRqaqqyl133VWS8desWZOPfvSjGTFiRPr165dhw4blU5/6VJ566qmWdZ5//vnMnTs3xx9/fPr27ZuTTz459913X8vr69evzznnnJM3velNqaqqSlVV1UvGeWE///nn3HPPfdlcM2bMeMn6VVVVufHGG3fr/gNAkTqWOgAAtBWPPfZY7rnnnrz5zW/Om9/85tx555357ne/m40bN2bu3LlJkq997Wu55ppr0rt377zzne/MHXfckfPOOy+33XZbDj/88DQ0NOTPf/5z+vbtm5///OcvO9Z+++2X0047reX54Ycf3mq+I488Mscee2zL83/7t3/bpf189tln06lTp13aFgB2lfIJQJs0YcKE/OpXv8oXvvCFnHrqqVmyZEkmTpyY3r175yc/+Un++te/ZuTIkUmSz33uc7nmmmuyefPmnHLKKZk5c2bL+9TU1OQb3/hG1qxZkx49euTUU0/NBz7wgXTs+NL/BL7+9a/P3XffncrKyiTJsccemwsvvLClRD733HO54YYbkiRz5sxJ3759c9BBB+Xaa6/NDTfckMsvvzxHHHFEFi9enAcffHC75bNr16755Cc/uVPH5Nhjj33JNhs2bMjHPvaxrF69On/7299SUVGR4447LhdffHH233//Fx2nSy65JPPmzcshhxyS+fPnZ9WqVbnqqquyYsWKNDc3t+xvr1690tzcnFmzZuW2227LE088kQMOOCBHHHFErrrqqhx44IE7lRsAEuUTgL3ANddck8GDB+eHP/xhbrrppgwfPjxDhgzJd77znXz605/OQQcdlLe//e1ZuXJlZs2aleeeey5Tpkx5yfv07NnzRc+fffbZJMlrX/vaJMm6deuycePGdOjQIUcddVSSpG/fvkmSBx98cKcyr1+/PgMGDMi+++6bAQMGZNq0aTnkkEO2u82vf/3rfO5zn2t5Pn78+Dz//PNpbGzMiBEjsu+++6a2tjb/8z//ky5duuSzn/3si7afPXt2Ro4cmR49eqSuri7jx4/Pli1bcsIJJ6RDhw5ZtGhRVq9endtuuy2/+c1vcv3116d3794ZO3ZsGhoasnTp0mzevFn5BGCXKJ8AtHtz5szJ0Ucfnccffzy//vWv8/vf/z5DhgzJ/PnzkyRHH310Xv3qV6eqqiqrVq3Kt7/97W2Wz3/26KOPZtasWenQoUM+8YlPJEmeeOKJJEnnzp1TVlaWJOnSpUuSpL6+fofzdu3aNX379k3Xrl1TW1ubn/zkJ3n44Yfzwx/+MPvuu+/Lbvfggw++qOSeeOKJefOb35xLL700P//5z/Pkk0/msMMOy5/+9KcsWbLkJdvPnj07Q4YMSfKP04efeuqpHHrooTnooIOSJN26dcujjz6aX/7yl2lubk6SHHzwwRk9enQOO+ywdOvWrWU5AOws5ROAduH5559/2ddeuPaxoqIiSbJly5Ykydq1a5MkixYtetH69fX12bx5c/bbb79tvt9vf/vbfPCDH8ymTZvy+c9/PsOHD0+SdO/ePUnS2NiY559/Ph06dGgZ6zWvec0O78stt9zSUl43bdqUYcOGZc2aNfn973+fAQMGvOx2EydOfMlpt7fffnumTp36knWffPLJlywbNGhQy+MXjs0jjzySRx555EXr/eUvf8l73/veVFdX57bbbsvEiROT/GOW99prr205LRkAdoa73QLQJr3qVa9Kkvztb39Lkqxatepl133h+s0XCt0LevfunST58pe/nIceeqjl56677nrZ4vnzn/8873//+7Nly5bMmTMnp5xySstrBx10ULp27Zrnn38+K1euTJKsWLEiSXLEEUfs0H41NDRk06ZN23ytQ4ed/8/yHXfckSQ544wzsmLFisyaNStJtjlDWV5e3vL4hWMzatSoFx2bn/3sZxk7dmyamppy8cUX5ze/+U1+/OMfZ8yYMVm5cmW+973v7XRGAEjMfALQRh155JG57777cuONN2bdunW7VHre97735TOf+UymTZuWUaNGtZTG7t27t5yS+88efvjhnHvuuXn22WczYMCALFmypOX01cmTJ6dr1645++yzM2vWrFxwwQUZPHhwfvSjH2WfffbJpEmTkvxjxvHKK6/Mxo0bW953xowZSZLLL788q1atyoc+9KG85S1vSY8ePVJbW5vGxsYcdthhOfLII3d6H1+Ycf3pT3+aSy65JD/96U93aLt3v/vduf766/PjH/84kyZNSu/evfOXv/wlv/71r7No0aKsXbs2F154YY455pgccMABWbZsWZJk//333+mMAJAonwC0IU1NTUn+MZN59tln53e/+12WLl2aJUuW5Kyzzmr5upMd9d73vjedOnXKt771rSxatCjl5eU5/PDDM3bs2G2u/+STT7bcZOiBBx7IAw880PLa+9///nTt2jXnnHNOGhsbc8stt+SOO+7IG97whnzsYx/LG9/4xiT/OOX3+9///ove94Xnl19+efr06ZMRI0Zk6dKl+dnPfpYDDzwwJ598cj72sY+9aGZyR02ePDl/+tOf8r//+7/53e9+l3PPPTeXXXZZq9v17Nkz8+fPz+zZs/Pb3/42S5cuzUEHHZTq6uoceOCBee6559KnT5/U1tbm6aefTteuXXPmmWfmjDPO2OmMAJAkZc3uHABAG7BmzZqcdNJJaWpqyu23375D33sJALQfrvkEoOSuueaajBkzJk1NTTnqqKNy6KGHljoSALCbOe0WgJJbu3ZtXvWqV2XYsGGZNm3aLt14BwBo25x2CwAAQOH8r2UAAAAKp3wCAABQuD1+zefSpUv39JAAAADsIYMGDdrm8pLccOjlwgAAANB+bW+y0Wm3AAAAFE75BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHAdSx0AoN0qK9v+683NeyYHAEA7YOYTAACAwimfAAAAFE75BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwnUsdQAAdlFZWevrNDcXnwMAYAeY+QQAAKBwZj4BaL/M/gJAu2HmEwAAgMKZ+QTar9Zmvcx4AQC0Ga2WzyVLluT888/P4YcfniR54xvfmA984AOZNm1ampqa0qNHj3zxi19MeXl5Fi5cmJtuuikdOnTIuHHjcvrppxe+AwAAALR9OzTz+aY3vSlz5sxpeX7hhRemuro6o0ePztVXX52ampqMGTMm8+bNS01NTTp16pSxY8dm1KhR6dq1a2HhAQAAaB926ZrPJUuWZOTIkUmS4cOHp7a2NsuXL0+/fv1SUVGRzp07Z+DAgVm2bNluDQsAAED7tEMzn6tXr86HPvShPPXUU5kyZUq2bt2a8vLyJEn37t1TV1eX+vr6dOvWrWWbbt26pa6urpjUAAAAtCutls/Xv/71mTJlSkaPHp01a9Zk4sSJaWpqanm9+WVu6PFyywEAAHjlafW02549e+Yd73hHysrKcvDBB+c1r3lNnnrqqTQ2NiZJ1q9fn8rKylRWVqa+vr5luw0bNqSysrK45AAAALQbrZbPhQsX5oYbbkiS1NXV5Yknnsipp56aRYsWJUkWL16cYcOGpX///lmxYkU2bdqUzZs3Z9myZRk8eHCx6QEAAGgXWj3tdsSIEfn4xz+eu+++O88++2wuueSSHHnkkZk+fXoWLFiQXr16ZcyYMenUqVOmTp2aSZMmpaysLJMnT05FRcWe2AcAAADauLLmPXxx5tKlSzNo0KA9OSSwtyor2/7rRf95a+vj74kMpdYejkGpPycAsAdtr+/t0letAAAAwM5QPgEAACjcDn3PJ8BLOJUQAICdYOYTAACAwimfAAAAFE75BAAAoHCu+QRg17n2FwDYQWY+AQAAKJzyCQAAQOGUTwAAAAqnfAIAAFA45RMAAIDCKZ8AAAAUTvkEAACgcL7nE9qi1r47MfH9iQAAtCtmPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHDKJwAAAIXrWOoAAECByspaX6e5ufgcALzimfkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOGUTwAAAAqnfAIAAFA45RMAAIDC7VD5bGxszIknnphbb70169aty4QJE1JdXZ3zzz8/zzzzTJJk4cKFOe2003L66afne9/7XqGhAQAAaF92qHxee+21OeCAA5Ikc+bMSXV1dW6++eb06dMnNTU12bJlS+bNm5cbb7wx8+fPz0033ZSNGzcWGhwAAID2o9Xy+cgjj2T16tU54YQTkiRLlizJyJEjkyTDhw9PbW1tli9fnn79+qWioiKdO3fOwIEDs2zZskKDAwAA0H60Wj6vuOKKzJgxo+X51q1bU15eniTp3r176urqUl9fn27durWs061bt9TV1RUQFwBod8rKtv8DwCvCdsvnD37wgxxzzDF53etet83Xm5ubd2o5AAAAr0wdt/fivffemzVr1uTee+/N448/nvLy8nTp0iWNjY3p3Llz1q9fn8rKylRWVqa+vr5luw0bNuSYY44pPDwAAADtw3bL5+zZs1sez507N717984DDzyQRYsW5eSTT87ixYszbNiw9O/fPxdddFE2bdqUffbZJ8uWLcvMmTMLDw8AAED7sN3yuS0f+chHMn369CxYsCC9evXKmDFj0qlTp0ydOjWTJk1KWVlZJk+enIqKiiLyAgAA0A6VNe/hCzSXLl2aQYMG7ckhof3ZkRtwlPra6tYy7ol8pc7Q1sdvCxlKPf6eyNAax6j0xwCAPWZ7fW+nZz7hFcE/lAAAYLdq9atWAAAA4F+lfAIAAFA45RMAAIDCKZ8AAAAUTvkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOGUTwAAAAqnfAIAAFA45RMAAIDCKZ8AAAAUTvkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOE6ljoAAEChysq2/3pz857JAfAKZ+YTAACAwimfAAAAFE75BAAAoHDKJwAAAIVzwyEAgKK56RGAmU8AAACKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhOra2wtatWzNjxow88cQT+fvf/54Pf/jDOeKIIzJt2rQ0NTWlR48e+eIXv5jy8vIsXLgwN910Uzp06JBx48bl9NNP3xP7AAAAQBvXavm855570rdv35xzzjlZu3Zt/uM//iMDBw5MdXV1Ro8enauvvjo1NTUZM2ZM5s2bl5qamnTq1Cljx47NqFGj0rVr1z2xHwAAALRhrZ52+453vCPnnHNOkmTdunXp2bNnlixZkpEjRyZJhg8fntra2ixfvjz9+vVLRUVFOnfunIEDB2bZsmXFpgcAoHVlZdv/AdgDWp35fMGZZ56Zxx9/PNddd13OPvvslJeXJ0m6d++eurq61NfXp1u3bi3rd+vWLXV1dbs/MQAAAO3ODpfP73znO3nwwQfziU98Is3NzS3L//nxP3u55QAAALzytHra7cqVK7Nu3bokyZFHHpmmpqbst99+aWxsTJKsX78+lZWVqaysTH19fct2GzZsSGVlZUGxAQAAaE9aLZ+/+c1v8vWvfz1JUl9fny1btmTo0KFZtGhRkmTx4sUZNmxY+vfvnxUrVmTTpk3ZvHlzli1blsGDBxebHgAAgHah1dNuzzzzzHzyk59MdXV1Ghsbc/HFF6dv376ZPn16FixYkF69emXMmDHp1KlTpk6dmkmTJqWsrCyTJ09ORUXFntgHAAAA2riy5j18cebSpUszaNCgPTkk7LzW7vxX9K/Njtx5sNTXVZf6GLWFDG19/LaQodTj74kMrXGM2v4x8Peq9L8nwF5je32v1dNuAQAA4F+lfAIAAFA45RMAAIDCKZ8AAAAUrtW73QIAwF7PTZmgcGY+AQAAKJzyCQAAQOGUTwAAAArnmk8AAErPNZew1zPzCQAAQOGUTwAAAArntFvaJqfeAADAXsXMJwAAAIVTPgEAACic024BAKAtcNkRezkznwAAABRO+QQAAKBwyicAAACFUz4BAAAonPIJAABA4ZRPAAAACqd8AgAAUDjlEwAAgMIpnwAAABRO+QQAAKBwyicAAACF61jqAAAAQBtQVrb915ub90wO9lpmPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwnXckZWuvPLKLF26NM8991zOPffc9OvXL9OmTUtTU1N69OiRL37xiykvL8/ChQtz0003pUOHDhk3blxOP/30ovMDAADQDrRaPn/5y1/m4YcfzoIFC9LQ0JBTTjklQ4YMSXV1dUaPHp2rr746NTU1GTNmTObNm5eampp06tQpY8eOzahRo9K1a9c9sR8AAAC0Ya2ednvsscfmS1/6UpJk//33z9atW7NkyZKMHDkySTJ8+PDU1tZm+fLl6devXyoqKtK5c+cMHDgwy5YtKzY9AAAA7UKr5XOfffZJly5dkiQ1NTU5/vjjs3Xr1pSXlydJunfvnrq6utTX16dbt24t23Xr1i11dXUFxQYAANgLlZVt/6cd2+EbDt11112pqanJxRdf/KLlzc3N21z/5ZYDAADwyrND5fP+++/Pddddl69+9aupqKhIly5d0tjYmCRZv359KisrU1lZmfr6+pZtNmzYkMrKymJSAwAA7G578axjW9Bq+Xz66adz5ZVX5vrrr2+5edDQoUOzaNGiJMnixYszbNiw9O/fPytWrMimTZuyefPmLFu2LIMHDy42PQAAAO1Cq3e7veOOO9LQ0JALLrigZdnll1+eiy66KAsWLEivXr0yZsyYdOrUKVOnTs2kSZNSVlaWyZMnp6KiotDwAAAAtA9lzXv44sylS5dm0KBBe3JI2qPWTmso+mPb1sffExlaU+pj1BYytPXx20KGUo+/JzK0xjFq+8fA36vS/54kpc9Y6vHbQoZSj98WtIVj0BYy/Au21/d2+IZDAAAAsKuUTwAAAAqnfAIAAFA45RMAAIDCtXq3WwAAgD2ind9sh+0z8wkAAEDhlE8AAAAKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHDKJwAAAIVTPgEAACic8gkAAEDhlE8AAAAKp3wCAABQOOUTAACAwimfAAAAFK5jqQPQBpWVbf/15uY9kwMAANhrmPkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOGUTwAAAAqnfAIAAFA45RMAAIDCKZ8AAAAUTvkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOF2qHyuWrUqJ554Yr75zW8mSdatW5cJEyakuro6559/fp555pkkycKFC3Paaafl9NNPz/e+973iUgMAANCutFo+t2zZkksvvTRDhgxpWTZnzpxUV1fn5ptvTp8+fVJTU5MtW7Zk3rx5ufHGGzN//vzcdNNN2bhxY6HhAQAAaB9aLZ/l5eX56le/msrKypZlS5YsyciRI5Mkw4cPT21tbZYvX55+/fqloqIinTt3zsCBA7Ns2bLikgMAANBudGx1hY4d07Hji1fbunVrysvLkyTdu3dPXV1d6uvr061bt5Z1unXrlrq6ut0cFwAAgPboX77hUHNz804tBwAA4JVnl8pnly5d0tjYmCRZv359KisrU1lZmfr6+pZ1NmzY8KJTdQEAAHjl2qXyOXTo0CxatChJsnjx4gwbNiz9+/fPihUrsmnTpmzevDnLli3L4MGDd2tYAAAA2qdWr/lcuXJlrrjiiqxduzYdO3bMokWLctVVV2XGjBlZsGBBevXqlTFjxqRTp06ZOnVqJk2alLKyskyePDkVFRV7Yh8AAABo48qa9/DFmUuXLs2gQYP25JDsrLKy7b++Jz4ypc7Q1sffExlaU+pj1BYytPXx20KGUo+/JzK0xjFq+8fA36vS/54kpc9Y6vHbQoZSj98WMpR6/LaS4V+wvb73L99wCAAAAFqjfAIAAFA45RMAAIDCKZ8AAAAUTvkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOGUTwAAAAqnfAIAAFA45RMAAIDCKZ8AAAAUTvkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOGUTwAAAAqnfAIAAFA45RMAAIDCKZ8AAAAUTvkEAACgcB1LHYBtKCvb/uvNzXsmBwAAwG5i5hMAAIDCKZ8AAAAUzmm3/59TXgEAAHY7M58AAAAUTvkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOGUTwAAAAqnfAIAAFA45RMAAIDCKZ8AAAAUTvkEAACgcMonAAAAhVM+AQAAKJzyCQAAQOGUTwAAAArXcXe/4ec///ksX748ZWVlmTlzZo4++ujdPQQAAADtzG4tn7/61a/y5z//OQsWLMgjjzySmTNnZsGCBbtzCAAAANqh3XrabW1tbU488cQkyaGHHpqnnnoqf/vb33bnEAAAALRDZc3Nzc27680+9alP5W1ve1tLAa2urs7nPve5HHLIIS3rLF26dHcNBwAAQBszaNCgbS7f7dd8/rNt9dqXCwIAAMDea7eedltZWZn6+vqW5xs2bEiPHj125xAAAAC0Q7u1fL71rW/NokWLkiS/+93vUllZmVe/+tW7cwgAAADaod1aPgcOHJijjjoqZ555Zi677LJ8+tOf3qHtrrzyypxxxhk57bTTsnjx4t0Zib1AY2NjTjzxxNx6662ljkIbs3DhwrznPe/JqaeemnvvvbfUcWgjNm/enClTpmTChAk588wzc//995c6Em3AqlWrcuKJJ+ab3/xmkmTdunWZMGFCqqurc/755+eZZ54pcUJKZVufjbPOOivjx4/PWWedlbq6uhInpFT+/2fjBffff3+qqqpKlKp92+3XfH784x/fqfV/+ctf5uGHH86CBQvS0NCQU045Jf/+7/++u2PRjl177bU54IADSh2DNqahoSHz5s3LLbfcki1btmTu3Lk54YQTSh2LNuD73/9+DjnkkEydOjXr16/P+9///tx5552ljkUJbdmyJZdeemmGDBnSsmzOnDmprq7O6NGjc/XVV6empibV1dUlTEkpbOuzMXv27IwbNy7veMc78q1vfSv//d//nWnTppUwJaWwrc9Gkvz973/PV77yFZcW7qLdOvO5K4499th86UtfSpLsv//+2bp1a5qamkqcirbikUceyerVq5UKXqK2tjZDhgzJq1/96lRWVubSSy8tdSTaiAMPPDAbN25MkmzatCkHHnhgiRNRauXl5fnqV7+aysrKlmVLlizJyJEjkyTDhw9PbW1tqeJRQtv6bHz605/OSSedlOTFf094ZdnWZyNJrrvuulRXV6e8vLxEydq3kpfPffbZJ126dEmS1NTU5Pjjj88+++xT4lS0FVdccUVmzJhR6hi0QX/961/T2NiYD33oQ6murvYPR1q8853vzGOPPZZRo0Zl/PjxmT59eqkjUWIdO3ZM586dX7Rs69atLf947N69u1MrX6G29dno0qVL9tlnnzQ1NeXmm2/Ou9/97hKlo5S29dn44x//mD/84Q8ZPXp0iVK1f4V+1crOuOuuu1JTU5Ovf/3rpY5CG/GDH/wgxxxzTF73uteVOgpt1MaNG3PNNdfksccey8SJE3PPPfekrKys1LEosdtuuy29evXKDTfckD/84Q+ZOXOma8bZrt34lefsJZqamjJt2rS85S1veclpl7xyfeELX8hFF11U6hjtWpson/fff3+uu+66fO1rX0tFRUWp49BG3HvvvVmzZk3uvffePP744ykvL89rX/vaDB06tNTRaAO6d++eAQMGpGPHjjn44IOz33775cknn0z37t1LHY0SW7ZsWY477rgkyRFHHJENGzakqanJWTW8SJcuXdLY2JjOnTtn/fr1Lzm1jle2Cy+8MH369MmUKVNKHYU2Yv369Xn00Udb7m+zYcOGjB8//iU3I2L7Sl4+n3766Vx55ZW58cYb07Vr11LHoQ2ZPXt2y+O5c+emd+/eiictjjvuuMyYMSPnnHNOnnrqqWzZssW1fSRJ+vTpk+XLl+ekk07K2rVrs99++ymevMTQoUOzaNGinHzyyVm8eHGGDRtW6ki0EQsXLkynTp3y0Y9+tNRRaEN69uyZu+66q+X5iBEjFM9dUPLyeccdd6ShoSEXXHBBy7IrrrgivXr1KmEqoK3r2bNnTjrppIwbNy5JctFFF6VDh5Jfxk4bcMYZZ2TmzJkZP358nnvuuVxyySWljkSJrVy5MldccUXWrl2bjh07ZtGiRbnqqqsyY8aMLFiwIL169cqYMWNKHZMS2NZn44knnsi+++6bCRMmJEkOPfRQf0degbb12Zg7d67Jsn9RWbMLHQAAACiYaQIAAAAKp3wCAABQOOUTAACAwimfAAAAFE75BAAAoHDKJwB7taamppx55pl55plnXnadCRMmpKqqKk8++WSS5M4770xVVVXmzp2bJFm7dm0mTZqUAcqq42kAAAT7SURBVAMGZODAgXnPe96T2trabb5XVVVVqqqq0rdv37z1rW/Nhz/84Tz44IM7lLWqqirvete7kvzj+42rqqpy55137szuAkCbpXwCsNeaPXt2+vfvnwceeCD9+/fPlClTdul9vvCFL6S2tjbnnXdeZsyYkaOPPjoNDQ0vu/5rX/vaXHbZZRk9enTuu+++VFdXZ/Xq1bu6Gzvlueee2yPjAMDO6ljqAABQhPXr1+faa6/N29/+9jz66KM599xzs2bNml16r0cffTQdO3bM8ccfnyOOOCLjxo3b7voVFRUZM2ZMxowZk9e85jWZNWtWvvKVr+TKK6/Mww8/nMsuuywrVqzIAQcckLFjx+bDH/5wysrKtvue48aNy+rVq9PU1JRDDz00M2fOzODBg7NkyZJMnDgxxx9/fBoaGvL888/nqquuyvTp0/PQQw9l3333zeGHH56bb755l/YdAHYXM58A7JXKyspSVlaWurq6NDU1ZcCAATnvvPN26b0GDx6cv//97zn55JNz3HHH5TOf+Uw2bty4Q9sef/zxSZKVK1fm2WefzXnnnZff/va3ueCCC1JVVZU5c+bklltuafV9hg4dmgsvvDBTpkxJXV1dZs6c+aLXa2trM2rUqJx11lm5+eabs2LFinziE5/If/7nf6ZXr147v9MAsJuZ+QRgr1RZWZnp06fn+uuvT0NDQ0aMGJHRo0dn1qxZL5ll/P/Pm5ubX7T8oosuysEHH5zFixdn5cqVufnmm9PQ0JDZs2e3muOf3+uPf/xj1qxZk3e9610ts5X33HNPfvrTn2bs2LEv+x6bN2/O73//+3zlK19JU1NTy/LGxsaWxyeccELOPffcJMmmTZvS3Nyc++67L/369cvEiRNbzQkARTPzCcBe6+yzz84vfvGL9OvXL+973/vyox/9KA899NBL1uvRo0eSpK6uLkmyYcOGJEnPnj1b1vnABz6Q7373u7nzzjtTVlaWhx9+eIcy/OxnP0uSHHXUUS3LXii1rZ1q+4KFCxfmvvvuy+jRo3PDDTe0vNc/30SpsrKy5fH48eNz4403pl+/frn77rtzxhln5NFHH92hsQCgKGY+AdgrPfLII/mv//qvDBkyJFu2bGk5TbZz584vWXfYsGG5/fbbM3PmzAwdOjS33nprOnXqlLe85S1JkrPOOiuHH354jjrqqDz22GNpbm7OG9/4xpcd++mnn84PfvCDrFy5Mt/5znfSpUuXfPCDH0yfPn1y8MEH5+677878+fPzi1/8Iknytre9bYf2afPmzXnooYeyatWq7a737W9/Ow0NDenTp0/69OmThx56KE888UTe8IY37NA4AFAE5ROAvVLXrl3T1NSUa665Jhs3bsyTTz6Zj3zkI3n961//knVPPvnkrF27Nrfccku+8Y1vpE+fPvnsZz+b173udUmS4447Lrfffntuu+22dOzYMSeccEKmT5/+smM//vjjueiii9K1a9e87W1vy0c+8pEcdthhSZIvf/nLufTSS3P11VfngAMOyEc/+tGceuqp292Xd7/73Vm8eHFLWT322GNbHm9LeXl5br311jz++OPZb7/98r73vS+DBg1q7ZABQKHKml+4GAUA9lITJkzI/PnzSx0DAF7RXPMJAABA4cx8AgAAUDgznwAAABRO+QQAAKBwyicAAACFUz4BAAAonPIJAABA4ZRPAAAACvd/SxoeQY0f1QIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMSdAAjcr4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f143fcd-a27c-4188-9a28-62c64eefd0da"
      },
      "source": [
        "y = df_train.fare_amount.values + 1e-10\n",
        "y ### for supervised learning: output vector y"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.54,  8.  , 34.  , ...,  4.5 ,  6.5 ,  7.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOeHvi3cu1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1458ae87-b99a-4411-9a1a-3183fc45fc52"
      },
      "source": [
        "# List first rows (post-cleaning):\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>22.54</td>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>34.00</td>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>11.50</td>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "31         22.54        -74.010483        40.717667  ...        6      6  2015\n",
              "310         8.00        -74.010727        40.710091  ...        5      6  2015\n",
              "314        34.00        -73.974899        40.751095  ...        1      6  2015\n",
              "321         8.00        -73.961784        40.759579  ...        0      6  2015\n",
              "486        11.50        -73.957443        40.761703  ...        0      6  2015\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lT9BBicw4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "47760a63-ebc3-46c4-a7c6-2d41a7f79fa3"
      },
      "source": [
        "X = df_train.drop(['fare_amount', 'month', 'year'], axis = 1)\n",
        "X.head() ### for supervised learning: input matrix X"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pickup_longitude  pickup_latitude  ...  hour  weekday\n",
              "31         -74.010483        40.717667  ...    21        6\n",
              "310        -74.010727        40.710091  ...     9        5\n",
              "314        -73.974899        40.751095  ...    23        1\n",
              "321        -73.961784        40.759579  ...    21        0\n",
              "486        -73.957443        40.761703  ...    19        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eC8SDPzczNY"
      },
      "source": [
        "### Optimum rmse: regression model objective function is Root Mean Square Error (RMSE); \n",
        "### Should be minimized (as close to zero as possible):\n",
        "\n",
        "y_global_orig = 0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTmWEhSc1qQ"
      },
      "source": [
        "### Bayesian Optimization - inputs:\n",
        "\n",
        "obj_func = 'XGBoost'\n",
        "n_start_AcqFunc = 100\n",
        "n_test = 500 # test points\n",
        "df = 3 # nu\n",
        "\n",
        "util_zero = 'dEI_GP'\n",
        "util_exact = 'dEI_GP'\n",
        "n_init = 5 # random initialisations\n",
        "opt = True\n",
        "\n",
        "test_perc = 0.667\n",
        "train_perc = 1 - test_perc\n",
        "\n",
        "n_test = int(len(df_train) * test_perc)\n",
        "n_train = int(len(df_train) - n_test)\n",
        "\n",
        "eps = 1e-08"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ngnRxbc7cg"
      },
      "source": [
        "### Objective function:\n",
        "\n",
        "if obj_func == 'XGBoost': # 6-D\n",
        "            \n",
        "    # Constraints:\n",
        "    param_lb_alpha = 0\n",
        "    param_ub_alpha = 10\n",
        "    \n",
        "    param_lb_gamma = 0\n",
        "    param_ub_gamma = 10\n",
        "    \n",
        "    param_lb_max_depth = 5\n",
        "    param_ub_max_depth = 15\n",
        "    \n",
        "    param_lb_min_child_weight = 1\n",
        "    param_ub_min_child_weight = 20\n",
        "    \n",
        "    param_lb_subsample = .5\n",
        "    param_ub_subsample = 1\n",
        "    \n",
        "    param_lb_colsample = .1\n",
        "    param_ub_colsample = 1\n",
        "    \n",
        "    # 6-D inputs' parameter bounds:\n",
        "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
        "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
        "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
        "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
        "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
        "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
        "        }\n",
        "       \n",
        "    # True y bounds:\n",
        "    dim = 6\n",
        "    \n",
        "    max_iter = 20  # iterations of Bayesian optimization\n",
        "    \n",
        "    operator = 1 \n",
        "    \n",
        "    n_est = 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJsNX29c_xA"
      },
      "source": [
        "### Surrogate derivatives: \n",
        "\n",
        "cov_func = squaredExponential()\n",
        "\n",
        "def kronDelta(X, Xstar):\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\n",
        "\n",
        "def se(X, Xstar, sigmaf, l, sigman):\n",
        "    return sigmaf * np.exp(-0.5 * cdist(X, Xstar) ** 2 / l ** 2) + sigman * kronDelta(X, Xstar)\n",
        "\n",
        "def deriv_se(X, Xstar, sigmaf, l, sigman):\n",
        "    return cdist(X, Xstar) / (l ** 2) * se(X, Xstar, sigmaf, l, sigman)\n",
        "\n",
        "def der_covmat(X, Xstar, sigmaf, l, sigman):\n",
        "      nx = len(X)\n",
        "      ny = len(Xstar)\n",
        "      return np.round(np.array([deriv_se(np.atleast_2d(i), np.atleast_2d(j), sigmaf, l, sigman) for (i, j) in itertools.product(X, Xstar)]).reshape(nx, ny), 8)\n",
        "\n",
        "class dGaussianProcess(GaussianProcess):\n",
        "    l = GaussianProcess(cov_func, optimize=opt).getcovparams()['l']\n",
        "    sigmaf = GaussianProcess(cov_func, optimize=opt).getcovparams()['sigmaf']\n",
        "    sigman = GaussianProcess(cov_func, optimize=opt).getcovparams()['sigman']\n",
        "\n",
        "    def AcqGrad(self, Xstar):\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = self.covfunc.K(self.X, Xstar).T\n",
        "        dKstar = der_covmat(self.X, Xstar, self.sigmaf, self.l, self.sigman).T\n",
        "        \n",
        "        alpha_Kstar = np.dot(np.linalg.inv(self.K + (self.sigman**2) * np.eye(len(self.X))), Kstar.T)\n",
        "        \n",
        "        dm = np.dot(-dKstar, self.alpha)\n",
        "        ds = -2 * np.dot(-dKstar, alpha_Kstar)\n",
        "        \n",
        "        return dm, ds\n",
        "        "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ZuEB2VdE0W"
      },
      "source": [
        "### Set-seeds:\n",
        "\n",
        "run_num_1 = 0\n",
        "run_num_2 = 2\n",
        "run_num_3 = 3\n",
        "run_num_4 = 4\n",
        "run_num_5 = 5\n",
        "run_num_6 = 6\n",
        "run_num_7 = 7\n",
        "run_num_8 = 8\n",
        "run_num_9 = 9\n",
        "run_num_10 = 10\n",
        "run_num_11 = 11\n",
        "run_num_12 = 12\n",
        "run_num_13 = 13\n",
        "run_num_14 = 14\n",
        "run_num_15 = 15\n",
        "run_num_16 = 16\n",
        "run_num_17 = 17\n",
        "run_num_18 = 18\n",
        "run_num_19 = 19\n",
        "run_num_20 = 20\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHMFEyPdCk4"
      },
      "source": [
        "### Cumulative Regret Calculator:\n",
        "\n",
        "def min_max_array(x):\n",
        "    new_list = []\n",
        "    for i, num in enumerate(x):\n",
        "            new_list.append(np.min(x[0:i+1]))\n",
        "    return new_list\n",
        "    "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJMhL70fdHz_"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \n",
        "    def __init__(self, mode, eps=eps, **params):\n",
        "        \n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "\n",
        "        mode_dict = {\n",
        "            'dEI_GP': self.dEI_GP\n",
        "        }\n",
        "\n",
        "        self.f = mode_dict[mode]\n",
        "    \n",
        "    def dEI_GP(self, tau, mean, std, ds, dm):\n",
        "        gamma = (mean - tau - self.eps) / (std + self.eps)\n",
        "        gamma_h = (mean - tau) / (std + self.eps)\n",
        "        dsdx = ds / (2 * (std + self.eps))\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (gamma * norm.cdf(gamma) + norm.pdf(gamma))\n",
        "        df1 = f / (std + self.eps) * dsdx \n",
        "        df2 = (std + self.eps) * norm.cdf(gamma) * dmdx\n",
        "        df = (df1 + df2)[0]\n",
        "        df_arr = []\n",
        "\n",
        "        for j in range(0, dim):\n",
        "          df_arr.append(df)\n",
        "        return f, np.asarray(df_arr).transpose()\n",
        "        \n",
        "    def d_eval(self, tau, mean, std, ds, dm):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, **self.params)\n",
        "        "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S422jNLsdIMm"
      },
      "source": [
        "## dGPGO_zero:\n",
        "\n",
        "zero_grad = 0\n",
        "\n",
        "class dGPGO_zero(GPGO):\n",
        "    n_start = n_start_AcqFunc\n",
        "\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start_AcqFunc):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.acqfunc,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "        self.start_points_arr = start_points_arr\n",
        "\n",
        "        return x_best, f_best\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)\n",
        "\n",
        "    def acqfunc(self, xnew, n_start=n_start_AcqFunc):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + eps)\n",
        "        dm, ds = self.GP.AcqGrad(xnew)\n",
        "        f, df = self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm)\n",
        "\n",
        "        return -f, -df * zero_grad\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McpUqQq4so4x"
      },
      "source": [
        "## dGPGO:\n",
        "\n",
        "grad = 1\n",
        "\n",
        "class dGPGO(GPGO):\n",
        "    n_start = n_start_AcqFunc\n",
        "\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start_AcqFunc):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.acqfunc,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "        self.start_points_arr = start_points_arr\n",
        "\n",
        "        return x_best, f_best\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)\n",
        "\n",
        "    def acqfunc(self, xnew, n_start=n_start_AcqFunc):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + eps)\n",
        "        dm, ds = self.GP.AcqGrad(xnew)\n",
        "        f, df = self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm)\n",
        "\n",
        "        return -f, -df * grad\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlilveEgdIR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec42737-2d53-4a67-dc36-d8058743372e"
      },
      "source": [
        "start_zero = time.time()\n",
        "start_zero"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1629368409.3072135"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlzDSHbUG-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687c5d55-bc42-44d9-821e-976be5b08878"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 1\n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_zero_1 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_1 = dGPGO_zero(surrogate_zero_1, Acquisition_new(util_zero), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
        "zero_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_1 = zero_1.getResult()[0]\n",
        "params_zero_1['max_depth'] = int(params_zero_1['max_depth'])\n",
        "params_zero_1['min_child_weight'] = int(params_zero_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_zero_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_zero_1 = xgb.train(params_zero_1, dX_zero_train1)\n",
        "pred_zero_1 = model_zero_1.predict(dX_zero_test1)\n",
        "\n",
        "rmse_zero_1 = np.sqrt(mean_squared_error(pred_zero_1, y_test1))\n",
        "rmse_zero_1"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [5.48813504 7.15189366 8.         0.92897281 8.         0.48128932]. \t  -0.5626915313589341 \t -0.46143572360276275\n",
            "init   \t [ 6.45894113  4.37587211 11.          0.52835649 13.          0.44509737]. \t  -0.5772881365468763 \t -0.46143572360276275\n",
            "init   \t [ 7.91725038  5.2889492  13.          0.6963924  14.          0.40365654]. \t  -0.5870544636272766 \t -0.46143572360276275\n",
            "init   \t [ 6.48171872  3.6824154  10.          0.88907838 16.          0.88307853]. \t  -0.46143572360276275 \t -0.46143572360276275\n",
            "init   \t [4.73608045 8.00910752 8.         0.83943977 8.         0.67592892]. \t  -0.5051288806760134 \t -0.46143572360276275\n",
            "1      \t [ 0.96098408  9.76459465  7.          0.75481219 17.          0.64436097]. \t  -0.5149059967458438 \t -0.46143572360276275\n",
            "2      \t [ 5.13759733  2.22657933 12.          0.58106013  2.          0.92007745]. \t  -0.46868154430393166 \t -0.46143572360276275\n",
            "3      \t [9.58067178 9.65734278 7.         0.88193436 1.         0.38223155]. \t  -0.5888715285109799 \t -0.46143572360276275\n",
            "4      \t [ 0.90969339  9.80979401 14.          0.8665633   3.          0.55460209]. \t  -0.5632471108749298 \t -0.46143572360276275\n",
            "5      \t [0.3028841  4.069464   5.         0.51500749 1.         0.27359263]. \t  -0.6787735406244566 \t -0.46143572360276275\n",
            "6      \t [ 8.87166351  9.3367646   5.          0.58200219 19.          0.4055524 ]. \t  -0.6087566280502396 \t -0.46143572360276275\n",
            "7      \t [ 0.51228404  8.90605177 14.          0.7949699  11.          0.73913262]. \t  \u001b[92m-0.46104762612970374\u001b[0m \t -0.46104762612970374\n",
            "8      \t [ 2.05150398  0.53599727  5.          0.71551734 11.          0.37377971]. \t  -0.60261611749113 \t -0.46104762612970374\n",
            "9      \t [8.38797278 0.6003286  6.         0.6181346  1.         0.60370114]. \t  -0.526164863483156 \t -0.46104762612970374\n",
            "10     \t [ 9.06530919  9.40796401 14.          0.73452132  4.          0.22638202]. \t  -0.6737273666958219 \t -0.46104762612970374\n",
            "11     \t [ 2.92761431  0.02841708 13.          0.97950295  9.          0.31091424]. \t  -0.5822275926277118 \t -0.46104762612970374\n",
            "12     \t [9.14092793 0.64739809 8.         0.90967035 8.         0.70435457]. \t  -0.5029298264030027 \t -0.46104762612970374\n",
            "13     \t [ 1.99180311  1.76156949  6.          0.52474573 18.          0.69397695]. \t  -0.5292680491699555 \t -0.46104762612970374\n",
            "14     \t [ 1.47165443  0.32474578 13.          0.93757169 15.          0.40230457]. \t  -0.5795930586132758 \t -0.46104762612970374\n",
            "15     \t [ 3.33998723  9.61997442 13.          0.87678219 17.          0.47112812]. \t  -0.562089749118206 \t -0.46104762612970374\n",
            "16     \t [ 0.59268574  5.88934857  8.          0.90451848 12.          0.19426621]. \t  -0.6701635295211161 \t -0.46104762612970374\n",
            "17     \t [ 7.96942817  9.76181769  7.          0.91046857 13.          0.32455735]. \t  -0.5870001390492389 \t -0.46104762612970374\n",
            "18     \t [0.45095418 9.96977628 8.         0.5330212  1.         0.30363698]. \t  -0.5984194428816372 \t -0.46104762612970374\n",
            "19     \t [ 0.28154114  3.4783841  11.          0.56828738  5.          0.85379896]. \t  -0.4789169596939045 \t -0.46104762612970374\n",
            "20     \t [3.69919707 0.85322265 6.         0.75904625 5.         0.97222422]. \t  -0.4807633478348844 \t -0.46104762612970374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.667222167746296"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJ9rN2KUJzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f197f14c-ed52-458b-bdc3-eadf4a7ddab2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 2\n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_zero_2 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_2 = dGPGO_zero(surrogate_zero_2, Acquisition_new(util_zero), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
        "zero_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_2 = zero_2.getResult()[0]\n",
        "params_zero_2['max_depth'] = int(params_zero_2['max_depth'])\n",
        "params_zero_2['min_child_weight'] = int(params_zero_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_zero_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_zero_2 = xgb.train(params_zero_2, dX_zero_train2)\n",
        "pred_zero_2 = model_zero_2.predict(dX_zero_test2)\n",
        "\n",
        "rmse_zero_2 = np.sqrt(mean_squared_error(pred_zero_2, y_test2))\n",
        "rmse_zero_2"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 4.35994902  0.25926232 11.          0.97386531 12.          0.47833102]. \t  -0.5147867600449748 \t -0.4765694615523879\n",
            "init   \t [ 3.30334821  2.04648634 10.          0.55997527  6.          0.71472339]. \t  -0.4765694615523879 \t -0.4765694615523879\n",
            "init   \t [ 4.9856117   5.86796978  8.          0.89266757 11.          0.59158659]. \t  -0.4946399399702889 \t -0.4765694615523879\n",
            "init   \t [ 4.07307832  1.76984624 13.          0.75262305  7.          0.35908193]. \t  -0.5884370816585467 \t -0.4765694615523879\n",
            "init   \t [ 1.16193318  1.81727038  9.          0.79837265 19.          0.29965165]. \t  -0.584982798458911 \t -0.4765694615523879\n",
            "1      \t [ 9.41115874  8.16019152 11.          0.70945365  2.          0.28765225]. \t  -0.5900551627442152 \t -0.4765694615523879\n",
            "2      \t [ 8.78180153  6.61060882 12.          0.91523653 18.          0.29687212]. \t  -0.5840366351865409 \t -0.4765694615523879\n",
            "3      \t [ 0.66591974  9.26661294 14.          0.96342421 18.          0.94909068]. \t  \u001b[92m-0.3879430352217851\u001b[0m \t -0.3879430352217851\n",
            "4      \t [0.53023554 8.79041977 6.         0.85342606 1.         0.93968064]. \t  -0.41489686189456554 \t -0.3879430352217851\n",
            "5      \t [6.47584802 1.65960359 8.         0.80737784 1.         0.90602123]. \t  -0.4063073187409662 \t -0.3879430352217851\n",
            "6      \t [ 2.29390808  9.72620131 13.          0.87738596  7.          0.2041919 ]. \t  -0.6789018673924765 \t -0.3879430352217851\n",
            "7      \t [ 9.77744834  2.26597384  5.          0.98618685 19.          0.58642903]. \t  -0.520384169717879 \t -0.3879430352217851\n",
            "8      \t [7.36877801 8.87815651 5.         0.56972286 4.         0.80625064]. \t  -0.5008693406902159 \t -0.3879430352217851\n",
            "9      \t [ 5.76886466  6.30636441 11.          0.85075313  6.          0.94564556]. \t  -0.39766690308447894 \t -0.3879430352217851\n",
            "10     \t [ 0.92680624  0.80829442  5.          0.99163751 10.          0.48273491]. \t  -0.5317089151903325 \t -0.3879430352217851\n",
            "11     \t [ 8.66397635  9.94226364  5.          0.61754568 19.          0.46861962]. \t  -0.5385752501499373 \t -0.3879430352217851\n",
            "12     \t [ 2.21750241  8.1937508   6.          0.88756269 17.          0.73205426]. \t  -0.4886010805216155 \t -0.3879430352217851\n",
            "13     \t [ 9.78109337  2.52726344  5.          0.62902817 11.          0.70291169]. \t  -0.5193465670402935 \t -0.3879430352217851\n",
            "14     \t [ 2.61078484  8.48438058 14.          0.79784401 12.          0.37480602]. \t  -0.5873563000820609 \t -0.3879430352217851\n",
            "15     \t [ 9.63046012  9.46551843 14.          0.83717129 10.          0.78268661]. \t  -0.4661700953958383 \t -0.3879430352217851\n",
            "16     \t [ 9.16837918  2.32035478 14.          0.50291805  1.          0.25746282]. \t  -0.6778946275066893 \t -0.3879430352217851\n",
            "17     \t [ 1.36729794  4.89791321 13.          0.50996816  1.          0.64172429]. \t  -0.5046458287629003 \t -0.3879430352217851\n",
            "18     \t [6.0348008  1.05838904 5.         0.60659353 6.         0.66244736]. \t  -0.5184813195331227 \t -0.3879430352217851\n",
            "19     \t [ 6.62258175  9.86406301  9.          0.79905852 14.          0.17174744]. \t  -0.6757313422236788 \t -0.3879430352217851\n",
            "20     \t [1.05391958 2.10485253 7.         0.55626962 1.         0.19660783]. \t  -0.6761021573122338 \t -0.3879430352217851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.600295852493361"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-45l3NU4UNiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef32642-8721-4300-e875-06ab1e81bd71"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 3\n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_zero_3 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_3 = dGPGO_zero(surrogate_zero_3, Acquisition_new(util_zero), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
        "zero_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_3 = zero_3.getResult()[0]\n",
        "params_zero_3['max_depth'] = int(params_zero_3['max_depth'])\n",
        "params_zero_3['min_child_weight'] = int(params_zero_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_zero_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_zero_3 = xgb.train(params_zero_3, dX_zero_train3)\n",
        "pred_zero_3 = model_zero_3.predict(dX_zero_test3)\n",
        "\n",
        "rmse_zero_3 = np.sqrt(mean_squared_error(pred_zero_3, y_test3))\n",
        "rmse_zero_3"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.50797903  7.08147823 13.          0.56066429 11.          0.11687321]. \t  -0.7165783188757435 \t -0.6409647951145182\n",
            "init   \t [ 0.40630737  2.47888297 11.          0.72040492 13.          0.23083313]. \t  -0.7204431346766296 \t -0.6409647951145182\n",
            "init   \t [ 4.53172301  2.15577008 11.          0.74631796  2.          0.60296868]. \t  -0.6409647951145182 \t -0.6409647951145182\n",
            "init   \t [ 2.59252447  4.15101197 13.          0.79330998  8.          0.24118096]. \t  -0.7214290072967551 \t -0.6409647951145182\n",
            "init   \t [ 5.44649018  7.80314765 10.          0.62879264 18.          0.44917413]. \t  -0.6558401549443297 \t -0.6409647951145182\n",
            "1      \t [4.88873245 9.27936348 6.         0.94344906 8.         0.25949204]. \t  -0.7188700813687741 \t -0.6409647951145182\n",
            "2      \t [ 8.93142368  1.52910591 13.          0.84039318 17.          0.60846833]. \t  \u001b[92m-0.6320347814043803\u001b[0m \t -0.6320347814043803\n",
            "3      \t [ 6.38594331  1.19109066  5.          0.81189053 13.          0.59164768]. \t  \u001b[92m-0.6289274956252369\u001b[0m \t -0.6289274956252369\n",
            "4      \t [ 1.02918863  9.32189805 13.          0.88333707  1.          0.86998588]. \t  \u001b[92m-0.45190108244647254\u001b[0m \t -0.45190108244647254\n",
            "5      \t [ 9.74929058  1.51205926 11.          0.50025602  8.          0.46132437]. \t  -0.6593087000926661 \t -0.45190108244647254\n",
            "6      \t [ 9.45052852  8.62641484  7.          0.79615518 14.          0.32790361]. \t  -0.708730491334048 \t -0.45190108244647254\n",
            "7      \t [ 8.92744991  9.09956287 12.          0.74944313  3.          0.11030352]. \t  -0.7194264138705575 \t -0.45190108244647254\n",
            "8      \t [0.28002919 1.86471402 5.         0.90893429 3.         0.44808833]. \t  -0.653236808211726 \t -0.45190108244647254\n",
            "9      \t [ 9.02893081  7.64399312 14.          0.72894099 15.          0.70609928]. \t  -0.6325306398208942 \t -0.45190108244647254\n",
            "10     \t [4.37264513 7.83931591 6.         0.65864599 1.         0.34090671]. \t  -0.7108079171350814 \t -0.45190108244647254\n",
            "11     \t [ 2.84857043  0.57472701  5.          0.53705172 19.          0.51858228]. \t  -0.6589951838647294 \t -0.45190108244647254\n",
            "12     \t [ 0.63346059  9.87550877  6.          0.74382195 14.          0.39340576]. \t  -0.7092906536332471 \t -0.45190108244647254\n",
            "13     \t [2.99254427 2.69228882 7.         0.63915731 9.         0.21678027]. \t  -0.7194910621859361 \t -0.45190108244647254\n",
            "14     \t [8.8237369  0.04240955 5.         0.7158964  1.         0.99950517]. \t  -0.48215926032694006 \t -0.45190108244647254\n",
            "15     \t [ 5.7586577   1.42812945 10.          0.74022438  8.          0.79053147]. \t  -0.5205954164321709 \t -0.45190108244647254\n",
            "16     \t [9.25339855 3.80341124 5.         0.92679998 9.         0.56391072]. \t  -0.6536743943093699 \t -0.45190108244647254\n",
            "17     \t [ 0.94336997  1.29293405 12.          0.62917786 19.          0.81056172]. \t  -0.5192284078227264 \t -0.45190108244647254\n",
            "18     \t [ 0.31516471  7.97295608 10.          0.77489537  9.          0.25855149]. \t  -0.7193805891172754 \t -0.45190108244647254\n",
            "19     \t [ 9.8542409   3.34207335 14.          0.75035548  3.          0.6035725 ]. \t  -0.6317346576420496 \t -0.45190108244647254\n",
            "20     \t [ 3.60954441  5.81173439  5.          0.60415615 17.          0.56274483]. \t  -0.6561590588771492 \t -0.45190108244647254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.649527586702978"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voPfk1UDUQU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1648ed9c-06af-4b9d-83ea-357dc2597591"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 4\n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_zero_4 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_4 = dGPGO_zero(surrogate_zero_4, Acquisition_new(util_zero), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
        "zero_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_4 = zero_4.getResult()[0]\n",
        "params_zero_4['max_depth'] = int(params_zero_4['max_depth'])\n",
        "params_zero_4['min_child_weight'] = int(params_zero_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_zero_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_zero_4 = xgb.train(params_zero_4, dX_zero_train4)\n",
        "pred_zero_4 = model_zero_4.predict(dX_zero_test4)\n",
        "\n",
        "rmse_zero_4 = np.sqrt(mean_squared_error(pred_zero_4, y_test4))\n",
        "rmse_zero_4"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.67029839 5.47232249 6.         0.92781047 9.         0.72795594]. \t  -0.5993772224326677 \t -0.4983304913999733\n",
            "init   \t [ 2.16089496  9.76274455 12.          0.62649118  9.          0.66966679]. \t  -0.6069567430422909 \t -0.4983304913999733\n",
            "init   \t [ 0.05159149  5.72356491  9.          0.99170034 10.          0.10808749]. \t  -0.7139334307278753 \t -0.4983304913999733\n",
            "init   \t [ 3.86571283  0.44160058 10.          0.90553105 18.          0.95407958]. \t  -0.4983304913999733 \t -0.4983304913999733\n",
            "init   \t [ 7.86305986  8.66289299  6.          0.53285477 14.          0.25117497]. \t  -0.7091576633146701 \t -0.4983304913999733\n",
            "1      \t [ 8.45443649  8.61014312 11.          0.83475494  1.          0.14018305]. \t  -0.7189305559932541 \t -0.4983304913999733\n",
            "2      \t [ 0.77431146  1.96668116 12.          0.50723361  3.          0.74768925]. \t  -0.6140173026050956 \t -0.4983304913999733\n",
            "3      \t [2.27858743 6.23199766 5.         0.58705984 2.         0.80794289]. \t  -0.5986031600147258 \t -0.4983304913999733\n",
            "4      \t [ 6.832625    9.87635293 14.          0.84450885 19.          0.20389666]. \t  -0.7153744507380464 \t -0.4983304913999733\n",
            "5      \t [ 7.37255369  2.03491596 13.          0.8921741   9.          0.46934318]. \t  -0.625550615237801 \t -0.4983304913999733\n",
            "6      \t [ 0.05992751  6.06320143 14.          0.89322475 17.          0.32211096]. \t  -0.6540035079472759 \t -0.4983304913999733\n",
            "7      \t [ 0.79250634  6.36332745  6.          0.84703891 17.          0.8628914 ]. \t  -0.5176542387032834 \t -0.4983304913999733\n",
            "8      \t [9.26767626 0.09691703 5.         0.59554562 3.         0.95249041]. \t  -0.524958204283032 \t -0.4983304913999733\n",
            "9      \t [ 9.93824172  2.34876498  7.          0.94210707 16.          0.6947317 ]. \t  -0.6069344861897712 \t -0.4983304913999733\n",
            "10     \t [3.43076773 0.51115291 6.         0.80398076 7.         0.163561  ]. \t  -0.7152347423610873 \t -0.4983304913999733\n",
            "11     \t [8.12039932 9.82838311 5.         0.6851891  3.         0.87462757]. \t  -0.5209212303565759 \t -0.4983304913999733\n",
            "12     \t [ 6.03647398  5.02077859  7.          0.51964174 12.          0.90396407]. \t  -0.5142832795776289 \t -0.4983304913999733\n",
            "13     \t [ 9.03174101  2.34471053 13.          0.52042845  3.          0.20136175]. \t  -0.7105427027924398 \t -0.4983304913999733\n",
            "14     \t [ 9.29091353  0.24848851 14.          0.65742606 17.          0.33836791]. \t  -0.655558370799832 \t -0.4983304913999733\n",
            "15     \t [3.19488299 9.73527155 6.         0.65266597 9.         0.60657999]. \t  -0.6104166590688074 \t -0.4983304913999733\n",
            "16     \t [ 2.02212851  9.9407933  10.          0.71009222  3.          0.59925198]. \t  -0.6118231860074616 \t -0.4983304913999733\n",
            "17     \t [ 7.85427932  8.81949282 14.          0.936894    6.          0.6768425 ]. \t  -0.6008015870134431 \t -0.4983304913999733\n",
            "18     \t [ 6.65760625  2.35253217 11.          0.58483338 14.          0.21088543]. \t  -0.7100640025661658 \t -0.4983304913999733\n",
            "19     \t [ 0.9141999   0.89707594 12.          0.91410151 11.          0.69348379]. \t  -0.6076939108347638 \t -0.4983304913999733\n",
            "20     \t [ 6.39825002  5.88858918  9.          0.67432081 18.          0.813834  ]. \t  -0.5921986299757287 \t -0.4983304913999733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.04081354533672"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kEnTd7MUdlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2d4761-77db-402a-a7af-f9ff7f6a6d1a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 5\n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_zero_5 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_5 = dGPGO_zero(surrogate_zero_5, Acquisition_new(util_zero), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
        "zero_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_5 = zero_5.getResult()[0]\n",
        "params_zero_5['max_depth'] = int(params_zero_5['max_depth'])\n",
        "params_zero_5['min_child_weight'] = int(params_zero_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_zero_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_zero_5 = xgb.train(params_zero_5, dX_zero_train5)\n",
        "pred_zero_5 = model_zero_5.predict(dX_zero_test5)\n",
        "\n",
        "rmse_zero_5 = np.sqrt(mean_squared_error(pred_zero_5, y_test5))\n",
        "rmse_zero_5"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.21993171  8.70732306 11.          0.68186845 10.          0.53957007]. \t  -0.5323233521622429 \t -0.48683475961332984\n",
            "init   \t [ 6.11743863  7.65907856  5.          0.64840025 16.          0.82745351]. \t  -0.48683475961332984 \t -0.48683475961332984\n",
            "init   \t [ 6.49458883  8.19472793  6.          0.93996852 19.          0.36647194]. \t  -0.586475577548452 \t -0.48683475961332984\n",
            "init   \t [ 6.28787909  5.7983781   6.          0.63290956 17.          0.18402673]. \t  -0.633609225551848 \t -0.48683475961332984\n",
            "init   \t [8.26554249 8.33492742 9.         0.97900675 3.         0.26957319]. \t  -0.6326249738244012 \t -0.48683475961332984\n",
            "1      \t [1.95474956 1.21548467 5.         0.65548996 6.         0.3261206 ]. \t  -0.5882279171378358 \t -0.48683475961332984\n",
            "2      \t [ 8.68915106  0.84881749 13.          0.9945373   7.          0.34624572]. \t  -0.5633370634302679 \t -0.48683475961332984\n",
            "3      \t [ 6.12310163  2.21013771 14.          0.74740665 18.          0.42989776]. \t  -0.5333068626906653 \t -0.48683475961332984\n",
            "4      \t [ 9.64635884  9.52633265 14.          0.67611826 10.          0.87506739]. \t  \u001b[92m-0.4423357727617705\u001b[0m \t -0.4423357727617705\n",
            "5      \t [ 0.42801231  0.53056997 14.          0.88001393  3.          0.60870677]. \t  -0.4956565139336144 \t -0.4423357727617705\n",
            "6      \t [ 0.33154545  2.45627958  8.          0.87454095 14.          0.94998466]. \t  \u001b[92m-0.4405807029170717\u001b[0m \t -0.4405807029170717\n",
            "7      \t [9.67414353 2.68949571 5.         0.62350468 9.         0.83649418]. \t  -0.4893925661153425 \t -0.4405807029170717\n",
            "8      \t [ 0.5259471   9.7567347  14.          0.91097263  1.          0.64281486]. \t  -0.49657614671134453 \t -0.4405807029170717\n",
            "9      \t [9.64880583 0.85455793 9.         0.80366346 1.         0.54097439]. \t  -0.5442200644862888 \t -0.4405807029170717\n",
            "10     \t [1.24717977 9.40645683 5.         0.65383928 3.         0.68852908]. \t  -0.5172211356155465 \t -0.4405807029170717\n",
            "11     \t [ 1.7633545   9.18687735 14.          0.62483521 19.          0.67316164]. \t  -0.4977308231633463 \t -0.4405807029170717\n",
            "12     \t [7.9980796  9.13466128 6.         0.81085598 9.         0.18640376]. \t  -0.6345727275625916 \t -0.4405807029170717\n",
            "13     \t [ 3.21802975  4.52251367 10.          0.91931377  1.          0.12302035]. \t  -0.6342698285007368 \t -0.4405807029170717\n",
            "14     \t [ 3.00873322  0.14737527 12.          0.50222445 10.          0.34409645]. \t  -0.5715820551871522 \t -0.4405807029170717\n",
            "15     \t [ 8.82566401  9.52943351 11.          0.94131416 17.          0.11574263]. \t  -0.6318490142275969 \t -0.4405807029170717\n",
            "16     \t [ 9.86354567  3.83813535  9.          0.75924362 13.          0.24806309]. \t  -0.6316030166906611 \t -0.4405807029170717\n",
            "17     \t [6.63742541 3.16264573 5.         0.84308591 3.         0.31230827]. \t  -0.5897960965418152 \t -0.4405807029170717\n",
            "18     \t [ 0.39655743  9.88537948  7.          0.8395494  15.          0.83639512]. \t  -0.46556996770739073 \t -0.4405807029170717\n",
            "19     \t [0.86243549 8.92584837 5.         0.6740487  9.         0.55305648]. \t  -0.5632466130939408 \t -0.4405807029170717\n",
            "20     \t [ 9.94984248  2.37676311  9.          0.96621815 19.          0.48539202]. \t  -0.531856562647136 \t -0.4405807029170717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.047520592626375"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVSH6caUgyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9edd69d-c04a-4dc6-cbf9-a71eac7087ae"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 6\n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_zero_6 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_6 = dGPGO_zero(surrogate_zero_6, Acquisition_new(util_zero), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
        "zero_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_6 = zero_6.getResult()[0]\n",
        "params_zero_6['max_depth'] = int(params_zero_6['max_depth'])\n",
        "params_zero_6['min_child_weight'] = int(params_zero_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_zero_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_zero_6 = xgb.train(params_zero_6, dX_zero_train6)\n",
        "pred_zero_6 = model_zero_6.predict(dX_zero_test6)\n",
        "\n",
        "rmse_zero_6 = np.sqrt(mean_squared_error(pred_zero_6, y_test6))\n",
        "rmse_zero_6"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5719256944003751 \t -0.5405445954433028\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.6047098118480896 \t -0.5405445954433028\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5405445954433028 \t -0.5405445954433028\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5678490550489279 \t -0.5405445954433028\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6145131146454834 \t -0.5405445954433028\n",
            "1      \t [ 2.61343239  0.80193947  5.          0.83898129 13.          0.84644718]. \t  -0.5477332582915869 \t -0.5405445954433028\n",
            "2      \t [ 9.72322443  9.21177696  5.          0.87917074 11.          0.86681736]. \t  \u001b[92m-0.4929247454390112\u001b[0m \t -0.4929247454390112\n",
            "3      \t [ 6.6253488   3.09318222  6.          0.95687426 16.          0.13946484]. \t  -0.690368704953711 \t -0.4929247454390112\n",
            "4      \t [ 8.54451719  6.55901746 13.          0.7507391   5.          0.45426156]. \t  -0.6083435565147685 \t -0.4929247454390112\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6192420483468785 \t -0.4929247454390112\n",
            "6      \t [ 1.15162056  8.90964142  5.          0.51235759 16.          0.30257234]. \t  -0.6348710910727494 \t -0.4929247454390112\n",
            "7      \t [ 0.85068891  9.82901317 11.          0.8341869   1.          0.33880369]. \t  -0.6276360919465666 \t -0.4929247454390112\n",
            "8      \t [ 9.32420466  6.39616005 13.          0.93300527 17.          0.34904443]. \t  -0.618513635024325 \t -0.4929247454390112\n",
            "9      \t [ 2.5352414   9.31776001 11.          0.69913965  8.          0.81482819]. \t  -0.5386758711280498 \t -0.4929247454390112\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.6983911087025447 \t -0.4929247454390112\n",
            "11     \t [ 7.01659189  0.12368581 13.          0.73757646 10.          0.2486598 ]. \t  -0.6917483076985717 \t -0.4929247454390112\n",
            "12     \t [ 1.90877122  0.88669904 11.          0.72802974 19.          0.27519153]. \t  -0.6920469274142503 \t -0.4929247454390112\n",
            "13     \t [8.90989579 3.88913177 8.         0.86631672 9.         0.59686825]. \t  -0.5703702118029026 \t -0.4929247454390112\n",
            "14     \t [ 6.8359928   9.89182285  7.          0.83897916 17.          0.34169562]. \t  -0.618337528663939 \t -0.4929247454390112\n",
            "15     \t [ 3.68500147  6.04160102  8.          0.55594848 12.          0.2508545 ]. \t  -0.6923666983699771 \t -0.4929247454390112\n",
            "16     \t [ 0.05509359  8.13873516 11.          0.83199834 17.          0.87257216]. \t  \u001b[92m-0.4656680239098458\u001b[0m \t -0.4656680239098458\n",
            "17     \t [1.20916044 1.67996637 8.         0.95434584 8.         0.65713107]. \t  -0.560720771561226 \t -0.4656680239098458\n",
            "18     \t [3.01249996 9.72062503 5.         0.82932198 9.         0.65554665]. \t  -0.572026808612368 \t -0.4656680239098458\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.537139253301325 \t -0.4656680239098458\n",
            "20     \t [ 1.59392983  0.4187331   5.          0.53325118 19.          0.2829738 ]. \t  -0.6911833881800746 \t -0.4656680239098458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.934873443584281"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WsphKSUj19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55630dbb-f0a6-4246-dfef-59a68a26d653"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 7\n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_zero_7 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_7 = dGPGO_zero(surrogate_zero_7, Acquisition_new(util_zero), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
        "zero_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_7 = zero_7.getResult()[0]\n",
        "params_zero_7['max_depth'] = int(params_zero_7['max_depth'])\n",
        "params_zero_7['min_child_weight'] = int(params_zero_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_zero_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_zero_7 = xgb.train(params_zero_7, dX_zero_train7)\n",
        "pred_zero_7 = model_zero_7.predict(dX_zero_test7)\n",
        "\n",
        "rmse_zero_7 = np.sqrt(mean_squared_error(pred_zero_7, y_test7))\n",
        "rmse_zero_7"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.76308289 7.79918792 8.         0.98911145 8.         0.98019056]. \t  -0.44500885348659536 \t -0.44173641078261416\n",
            "init   \t [ 5.3849587   5.01120464 13.          0.74994125  5.          0.88192131]. \t  -0.4488374676936292 \t -0.44173641078261416\n",
            "init   \t [ 3.30839249  3.9294231  12.          0.6440728  13.          0.41137564]. \t  -0.5669653799025498 \t -0.44173641078261416\n",
            "init   \t [9.29528191 2.6258377  5.         0.80027446 1.         0.86616513]. \t  -0.4661284696195417 \t -0.44173641078261416\n",
            "init   \t [ 1.74052764  7.90763512 14.          0.7244129   4.          0.77536887]. \t  -0.44173641078261416 \t -0.44173641078261416\n",
            "1      \t [3.43305102 3.00339076 8.         0.71322679 4.         0.33322219]. \t  -0.5720117286339644 \t -0.44173641078261416\n",
            "2      \t [ 7.6343627   1.31181598  5.          0.5769645  12.          0.84874959]. \t  -0.48924755963876193 \t -0.44173641078261416\n",
            "3      \t [ 9.12127254  9.64651695 14.          0.53624962  1.          0.38247449]. \t  -0.5784219709012162 \t -0.44173641078261416\n",
            "4      \t [ 4.51243396  9.79601217  8.          0.69773915 19.          0.69687222]. \t  -0.48670271968935647 \t -0.44173641078261416\n",
            "5      \t [ 8.06748781  9.6311716   5.          0.89165168 11.          0.90769253]. \t  -0.47467849504450826 \t -0.44173641078261416\n",
            "6      \t [ 9.84853722  9.76587477 12.          0.66808012 15.          0.83895675]. \t  -0.464300180546447 \t -0.44173641078261416\n",
            "7      \t [6.4915356  8.69600226 5.         0.66481282 2.         0.54976375]. \t  -0.5200045453863744 \t -0.44173641078261416\n",
            "8      \t [ 0.86712134  2.53401598  5.          0.75823741 18.          0.7884932 ]. \t  -0.4803186810834383 \t -0.44173641078261416\n",
            "9      \t [ 1.29932493  8.0055142  14.          0.51325501 19.          0.83654101]. \t  -0.4650670910068424 \t -0.44173641078261416\n",
            "10     \t [ 8.667653    2.8973594  14.          0.53825633 18.          0.10643456]. \t  -0.6861760180900373 \t -0.44173641078261416\n",
            "11     \t [ 9.89504759  2.29066357 10.          0.72562459  8.          0.65324681]. \t  -0.4791774660247169 \t -0.44173641078261416\n",
            "12     \t [ 9.94824081  0.39149062 12.          0.59113076  1.          0.49709751]. \t  -0.5081268521688408 \t -0.44173641078261416\n",
            "13     \t [ 9.60964399  8.52338044  5.          0.92796141 17.          0.69277707]. \t  -0.49365990657681785 \t -0.44173641078261416\n",
            "14     \t [ 3.34596156  0.67230605 11.          0.7366642  19.          0.29949997]. \t  -0.5628067831028366 \t -0.44173641078261416\n",
            "15     \t [ 9.60816545  8.33912452 11.          0.58307044  8.          0.36972424]. \t  -0.5786370523858145 \t -0.44173641078261416\n",
            "16     \t [ 0.47345619  1.31956961  7.          0.68987164 11.          0.9514247 ]. \t  -0.466265051270997 \t -0.44173641078261416\n",
            "17     \t [ 8.96081021  0.38413512 14.          0.90624871 12.          0.41560009]. \t  -0.565007973216504 \t -0.44173641078261416\n",
            "18     \t [ 9.71411962  1.71380043  5.          0.50726337 19.          0.71982498]. \t  -0.4912391351239423 \t -0.44173641078261416\n",
            "19     \t [ 0.18764716  0.58029521 13.          0.75995865  6.          0.69134836]. \t  -0.46855437085127594 \t -0.44173641078261416\n",
            "20     \t [ 1.77048754  9.0838881   6.          0.98375355 14.          0.13025417]. \t  -0.6815881776266557 \t -0.44173641078261416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.499924380514474"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI8sFP4ZUmOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6975d1c6-89c1-4be0-ba77-e3ec68ac17da"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 8\n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_zero_8 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_8 = dGPGO_zero(surrogate_zero_8, Acquisition_new(util_zero), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
        "zero_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_8 = zero_8.getResult()[0]\n",
        "params_zero_8['max_depth'] = int(params_zero_8['max_depth'])\n",
        "params_zero_8['min_child_weight'] = int(params_zero_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_zero_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_zero_8 = xgb.train(params_zero_8, dX_zero_train8)\n",
        "pred_zero_8 = model_zero_8.predict(dX_zero_test8)\n",
        "\n",
        "rmse_zero_8 = np.sqrt(mean_squared_error(pred_zero_8, y_test8))\n",
        "rmse_zero_8"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.5450023023990902 \t -0.47785117417083445\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.48415390639601685 \t -0.47785117417083445\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5514374023096014 \t -0.47785117417083445\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.509390123714371 \t -0.47785117417083445\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.47785117417083445 \t -0.47785117417083445\n",
            "1      \t [4.42022545 5.48487111 9.         0.97165909 3.         0.63617522]. \t  -0.4933129789895931 \t -0.47785117417083445\n",
            "2      \t [ 9.3432851   3.80536023 13.          0.82203895 19.          0.99569116]. \t  -0.4825289719254064 \t -0.47785117417083445\n",
            "3      \t [ 2.52429836  9.02824683 14.          0.59641093 17.          0.61934886]. \t  -0.5047477495526641 \t -0.47785117417083445\n",
            "4      \t [ 9.53473907  5.08424998 11.          0.50652828 18.          0.67121466]. \t  -0.5164144636241208 \t -0.47785117417083445\n",
            "5      \t [6.89072012 1.88822945 5.         0.9252956  8.         0.40577637]. \t  -0.5624845374478475 \t -0.47785117417083445\n",
            "6      \t [ 2.42575645  9.87357367  5.          0.61143882 19.          0.11833201]. \t  -0.6358778400175291 \t -0.47785117417083445\n",
            "7      \t [ 0.9931658   1.40870497 14.          0.50614818  6.          0.71944448]. \t  -0.48485914970894306 \t -0.47785117417083445\n",
            "8      \t [ 9.09148899  1.42093493  5.          0.88801001 17.          0.53231573]. \t  -0.5546570042431702 \t -0.47785117417083445\n",
            "9      \t [9.69554908 3.70013633 5.         0.72727157 1.         0.49530336]. \t  -0.5595421515943506 \t -0.47785117417083445\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.558869771245836 \t -0.47785117417083445\n",
            "11     \t [1.66316873 0.47469495 7.         0.60820298 1.         0.50689458]. \t  -0.5436576355709689 \t -0.47785117417083445\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.5062409032591381 \t -0.47785117417083445\n",
            "13     \t [5.63420638 9.7026496  5.         0.64869539 8.         0.22475566]. \t  -0.6389332388103492 \t -0.47785117417083445\n",
            "14     \t [ 0.63194856  9.08760061 12.          0.51277257  1.          0.85579144]. \t  -0.4885511515989219 \t -0.47785117417083445\n",
            "15     \t [ 8.35277365  5.17929354 14.          0.67305838  1.          0.70101595]. \t  -0.5154357312421961 \t -0.47785117417083445\n",
            "16     \t [ 8.62413803  1.63568526 10.          0.78670071 12.          0.5099252 ]. \t  -0.5367709050823309 \t -0.47785117417083445\n",
            "17     \t [ 3.5121134   5.24088616  9.          0.68973249 19.          0.78159968]. \t  -0.4838024291467368 \t -0.47785117417083445\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.6380286876071068 \t -0.47785117417083445\n",
            "19     \t [ 3.45271174  0.75931377  5.          0.81313484 19.          0.57119025]. \t  -0.5573456183368956 \t -0.47785117417083445\n",
            "20     \t [ 7.79664999  9.9555277   7.          0.88786243 19.          0.48083734]. \t  -0.5413755664817078 \t -0.47785117417083445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.797573101898328"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5IYus6UpAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998d0203-2c2f-4201-b4ac-2802a7850a5c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 9\n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_zero_9 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_9 = dGPGO_zero(surrogate_zero_9, Acquisition_new(util_zero), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
        "zero_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_9 = zero_9.getResult()[0]\n",
        "params_zero_9['max_depth'] = int(params_zero_9['max_depth'])\n",
        "params_zero_9['min_child_weight'] = int(params_zero_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_zero_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_zero_9 = xgb.train(params_zero_9, dX_zero_train9)\n",
        "pred_zero_9 = model_zero_9.predict(dX_zero_test9)\n",
        "\n",
        "rmse_zero_9 = np.sqrt(mean_squared_error(pred_zero_9, y_test9))\n",
        "rmse_zero_9"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.10374154  5.01874592 11.          0.50377155  2.          0.29670281]. \t  -0.6545930802207814 \t -0.4584168030068045\n",
            "init   \t [ 4.18508181  2.48101168 13.          0.69794293  2.          0.25009871]. \t  -0.7166132091943936 \t -0.4584168030068045\n",
            "init   \t [ 8.78559086  9.50964032 13.          0.98395204 11.          0.90820641]. \t  -0.4584168030068045 \t -0.4584168030068045\n",
            "init   \t [ 6.66898973  5.47837783  6.          0.97165345 12.          0.72499481]. \t  -0.48839211091816903 \t -0.4584168030068045\n",
            "init   \t [ 8.24870465  4.65668475 13.          0.68760467  9.          0.98502332]. \t  -0.46354466019784824 \t -0.4584168030068045\n",
            "1      \t [6.73714319 2.39608167 5.         0.58130302 3.         0.163077  ]. \t  -0.7145926373770018 \t -0.4584168030068045\n",
            "2      \t [ 4.24955662  9.67331527 12.          0.64012695  7.          0.96617478]. \t  -0.4599552738922936 \t -0.4584168030068045\n",
            "3      \t [ 3.60566534  9.79805332 11.          0.62032576 16.          0.3578496 ]. \t  -0.6301914521165797 \t -0.4584168030068045\n",
            "4      \t [ 4.86601509  0.61279594  8.          0.72162785 18.          0.68911833]. \t  -0.4897803511428914 \t -0.4584168030068045\n",
            "5      \t [ 0.42678797  0.40430921 14.          0.96202194 10.          0.10119674]. \t  -0.7201378581044144 \t -0.4584168030068045\n",
            "6      \t [1.13863488 5.86180669 5.         0.9953054  3.         0.88639082]. \t  -0.4613511892865009 \t -0.4584168030068045\n",
            "7      \t [9.20185355 9.40235017 8.         0.60433558 1.         0.94540222]. \t  -0.4678077722812364 \t -0.4584168030068045\n",
            "8      \t [ 7.07313313  8.94084339  5.          0.99439529 19.          0.33798274]. \t  -0.6320298567588886 \t -0.4584168030068045\n",
            "9      \t [ 0.32747031  4.15373137 11.          0.86073055 16.          0.59344779]. \t  -0.4847822875702831 \t -0.4584168030068045\n",
            "10     \t [ 9.89680111  6.16985579 13.          0.58428363 17.          0.77251451]. \t  -0.4901785694142925 \t -0.4584168030068045\n",
            "11     \t [ 0.89628785  3.68529458  7.          0.8923295  10.          0.50355901]. \t  -0.4926976357635584 \t -0.4584168030068045\n",
            "12     \t [ 8.88256183  1.86768316 10.          0.88865081 13.          0.70876109]. \t  -0.4907324671255294 \t -0.4584168030068045\n",
            "13     \t [ 4.04453993  0.29784113 14.          0.62598795 17.          0.38851381]. \t  -0.6317392704296504 \t -0.4584168030068045\n",
            "14     \t [ 0.80844375  8.9121894   5.          0.56134557 18.          0.84901701]. \t  -0.496220520490058 \t -0.4584168030068045\n",
            "15     \t [4.62485155 9.65182222 5.         0.58896427 8.         0.36022331]. \t  -0.6322698857670199 \t -0.4584168030068045\n",
            "16     \t [ 8.77945628  5.51694736 12.          0.84613589  4.          0.64741755]. \t  -0.4798840189273464 \t -0.4584168030068045\n",
            "17     \t [ 3.89448326  9.77793126 11.          0.87312755  1.          0.25420157]. \t  -0.7204829959803113 \t -0.4584168030068045\n",
            "18     \t [9.96461831 1.05025842 5.         0.85654396 9.         0.88077949]. \t  -0.47633394276149926 \t -0.4584168030068045\n",
            "19     \t [ 2.87706677  6.16957719 14.          0.55246385 12.          0.92866915]. \t  -0.4653617556003965 \t -0.4584168030068045\n",
            "20     \t [ 2.2888102   0.47311819 10.          0.52676226  6.          0.86310876]. \t  -0.46328602169921157 \t -0.4584168030068045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6575353004618085"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD494io_Ur7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f06510-6a77-497d-ca6c-1d1cdb908fb1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 10\n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_zero_10 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_10 = dGPGO_zero(surrogate_zero_10, Acquisition_new(util_zero), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
        "zero_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_10 = zero_10.getResult()[0]\n",
        "params_zero_10['max_depth'] = int(params_zero_10['max_depth'])\n",
        "params_zero_10['min_child_weight'] = int(params_zero_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_zero_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_zero_10 = xgb.train(params_zero_10, dX_zero_train10)\n",
        "pred_zero_10 = model_zero_10.predict(dX_zero_test10)\n",
        "\n",
        "rmse_zero_10 = np.sqrt(mean_squared_error(pred_zero_10, y_test10))\n",
        "rmse_zero_10"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.71320643  0.20751949  5.          0.72150747 17.          0.12265456]. \t  -0.7090674967614334 \t -0.4737745634473992\n",
            "init   \t [ 7.0920801   2.65566127 13.          0.57518893 17.          0.83494165]. \t  -0.4737745634473992 \t -0.4737745634473992\n",
            "init   \t [ 3.36071584  8.90816531  6.          0.86087766 15.          0.75469196]. \t  -0.4755277191484213 \t -0.4737745634473992\n",
            "init   \t [ 5.40880931  1.31458152  8.          0.57108502 14.          0.62551123]. \t  -0.48811859212530173 \t -0.4737745634473992\n",
            "init   \t [1.82631436 8.26082248 6.         0.80888349 5.         0.15900694]. \t  -0.7057210222477256 \t -0.4737745634473992\n",
            "1      \t [8.31989768 3.09778055 7.         0.64798085 3.         0.98471878]. \t  \u001b[92m-0.46336171949490257\u001b[0m \t -0.46336171949490257\n",
            "2      \t [ 1.51483713  6.46720195 14.          0.87676044  8.          0.10934204]. \t  -0.7062921047588426 \t -0.46336171949490257\n",
            "3      \t [ 0.44494294  2.20797313 10.          0.76097539  2.          0.34290111]. \t  -0.6180922534763184 \t -0.46336171949490257\n",
            "4      \t [ 6.47425096  8.4482791  10.          0.69239539 11.          0.47622913]. \t  -0.5685254207537167 \t -0.46336171949490257\n",
            "5      \t [ 0.20896963  7.17600684 13.          0.63836859 16.          0.77244006]. \t  \u001b[92m-0.45403228727774253\u001b[0m \t -0.45403228727774253\n",
            "6      \t [ 9.38854854  7.91087361  8.          0.57475286 19.          0.33969987]. \t  -0.6147666257838221 \t -0.45403228727774253\n",
            "7      \t [ 9.16520307  0.72602801 12.          0.91999471  9.          0.54336218]. \t  -0.5572402945816037 \t -0.45403228727774253\n",
            "8      \t [ 8.33810851  9.8990204  14.          0.61893039  5.          0.69227045]. \t  -0.4851208634097359 \t -0.45403228727774253\n",
            "9      \t [ 0.12250572  0.33829451 12.          0.86512188 11.          0.46912604]. \t  -0.5583881911050124 \t -0.45403228727774253\n",
            "10     \t [ 0.8556149   1.34495008  5.          0.88655293 18.          0.15072152]. \t  -0.7048793212420641 \t -0.45403228727774253\n",
            "11     \t [9.53733075 9.92797623 5.         0.78642342 2.         0.9933049 ]. \t  -0.4656240229085077 \t -0.45403228727774253\n",
            "12     \t [1.69575137 2.28628662 5.         0.76046389 8.         0.60065268]. \t  -0.5134648983420089 \t -0.45403228727774253\n",
            "13     \t [ 6.15733989  0.33396443 13.          0.69796716  1.          0.92875212]. \t  \u001b[92m-0.4397628730770955\u001b[0m \t -0.4397628730770955\n",
            "14     \t [ 9.08727548  7.95071568  5.          0.91034544 13.          0.55647706]. \t  -0.5735578706821028 \t -0.4397628730770955\n",
            "15     \t [ 1.45996834  0.59889716 12.          0.82815203 19.          0.24922559]. \t  -0.7069273813967614 \t -0.4397628730770955\n",
            "16     \t [ 9.42421731  1.45800675  5.          0.89810349 11.          0.91467377]. \t  -0.4712218735320602 \t -0.4397628730770955\n",
            "17     \t [ 2.80210285  9.24943163 14.          0.85818339  2.          0.85428456]. \t  \u001b[92m-0.4387844304056118\u001b[0m \t -0.4387844304056118\n",
            "18     \t [7.60796195 7.8170794  5.         0.85145938 8.         0.57832563]. \t  -0.5106716993417335 \t -0.4387844304056118\n",
            "19     \t [ 8.10151611  9.78827602 14.          0.68048433 19.          0.19392154]. \t  -0.7116333725331636 \t -0.4387844304056118\n",
            "20     \t [ 1.11903492  9.93958695  9.          0.79898321 10.          0.44087381]. \t  -0.5625838427005065 \t -0.4387844304056118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.521324504618526"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03Sq0TvUuhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d914d85-515b-4359-e775-d97f685ac4a2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 11\n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_zero_11 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_11 = dGPGO_zero(surrogate_zero_11, Acquisition_new(util_zero), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
        "zero_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_11 = zero_11.getResult()[0]\n",
        "params_zero_11['max_depth'] = int(params_zero_11['max_depth'])\n",
        "params_zero_11['min_child_weight'] = int(params_zero_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_zero_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_zero_11 = xgb.train(params_zero_11, dX_zero_train11)\n",
        "pred_zero_11 = model_zero_11.predict(dX_zero_test11)\n",
        "\n",
        "rmse_zero_11 = np.sqrt(mean_squared_error(pred_zero_11, y_test11))\n",
        "rmse_zero_11"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.80269689  0.19475241  6.          0.59705781 13.          0.47818324]. \t  -0.5922349044250168 \t -0.49898623219170346\n",
            "init   \t [ 4.85427098  0.12780815  5.          0.91309068 14.          0.86571558]. \t  -0.49898623219170346 \t -0.49898623219170346\n",
            "init   \t [ 7.2996447   1.08736072 10.          0.92857712 18.          0.66910061]. \t  -0.5404544349803458 \t -0.49898623219170346\n",
            "init   \t [ 0.20483613  1.16737269  7.          0.57895615 16.          0.83644782]. \t  -0.5108833748715963 \t -0.49898623219170346\n",
            "init   \t [ 3.44624491  3.18798797 14.          0.54197657 15.          0.63958906]. \t  -0.5545314939891337 \t -0.49898623219170346\n",
            "1      \t [9.77136617 6.6548802  7.         0.51036649 9.         0.81011527]. \t  -0.5252491147925742 \t -0.49898623219170346\n",
            "2      \t [0.5279662  8.15331655 5.         0.83127487 9.         0.53242685]. \t  -0.5931601503405173 \t -0.49898623219170346\n",
            "3      \t [8.62555756 1.5478147  8.         0.99964468 2.         0.74874718]. \t  \u001b[92m-0.490360342610192\u001b[0m \t -0.490360342610192\n",
            "4      \t [ 0.90299561  9.42808632 14.          0.71344248  9.          0.5250902 ]. \t  -0.5778138976577691 \t -0.490360342610192\n",
            "5      \t [ 5.37271973  6.74878506 12.          0.69507758  3.          0.34109729]. \t  -0.5771395586534209 \t -0.490360342610192\n",
            "6      \t [ 2.80563958  8.51387637  8.          0.72474118 19.          0.16647242]. \t  -0.6892058127918498 \t -0.490360342610192\n",
            "7      \t [ 0.77265856  1.6177329  13.          0.54135345  6.          0.64546795]. \t  -0.5525358359299481 \t -0.490360342610192\n",
            "8      \t [0.47065357 6.80536856 5.         0.94482943 1.         0.93677618]. \t  -0.5032158363867651 \t -0.490360342610192\n",
            "9      \t [ 8.74582539  1.09960491 14.          0.69981761  9.          0.73083826]. \t  -0.4904447717887598 \t -0.490360342610192\n",
            "10     \t [ 9.62795963  5.53773092  5.          0.74613073 18.          0.44845649]. \t  -0.6039998092641087 \t -0.490360342610192\n",
            "11     \t [ 8.61765552  0.94349031 10.          0.85641829  4.          0.91576259]. \t  \u001b[92m-0.47852786064513103\u001b[0m \t -0.47852786064513103\n",
            "12     \t [3.50049766 0.06230783 7.         0.82711607 1.         0.94515625]. \t  -0.4818438762403437 \t -0.47852786064513103\n",
            "13     \t [ 9.79448361  9.59732472 14.          0.70844799  8.          0.1372886 ]. \t  -0.689949911392042 \t -0.47852786064513103\n",
            "14     \t [ 8.40730244  5.3235921  14.          0.57824885  9.          0.96689527]. \t  -0.4867986869106529 \t -0.47852786064513103\n",
            "15     \t [ 8.25290284  8.43147733 13.          0.56283859 16.          0.39976527]. \t  -0.5937125898059817 \t -0.47852786064513103\n",
            "16     \t [ 3.71272268  4.57338191 11.          0.7134411  10.          0.83979859]. \t  -0.4880121906758131 \t -0.47852786064513103\n",
            "17     \t [5.79495589 8.8757833  5.         0.90293876 5.         0.35892341]. \t  -0.5976299536228694 \t -0.47852786064513103\n",
            "18     \t [ 6.19006075  3.41925871 11.          0.95286331  6.          0.39133224]. \t  -0.5745107435758626 \t -0.47852786064513103\n",
            "19     \t [0.77256475 1.22732493 7.         0.97692145 7.         0.9071396 ]. \t  \u001b[92m-0.4728689341077363\u001b[0m \t -0.4728689341077363\n",
            "20     \t [7.46566309 0.73949694 7.         0.58224414 9.         0.25972328]. \t  -0.6904564119816002 \t -0.4728689341077363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.084919441252919"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_nP9lQjUztV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f028f30e-4f56-4944-fe6b-204ed75b7c91"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_zero_12 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_12 = dGPGO_zero(surrogate_zero_12, Acquisition_new(util_zero), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
        "zero_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_12 = zero_12.getResult()[0]\n",
        "params_zero_12['max_depth'] = int(params_zero_12['max_depth'])\n",
        "params_zero_12['min_child_weight'] = int(params_zero_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_zero_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_zero_12 = xgb.train(params_zero_12, dX_zero_train12)\n",
        "pred_zero_12 = model_zero_12.predict(dX_zero_test12)\n",
        "\n",
        "rmse_zero_12 = np.sqrt(mean_squared_error(pred_zero_12, y_test12))\n",
        "rmse_zero_12"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.54162842 7.40049697 6.         0.54321714 4.         0.11311747]. \t  -0.6840535846029854 \t -0.5032799564384677\n",
            "init   \t [ 9.18747008  9.00714854 14.          0.97847467 11.          0.35544552]. \t  -0.6305456734924068 \t -0.5032799564384677\n",
            "init   \t [ 6.06083184  9.44225136 14.          0.95626942  5.          0.56910342]. \t  -0.6144010633484512 \t -0.5032799564384677\n",
            "init   \t [ 5.52037633  4.85377414  7.          0.97886436 17.          0.78810441]. \t  -0.5032799564384677 \t -0.5032799564384677\n",
            "init   \t [ 0.20809798  1.35210178  5.          0.65494879 16.          0.36062811]. \t  -0.6516977337723864 \t -0.5032799564384677\n",
            "1      \t [9.46555822 8.57190559 5.         0.50164398 5.         0.71992807]. \t  -0.5227768283895782 \t -0.5032799564384677\n",
            "2      \t [9.04256367 2.61736915 8.         0.66026854 8.         0.14510453]. \t  -0.6776847411315782 \t -0.5032799564384677\n",
            "3      \t [6.03751892 2.08855857 8.         0.88966175 1.         0.6215545 ]. \t  -0.537741041779077 \t -0.5032799564384677\n",
            "4      \t [ 0.24796255  2.18203944 14.          0.56497025 17.          0.63132662]. \t  -0.5478949507371567 \t -0.5032799564384677\n",
            "5      \t [ 1.93384153  7.13950146  8.          0.85480597 18.          0.33734734]. \t  -0.6395317281169034 \t -0.5032799564384677\n",
            "6      \t [ 0.40359854  2.22527636 10.          0.60258213 10.          0.38255809]. \t  -0.6389275427144069 \t -0.5032799564384677\n",
            "7      \t [ 0.28427394  4.67296732 14.          0.84909523  4.          0.99176009]. \t  \u001b[92m-0.46369522867609564\u001b[0m \t -0.46369522867609564\n",
            "8      \t [ 7.63658847  0.39719075 14.          0.96199388 14.          0.84093877]. \t  -0.48258324270646796 \t -0.46369522867609564\n",
            "9      \t [ 0.50213582  8.87075    12.          0.71856843 12.          0.66120412]. \t  -0.5367112816028642 \t -0.46369522867609564\n",
            "10     \t [ 4.27921374  9.2199845   6.          0.67076861 12.          0.56605459]. \t  -0.628155564863435 \t -0.46369522867609564\n",
            "11     \t [ 7.63578483  4.07501457 14.          0.97723751  1.          0.2033266 ]. \t  -0.6773954967709924 \t -0.46369522867609564\n",
            "12     \t [ 9.06259994  1.4172751   5.          0.81347212 19.          0.38881712]. \t  -0.650363142837499 \t -0.46369522867609564\n",
            "13     \t [ 9.38461996  5.62749581 12.          0.98689844 17.          0.89710846]. \t  -0.4762695359794075 \t -0.46369522867609564\n",
            "14     \t [ 4.55964005  0.30434123  5.          0.52399968 11.          0.14256504]. \t  -0.6841482061412913 \t -0.46369522867609564\n",
            "15     \t [ 6.59943135  1.71178305 14.          0.69283152  8.          0.30459736]. \t  -0.635622313137239 \t -0.46369522867609564\n",
            "16     \t [5.99294447 7.0114806  5.         0.86352024 2.         0.47030879]. \t  -0.629371597372417 \t -0.46369522867609564\n",
            "17     \t [ 9.63630565  6.83547158  8.          0.6195521  13.          0.11712353]. \t  -0.6780822711639433 \t -0.46369522867609564\n",
            "18     \t [0.93465239 0.11560545 5.         0.52904926 2.         0.78190279]. \t  -0.5147104266754287 \t -0.46369522867609564\n",
            "19     \t [ 4.02953989  6.35914994 11.          0.73063178  8.          0.53793685]. \t  -0.6140059235484745 \t -0.46369522867609564\n",
            "20     \t [ 0.49442426  9.20422434 11.          0.70522852  1.          0.50168224]. \t  -0.62709850175482 \t -0.46369522867609564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.548920198104285"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDI2Bi9vU05U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd28ad7-2269-49d3-c914-644f137e8b23"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 13\n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_zero_13 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_13 = dGPGO_zero(surrogate_zero_13, Acquisition_new(util_zero), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
        "zero_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_13 = zero_13.getResult()[0]\n",
        "params_zero_13['max_depth'] = int(params_zero_13['max_depth'])\n",
        "params_zero_13['min_child_weight'] = int(params_zero_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_zero_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_zero_13 = xgb.train(params_zero_13, dX_zero_train13)\n",
        "pred_zero_13 = model_zero_13.predict(dX_zero_test13)\n",
        "\n",
        "rmse_zero_13 = np.sqrt(mean_squared_error(pred_zero_13, y_test13))\n",
        "rmse_zero_13"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.77702411  2.3754122  11.          0.94649135 13.          0.7827256 ]. \t  -0.5099204187421568 \t -0.5099204187421568\n",
            "init   \t [ 7.51661514  6.07343344 11.          0.69402149 11.          0.13153287]. \t  -0.7086153631136594 \t -0.5099204187421568\n",
            "init   \t [ 2.98449471  0.58512492 10.          0.73579614 12.          0.33065195]. \t  -0.6372461781857162 \t -0.5099204187421568\n",
            "init   \t [ 3.47581215  0.0941277  11.          0.86143432  8.          0.58454932]. \t  -0.5702017504451442 \t -0.5099204187421568\n",
            "init   \t [ 4.70137857  6.24432527 10.          0.8149145  18.          0.10784416]. \t  -0.7101125715665313 \t -0.5099204187421568\n",
            "1      \t [1.1119361  5.43221306 6.         0.56899303 8.         0.32100319]. \t  -0.6493427493033831 \t -0.5099204187421568\n",
            "2      \t [5.39023698 3.80105709 8.         0.54170057 1.         0.56798884]. \t  -0.621866591058205 \t -0.5099204187421568\n",
            "3      \t [ 0.5185863   5.23876151 13.          0.63798348  5.          0.7914799 ]. \t  \u001b[92m-0.5090480969691882\u001b[0m \t -0.5090480969691882\n",
            "4      \t [ 9.65518672  0.13040633  6.          0.63296628 18.          0.44133279]. \t  -0.632314879589983 \t -0.5090480969691882\n",
            "5      \t [ 9.80722669  7.22571611  7.          0.5026908  19.          0.92519376]. \t  -0.5113851690128051 \t -0.5090480969691882\n",
            "6      \t [ 6.75965929  9.42320667 14.          0.75932127  4.          0.51195623]. \t  -0.6206028595311828 \t -0.5090480969691882\n",
            "7      \t [9.95671825 0.88335607 5.         0.76593799 7.         0.60049246]. \t  -0.5952873237754599 \t -0.5090480969691882\n",
            "8      \t [ 1.88898055  9.92199995 14.          0.53783103 12.          0.63359705]. \t  -0.5738907345204808 \t -0.5090480969691882\n",
            "9      \t [0.32121091 9.03384774 6.         0.79493663 1.         0.29330571]. \t  -0.6544818879601484 \t -0.5090480969691882\n",
            "10     \t [ 9.64211232  3.05396831 11.          0.80784352  5.          0.67910498]. \t  -0.5747201434527256 \t -0.5090480969691882\n",
            "11     \t [ 1.60018805  0.54211528  7.          0.80910607 18.          0.55232873]. \t  -0.6209454018308087 \t -0.5090480969691882\n",
            "12     \t [ 6.1829314   9.53799326  5.          0.66916589 13.          0.57013155]. \t  -0.6295351865431291 \t -0.5090480969691882\n",
            "13     \t [7.99022981 8.23715587 5.         0.79172469 5.         0.39852784]. \t  -0.6543343020941466 \t -0.5090480969691882\n",
            "14     \t [ 8.15066897  1.98276445 13.          0.64083395 19.          0.16153982]. \t  -0.7124294948180114 \t -0.5090480969691882\n",
            "15     \t [ 2.48128047  9.70146472  6.          0.59664322 19.          0.9781653 ]. \t  -0.5140838980696801 \t -0.5090480969691882\n",
            "16     \t [ 9.08793394  9.83715934 14.          0.60607659 14.          0.5136415 ]. \t  -0.6216172534981154 \t -0.5090480969691882\n",
            "17     \t [ 1.80857777  4.26197    14.          0.84061579 16.          0.40224246]. \t  -0.637460469665298 \t -0.5090480969691882\n",
            "18     \t [ 5.1333583   3.10658752 14.          0.74854361  1.          0.92539511]. \t  \u001b[92m-0.4793091157473782\u001b[0m \t -0.4793091157473782\n",
            "19     \t [ 0.65009462  9.91088996 13.          0.92448902  2.          0.37838494]. \t  -0.6501362550952002 \t -0.4793091157473782\n",
            "20     \t [ 0.18073363  6.71350851  7.          0.80658597 14.          0.6264676 ]. \t  -0.5752866688439547 \t -0.4793091157473782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.601162573826364"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2F_Q194U3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6aeb97-20eb-4e66-e039-198c8d56899f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 14\n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_zero_14 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_14 = dGPGO_zero(surrogate_zero_14, Acquisition_new(util_zero), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
        "zero_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_14 = zero_14.getResult()[0]\n",
        "params_zero_14['max_depth'] = int(params_zero_14['max_depth'])\n",
        "params_zero_14['min_child_weight'] = int(params_zero_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_zero_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_zero_14 = xgb.train(params_zero_14, dX_zero_train14)\n",
        "pred_zero_14 = model_zero_14.predict(dX_zero_test14)\n",
        "\n",
        "rmse_zero_14 = np.sqrt(mean_squared_error(pred_zero_14, y_test14))\n",
        "rmse_zero_14"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.13943344  7.73165052 12.          0.6831412  11.          0.37876233]. \t  -0.558794499921046 \t -0.4448140077853998\n",
            "init   \t [ 9.57603739  5.13116712 14.          0.76959997 12.          0.71328228]. \t  -0.49979629433789113 \t -0.4448140077853998\n",
            "init   \t [5.34950319 2.47493539 5.         0.50293689 6.         0.29706373]. \t  -0.5741697988899073 \t -0.4448140077853998\n",
            "init   \t [ 2.94506579  3.45329697  8.          0.87620946 14.          0.9783044 ]. \t  -0.4448140077853998 \t -0.4448140077853998\n",
            "init   \t [ 1.11811929  1.73004086  5.          0.73745288 12.          0.20586008]. \t  -0.6214151152359092 \t -0.4448140077853998\n",
            "1      \t [ 6.50637223  2.67617722 14.          0.53562507  1.          0.16862152]. \t  -0.6294794339238933 \t -0.4448140077853998\n",
            "2      \t [ 9.97732733  0.9008687  13.          0.65397817 19.          0.96533011]. \t  -0.46274298441446626 \t -0.4448140077853998\n",
            "3      \t [ 0.28409124  4.13353348 13.          0.96339983  6.          0.90100709]. \t  \u001b[92m-0.42439805248956936\u001b[0m \t -0.42439805248956936\n",
            "4      \t [9.32373648 9.05676215 9.         0.53064322 3.         0.70657534]. \t  -0.5066999638361388 \t -0.42439805248956936\n",
            "5      \t [ 9.52454394  8.82757271  9.          0.90064956 19.          0.16022914]. \t  -0.6201852747998959 \t -0.42439805248956936\n",
            "6      \t [ 5.21920054  9.35580917 14.          0.81835368  4.          0.54800317]. \t  -0.4907240663685837 \t -0.42439805248956936\n",
            "7      \t [ 0.9687803   2.15143442 14.          0.51651811 18.          0.56051657]. \t  -0.5069463513331254 \t -0.42439805248956936\n",
            "8      \t [ 9.40430013  0.15700131  7.          0.99940272 12.          0.43771306]. \t  -0.50114980828739 \t -0.42439805248956936\n",
            "9      \t [0.38652312 9.54840602 9.         0.51100563 1.         0.36720837]. \t  -0.5713884526711975 \t -0.42439805248956936\n",
            "10     \t [ 2.08494663  9.70581169 14.          0.64307382  3.          0.10613693]. \t  -0.6314022013205511 \t -0.42439805248956936\n",
            "11     \t [ 1.55781918  0.46142569 10.          0.80240433 16.          0.22604037]. \t  -0.6223833972775541 \t -0.42439805248956936\n",
            "12     \t [ 8.32471637  8.64868248  5.          0.80719895 12.          0.64183859]. \t  -0.5064508572748014 \t -0.42439805248956936\n",
            "13     \t [ 1.40044045  8.51657075 10.          0.9376679  19.          0.10378975]. \t  -0.619787592651992 \t -0.42439805248956936\n",
            "14     \t [ 6.67907222  1.16181929 11.          0.57892751  8.          0.43787261]. \t  -0.5089510029010434 \t -0.42439805248956936\n",
            "15     \t [ 0.4418527   4.49961454  5.          0.81022894 19.          0.68885116]. \t  -0.5043665497579306 \t -0.42439805248956936\n",
            "16     \t [3.08911491 9.05396381 5.         0.56731196 7.         0.30286489]. \t  -0.563723404842588 \t -0.42439805248956936\n",
            "17     \t [ 7.58744711  5.19042144  5.          0.89542819 17.          0.12960574]. \t  -0.6210679687986883 \t -0.42439805248956936\n",
            "18     \t [9.95902868 1.23916696 7.         0.60336262 2.         0.41962125]. \t  -0.5646831606854809 \t -0.42439805248956936\n",
            "19     \t [1.20508963 0.14448649 8.         0.50927505 2.         0.71549909]. \t  -0.48572060865355005 \t -0.42439805248956936\n",
            "20     \t [ 0.17539185  8.79567306  9.          0.56601724 13.          0.83976647]. \t  -0.48014414278881856 \t -0.42439805248956936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.562117790499774"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po5wImJaU6VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6990dcf9-b68a-45c8-a8d3-46278bc9a111"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 15\n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_zero_15 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_15 = dGPGO_zero(surrogate_zero_15, Acquisition_new(util_zero), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
        "zero_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_15 = zero_15.getResult()[0]\n",
        "params_zero_15['max_depth'] = int(params_zero_15['max_depth'])\n",
        "params_zero_15['min_child_weight'] = int(params_zero_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_zero_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_zero_15 = xgb.train(params_zero_15, dX_zero_train15)\n",
        "pred_zero_15 = model_zero_15.predict(dX_zero_test15)\n",
        "\n",
        "rmse_zero_15 = np.sqrt(mean_squared_error(pred_zero_15, y_test15))\n",
        "rmse_zero_15"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.48817697  1.78895925 12.          0.55549316  8.          0.93397854]. \t  -0.48943791400638287 \t -0.48943791400638287\n",
            "init   \t [ 0.24953032  8.22298097 12.          0.62494951 11.          0.12924598]. \t  -0.6992441679399787 \t -0.48943791400638287\n",
            "init   \t [ 5.02017228  5.50882771 11.          0.85295832 19.          0.13548008]. \t  -0.69945028775584 \t -0.48943791400638287\n",
            "init   \t [2.0023081  9.98543403 7.         0.6295772  2.         0.526127  ]. \t  -0.6264981748624211 \t -0.48943791400638287\n",
            "init   \t [ 5.09715306  9.45038417 11.          0.7388277  16.          0.22739973]. \t  -0.6964615643692806 \t -0.48943791400638287\n",
            "1      \t [ 0.29158961  4.9949242  12.          0.89124583  3.          0.67554049]. \t  -0.56524595824364 \t -0.48943791400638287\n",
            "2      \t [3.68214008 4.55748717 6.         0.60488381 8.         0.88973248]. \t  -0.494310985641296 \t -0.48943791400638287\n",
            "3      \t [9.75991344 6.15203198 6.         0.65490407 1.         0.73816291]. \t  -0.49903240523735964 \t -0.48943791400638287\n",
            "4      \t [ 0.04347405  0.16019908 11.          0.96902942  9.          0.40185268]. \t  -0.6876830293606538 \t -0.48943791400638287\n",
            "5      \t [ 6.65116837  8.16324548 14.          0.95750787  1.          0.74925927]. \t  \u001b[92m-0.4743330993644729\u001b[0m \t -0.4743330993644729\n",
            "6      \t [ 0.41861043  0.05076637  5.          0.78940316 12.          0.77619725]. \t  -0.5149313800202362 \t -0.4743330993644729\n",
            "7      \t [ 7.77234852  1.95889592  5.          0.55968406 18.          0.52767162]. \t  -0.6461164158806947 \t -0.4743330993644729\n",
            "8      \t [5.32061454 0.87905509 9.         0.65906978 2.         0.4571849 ]. \t  -0.622249884738296 \t -0.4743330993644729\n",
            "9      \t [ 0.53269319  3.10786722 14.          0.59617872 16.          0.1338107 ]. \t  -0.700992505050063 \t -0.4743330993644729\n",
            "10     \t [8.41830164 2.50834975 6.         0.52592121 5.         0.6895569 ]. \t  -0.5821070207728769 \t -0.4743330993644729\n",
            "11     \t [ 2.97928851  7.73905112  6.          0.87870551 18.          0.36796416]. \t  -0.6830838005681892 \t -0.4743330993644729\n",
            "12     \t [ 8.0973651   9.90166176  7.          0.50233761 10.          0.59371821]. \t  -0.5773798909552077 \t -0.4743330993644729\n",
            "13     \t [ 9.84854774  1.49237876 12.          0.97098098 16.          0.67156842]. \t  -0.5637627524330794 \t -0.4743330993644729\n",
            "14     \t [ 9.9193931   8.28526555 13.          0.70468818 12.          0.10333636]. \t  -0.6955701209494888 \t -0.4743330993644729\n",
            "15     \t [ 9.71215516  9.31848438 12.          0.64520711  3.          0.99268325]. \t  -0.4762564116777547 \t -0.4743330993644729\n",
            "16     \t [ 8.7735228   9.10842887  7.          0.87510926 19.          0.71606378]. \t  -0.5009626890975292 \t -0.4743330993644729\n",
            "17     \t [ 0.11138125  2.32868485  8.          0.89421542 19.          0.22148224]. \t  -0.6999445797700907 \t -0.4743330993644729\n",
            "18     \t [ 3.65460641  0.68117156 10.          0.81249189 14.          0.73959896]. \t  -0.48865722230769765 \t -0.4743330993644729\n",
            "19     \t [2.51586523 4.75024405 5.         0.73697691 1.         0.87048338]. \t  -0.48149378293622097 \t -0.4743330993644729\n",
            "20     \t [0.01993927 9.65913928 7.         0.55264739 8.         0.94506564]. \t  -0.49306942918418556 \t -0.4743330993644729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.557298493141957"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrAQN-pU9Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdcea6b-5b43-4588-b0a8-dd0d139a99d2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 16\n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_zero_16 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_16 = dGPGO_zero(surrogate_zero_16, Acquisition_new(util_zero), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
        "zero_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_16 = zero_16.getResult()[0]\n",
        "params_zero_16['max_depth'] = int(params_zero_16['max_depth'])\n",
        "params_zero_16['min_child_weight'] = int(params_zero_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_zero_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_zero_16 = xgb.train(params_zero_16, dX_zero_train16)\n",
        "pred_zero_16 = model_zero_16.predict(dX_zero_test16)\n",
        "\n",
        "rmse_zero_16 = np.sqrt(mean_squared_error(pred_zero_16, y_test16))\n",
        "rmse_zero_16"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.23291079 5.23163341 6.         0.65430839 5.         0.30077285]. \t  -0.6357813258069683 \t -0.6345701590947206\n",
            "init   \t [6.88726162 1.63731425 7.         0.97050543 2.         0.25392012]. \t  -0.7029752724132097 \t -0.6345701590947206\n",
            "init   \t [ 5.94328983  5.6393473   5.          0.67602695 19.          0.42538144]. \t  -0.6345701590947206 \t -0.6345701590947206\n",
            "init   \t [ 0.88741148  3.08148142 14.          0.56043938  9.          0.27515386]. \t  -0.7076230970293895 \t -0.6345701590947206\n",
            "init   \t [ 2.74631586  1.30996118 11.          0.52160786  8.          0.27956463]. \t  -0.7061563820165734 \t -0.6345701590947206\n",
            "1      \t [ 7.8937256   1.5972923  14.          0.61610774 17.          0.78739284]. \t  \u001b[92m-0.5498317738591506\u001b[0m \t -0.5498317738591506\n",
            "2      \t [ 9.01655783  8.21383177  9.          0.60772965 10.          0.9401803 ]. \t  \u001b[92m-0.4830822737254163\u001b[0m \t -0.4830822737254163\n",
            "3      \t [ 4.35132073  9.89698316 12.          0.94137984 16.          0.57741056]. \t  -0.5389659972108385 \t -0.4830822737254163\n",
            "4      \t [ 3.38377852  9.31285251 11.          0.88244942  1.          0.67627774]. \t  -0.5404220213450739 \t -0.4830822737254163\n",
            "5      \t [ 9.63904847  1.13624975 14.          0.67706869  2.          0.95236673]. \t  \u001b[92m-0.4641319096590556\u001b[0m \t -0.4641319096590556\n",
            "6      \t [ 0.19317903  0.6596816   6.          0.86233301 15.          0.41906313]. \t  -0.6308098904949025 \t -0.4641319096590556\n",
            "7      \t [ 3.86147645  9.68905939  5.          0.65198689 12.          0.68426055]. \t  -0.5557826482727706 \t -0.4641319096590556\n",
            "8      \t [ 1.22130867  0.64008351 12.          0.59515137 19.          0.65193522]. \t  -0.5486769508942775 \t -0.4641319096590556\n",
            "9      \t [8.31596139 8.08775071 5.         0.5698323  1.         0.11769544]. \t  -0.708927036050009 \t -0.4641319096590556\n",
            "10     \t [ 9.39421065  1.15238895 10.          0.86257926 10.          0.82976642]. \t  -0.5380808068994449 \t -0.4641319096590556\n",
            "11     \t [ 8.63952355  8.87914214 14.          0.57943185  3.          0.83429859]. \t  -0.5495434658820855 \t -0.4641319096590556\n",
            "12     \t [ 2.23523952  3.62733473 12.          0.51556206  2.          0.2036675 ]. \t  -0.7124687886931153 \t -0.4641319096590556\n",
            "13     \t [ 0.55900516  9.18229404  7.          0.95696256 17.          0.1903245 ]. \t  -0.701856248538269 \t -0.4641319096590556\n",
            "14     \t [ 5.94853197  4.92024227  5.          0.75769119 13.          0.52349427]. \t  -0.5762899773358159 \t -0.4641319096590556\n",
            "15     \t [ 3.22380477  8.8210053  12.          0.91827732  7.          0.1365336 ]. \t  -0.7049504170537606 \t -0.4641319096590556\n",
            "16     \t [ 9.80203814  5.08834263 14.          0.60567693  8.          0.92815593]. \t  -0.4763161418293736 \t -0.4641319096590556\n",
            "17     \t [ 0.27313091  7.4504504  11.          0.83119788 12.          0.13163591]. \t  -0.7048204941457863 \t -0.4641319096590556\n",
            "18     \t [ 4.68384527  1.49985833 10.          0.81523271 14.          0.8945648 ]. \t  -0.4703046715523559 \t -0.4641319096590556\n",
            "19     \t [ 9.51378666  7.51596488 11.          0.92606042 19.          0.78246721]. \t  -0.5394207848307663 \t -0.4641319096590556\n",
            "20     \t [ 9.8049394   5.64638797 12.          0.54117047 14.          0.36652185]. \t  -0.638525088069505 \t -0.4641319096590556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.641237803563213"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXelbcAVVCqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bcea218-858b-4648-b731-02fb01d781c4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 17\n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_zero_17 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_17 = dGPGO_zero(surrogate_zero_17, Acquisition_new(util_zero), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
        "zero_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_17 = zero_17.getResult()[0]\n",
        "params_zero_17['max_depth'] = int(params_zero_17['max_depth'])\n",
        "params_zero_17['min_child_weight'] = int(params_zero_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_zero_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_zero_17 = xgb.train(params_zero_17, dX_zero_train17)\n",
        "pred_zero_17 = model_zero_17.predict(dX_zero_test17)\n",
        "\n",
        "rmse_zero_17 = np.sqrt(mean_squared_error(pred_zero_17, y_test17))\n",
        "rmse_zero_17"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.94665003  5.30586756 11.          0.94443241 14.          0.80828691]. \t  -0.48092361225642916 \t -0.48092361225642916\n",
            "init   \t [ 6.56333522  6.37520896 12.          0.81487881 18.          0.42203224]. \t  -0.634455605137701 \t -0.48092361225642916\n",
            "init   \t [ 9.45683187  0.6004468  11.          0.5171566  10.          0.53881211]. \t  -0.6046684392629649 \t -0.48092361225642916\n",
            "init   \t [2.72705857 1.19063434 6.         0.74176431 6.         0.10101151]. \t  -0.6562801618178493 \t -0.48092361225642916\n",
            "init   \t [ 4.77631812  5.24671297 13.          0.66254476 19.          0.36708086]. \t  -0.6415104035419145 \t -0.48092361225642916\n",
            "1      \t [ 0.65702322  5.79284078 13.          0.75136902  1.          0.30306068]. \t  -0.6424064809093688 \t -0.48092361225642916\n",
            "2      \t [ 6.93446178  8.68032298 13.          0.78195789  7.          0.91906958]. \t  \u001b[92m-0.47529593119031155\u001b[0m \t -0.47529593119031155\n",
            "3      \t [9.72843652 3.88893279 9.         0.6901555  1.         0.31608219]. \t  -0.6407338037654788 \t -0.47529593119031155\n",
            "4      \t [ 9.65057736  8.52725784  5.          0.68420234 13.          0.40008732]. \t  -0.6446152251367371 \t -0.47529593119031155\n",
            "5      \t [ 4.97204887  2.40072226  5.          0.54268748 19.          0.30995407]. \t  -0.6473150344064219 \t -0.47529593119031155\n",
            "6      \t [0.12174033 8.73496008 5.         0.89827646 5.         0.85354798]. \t  -0.4993728869548043 \t -0.47529593119031155\n",
            "7      \t [ 2.91443079  0.16723755 13.          0.598201    6.          0.91729605]. \t  -0.48363736808522184 \t -0.47529593119031155\n",
            "8      \t [7.20615247 9.36901627 6.         0.85465034 7.         0.6878262 ]. \t  -0.5223074457605629 \t -0.47529593119031155\n",
            "9      \t [ 9.02586164  0.59354638 10.          0.86038693 18.          0.90794111]. \t  -0.48184301693459747 \t -0.47529593119031155\n",
            "10     \t [ 1.66641474  7.47633023  5.          0.68203645 17.          0.15512069]. \t  -0.6612773320145335 \t -0.47529593119031155\n",
            "11     \t [ 0.12410542  7.60180472 13.          0.97425003  8.          0.76245421]. \t  \u001b[92m-0.47195842108145214\u001b[0m \t -0.47195842108145214\n",
            "12     \t [ 6.90469319  3.60127194  5.          0.92830139 10.          0.47070468]. \t  -0.6006468235616991 \t -0.47195842108145214\n",
            "13     \t [5.87185819 9.07164719 8.         0.7145923  1.         0.83381396]. \t  -0.48740017629023563 \t -0.47195842108145214\n",
            "14     \t [ 9.06002681  9.03779351 14.          0.60614154  1.          0.16421461]. \t  -0.6627557424336108 \t -0.47195842108145214\n",
            "15     \t [ 0.59590325  0.30071901  8.          0.9148403  16.          0.62633871]. \t  -0.507330476870976 \t -0.47195842108145214\n",
            "16     \t [ 8.69529605  0.27226865 14.          0.90636352  1.          0.39638219]. \t  -0.6352506435607296 \t -0.47195842108145214\n",
            "17     \t [ 5.5997101   2.80089542 11.          0.91397635  4.          0.58148876]. \t  -0.49267256921662134 \t -0.47195842108145214\n",
            "18     \t [ 0.25691043  8.80430419 11.          0.61482757 19.          0.833227  ]. \t  -0.497522195612467 \t -0.47195842108145214\n",
            "19     \t [ 9.24004771  6.36004976 11.          0.81644504 12.          0.66235401]. \t  -0.5080582866060617 \t -0.47195842108145214\n",
            "20     \t [ 0.36491209  6.36419877  5.          0.61520613 11.          0.46519209]. \t  -0.6073236331337577 \t -0.47195842108145214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.758964442352727"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJG2fAtAVFDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794abdd3-d598-409a-cfcd-e0b1719add79"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 18\n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_zero_18 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_18 = dGPGO_zero(surrogate_zero_18, Acquisition_new(util_zero), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
        "zero_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_18 = zero_18.getResult()[0]\n",
        "params_zero_18['max_depth'] = int(params_zero_18['max_depth'])\n",
        "params_zero_18['min_child_weight'] = int(params_zero_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_zero_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_zero_18 = xgb.train(params_zero_18, dX_zero_train18)\n",
        "pred_zero_18 = model_zero_18.predict(dX_zero_test18)\n",
        "\n",
        "rmse_zero_18 = np.sqrt(mean_squared_error(pred_zero_18, y_test18))\n",
        "rmse_zero_18"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.50374242 5.05453374 6.         0.59092011 3.         0.28357516]. \t  -0.6513740959050699 \t -0.4474949996843899\n",
            "init   \t [0.11506734 4.26891483 9.         0.81785956 5.         0.63489043]. \t  -0.5068571689279089 \t -0.4474949996843899\n",
            "init   \t [ 2.8861259   6.35547834 11.          0.64267955 14.          0.27877092]. \t  -0.6466851980958979 \t -0.4474949996843899\n",
            "init   \t [6.57189031 6.99655629 8.         0.63235896 4.         0.52894035]. \t  -0.5514036168275981 \t -0.4474949996843899\n",
            "init   \t [ 6.66600348  2.11312037 14.          0.74363461  4.          0.73174558]. \t  -0.4474949996843899 \t -0.4474949996843899\n",
            "1      \t [ 8.67093232  0.11649132  5.          0.92962202 15.          0.53672863]. \t  -0.5562589356787108 \t -0.4474949996843899\n",
            "2      \t [ 8.43851229  2.41114508 13.          0.75771586 19.          0.86905071]. \t  \u001b[92m-0.44050164109333145\u001b[0m \t -0.44050164109333145\n",
            "3      \t [ 9.44281001  9.01534322  7.          0.99142432 16.          0.37631199]. \t  -0.549158457418693 \t -0.44050164109333145\n",
            "4      \t [ 3.19538294  9.91737336 14.          0.7976317   5.          0.25704487]. \t  -0.6515598146742108 \t -0.44050164109333145\n",
            "5      \t [ 1.97643014  8.37982471  5.          0.63246176 17.          0.45403539]. \t  -0.5634039832057651 \t -0.44050164109333145\n",
            "6      \t [ 1.26601315  0.31299408  6.          0.95296222 16.          0.13649883]. \t  -0.6466742489441083 \t -0.44050164109333145\n",
            "7      \t [ 1.18347798  2.03195078 14.          0.62549517 10.          0.6517083 ]. \t  -0.511373695923542 \t -0.44050164109333145\n",
            "8      \t [6.72039962 1.0287777  9.         0.62848622 9.         0.70815446]. \t  -0.5159589935554622 \t -0.44050164109333145\n",
            "9      \t [ 7.49192948  9.60283663 14.          0.93718151 19.          0.24625933]. \t  -0.6476549496904568 \t -0.44050164109333145\n",
            "10     \t [ 3.63870552  9.73349763  7.          0.73625258 11.          0.87968363]. \t  -0.4484432027321388 \t -0.44050164109333145\n",
            "11     \t [9.98653758 8.80568206 9.         0.86984433 9.         0.78984159]. \t  -0.45607631534129245 \t -0.44050164109333145\n",
            "12     \t [ 1.68019344  0.20490035 13.          0.89332378 19.          0.17761947]. \t  -0.6471945108281247 \t -0.44050164109333145\n",
            "13     \t [0.58655573 8.89860432 5.         0.62593909 1.         0.46819727]. \t  -0.5622837222481027 \t -0.44050164109333145\n",
            "14     \t [ 4.25762222  4.02301922  9.          0.6129246  19.          0.59981465]. \t  -0.5142020783326252 \t -0.44050164109333145\n",
            "15     \t [ 5.93963174  0.35038192 14.          0.99474968 14.          0.46057279]. \t  -0.5351521207701755 \t -0.44050164109333145\n",
            "16     \t [2.5556895  0.91787864 5.         0.57129972 6.         0.40466558]. \t  -0.5665188856299738 \t -0.44050164109333145\n",
            "17     \t [ 8.60576508  8.57930928 14.          0.78656855  7.          0.59728424]. \t  -0.5071486743154364 \t -0.44050164109333145\n",
            "18     \t [ 9.56072091  9.1368036  14.          0.68050527  1.          0.47597207]. \t  -0.5560730708083954 \t -0.44050164109333145\n",
            "19     \t [ 7.60556784  0.42682492 10.          0.97994323  1.          0.15105913]. \t  -0.6491267657624828 \t -0.44050164109333145\n",
            "20     \t [ 2.08722767  3.98919754  5.          0.76742571 11.          0.93277603]. \t  -0.46014482327322215 \t -0.44050164109333145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.746067780922308"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHidSEGcVHvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a034cc67-7b84-4d0a-d525-e70ade7cd38f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 19\n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_zero_19 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_19 = dGPGO_zero(surrogate_zero_19, Acquisition_new(util_zero), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
        "zero_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_19 = zero_19.getResult()[0]\n",
        "params_zero_19['max_depth'] = int(params_zero_19['max_depth'])\n",
        "params_zero_19['min_child_weight'] = int(params_zero_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_zero_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_zero_19 = xgb.train(params_zero_19, dX_zero_train19)\n",
        "pred_zero_19 = model_zero_19.predict(dX_zero_test19)\n",
        "\n",
        "rmse_zero_19 = np.sqrt(mean_squared_error(pred_zero_19, y_test19))\n",
        "rmse_zero_19"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.97533602  7.61249717 13.          0.85765469 11.          0.39830191]. \t  -0.5851081958447377 \t -0.4870287725699859\n",
            "init   \t [ 0.82999565  6.71977081  6.          0.50407413 19.          0.67209466]. \t  -0.5245729886945251 \t -0.4870287725699859\n",
            "init   \t [ 2.15923256  5.49027432 12.          0.52588686 10.          0.20235326]. \t  -0.677999989539271 \t -0.4870287725699859\n",
            "init   \t [4.99659267 1.52108422 6.         0.73481085 4.         0.71949465]. \t  -0.4940235599803803 \t -0.4870287725699859\n",
            "init   \t [ 3.72927156  9.46160045  5.          0.80554614 18.          0.97708466]. \t  -0.4870287725699859 \t -0.4870287725699859\n",
            "1      \t [ 8.33060043  1.42030563  8.          0.92863724 14.          0.78606141]. \t  \u001b[92m-0.48699206254226385\u001b[0m \t -0.48699206254226385\n",
            "2      \t [ 7.89674065  9.31460122 11.          0.96158687  5.          0.67754853]. \t  -0.49785049104905016 \t -0.48699206254226385\n",
            "3      \t [ 5.37126961  3.11618495 14.          0.88535151  1.          0.84812976]. \t  \u001b[92m-0.47782430490951233\u001b[0m \t -0.47782430490951233\n",
            "4      \t [ 3.65000245  2.90359952 13.          0.98940034 19.          0.29019455]. \t  -0.5803493338026307 \t -0.47782430490951233\n",
            "5      \t [0.25768796 8.33414072 6.         0.99948369 4.         0.66680619]. \t  -0.5074502274890225 \t -0.47782430490951233\n",
            "6      \t [ 1.10650842  9.77537077 14.          0.83431584  1.          0.5884296 ]. \t  -0.5035756891739214 \t -0.47782430490951233\n",
            "7      \t [ 9.20734788  9.40702602 11.          0.68264617 14.          0.68723167]. \t  -0.5088698943836598 \t -0.47782430490951233\n",
            "8      \t [1.60895472e-01 8.27074864e-03 1.30000000e+01 6.91893018e-01\n",
            " 5.00000000e+00 4.60327119e-01]. \t  -0.5676975299111877 \t -0.47782430490951233\n",
            "9      \t [7.08488952 8.05904216 5.         0.93088713 1.         0.30662699]. \t  -0.5910777842699757 \t -0.47782430490951233\n",
            "10     \t [ 6.55489773  0.06438745 14.          0.53351105 11.          0.76391016]. \t  -0.4835963309582638 \t -0.47782430490951233\n",
            "11     \t [ 1.57264207  2.19631308  6.          0.5882725  10.          0.94313253]. \t  -0.4787116627447901 \t -0.47782430490951233\n",
            "12     \t [ 9.73180729  6.57278116  6.          0.95590535 11.          0.18540044]. \t  -0.680192783781354 \t -0.47782430490951233\n",
            "13     \t [ 4.44431879  9.49168646 13.          0.64581011 19.          0.4702202 ]. \t  -0.5592489367596285 \t -0.47782430490951233\n",
            "14     \t [ 0.11534005  0.08986117  8.          0.95341643 19.          0.48014014]. \t  -0.5639400804347694 \t -0.47782430490951233\n",
            "15     \t [ 4.48364258  9.8096896   5.          0.80760511 11.          0.86399136]. \t  -0.488055348011302 \t -0.47782430490951233\n",
            "16     \t [ 8.69943878  7.36890408  7.          0.86891549 19.          0.36276071]. \t  -0.5930096394832391 \t -0.47782430490951233\n",
            "17     \t [ 8.83749539  4.67326392 12.          0.87726007 17.          0.55414829]. \t  -0.5621734743515019 \t -0.47782430490951233\n",
            "18     \t [ 7.96112468  2.0900309  13.          0.68207038  6.          0.40980539]. \t  -0.5833797207184752 \t -0.47782430490951233\n",
            "19     \t [ 2.07372661  6.38455908 10.          0.63595289  1.          0.64487018]. \t  -0.506814497007569 \t -0.47782430490951233\n",
            "20     \t [ 3.82670531  7.06245628  8.          0.7747773  14.          0.11384763]. \t  -0.67893464439286 \t -0.47782430490951233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.542836301698574"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWGPYRJhVKsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800d9fa6-65bb-4ea4-d186-daac509eae0e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'zero' Acquisition Function run number = 20\n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_zero_20 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "zero_20 = dGPGO_zero(surrogate_zero_20, Acquisition_new(util_zero), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
        "zero_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_zero_20 = zero_20.getResult()[0]\n",
        "params_zero_20['max_depth'] = int(params_zero_20['max_depth'])\n",
        "params_zero_20['min_child_weight'] = int(params_zero_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_zero_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_zero_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_zero_20 = xgb.train(params_zero_20, dX_zero_train20)\n",
        "pred_zero_20 = model_zero_20.predict(dX_zero_test20)\n",
        "\n",
        "rmse_zero_20 = np.sqrt(mean_squared_error(pred_zero_20, y_test20))\n",
        "rmse_zero_20"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.88130801  8.97713728 14.          0.81074445  8.          0.95540649]. \t  -0.4485352768858121 \t -0.4485352768858121\n",
            "init   \t [6.72865655 0.41173329 8.         0.6361582  7.         0.76174061]. \t  -0.47208091450542966 \t -0.4485352768858121\n",
            "init   \t [ 4.77387703  8.66202323 10.          0.51833215  7.          0.10123387]. \t  -0.7316473600840852 \t -0.4485352768858121\n",
            "init   \t [ 5.75489985  4.74524381  8.          0.78084343 15.          0.26643049]. \t  -0.7314226542252507 \t -0.4485352768858121\n",
            "init   \t [ 4.53444     4.47342833  8.          0.91974896 18.          0.35997552]. \t  -0.6494230116583573 \t -0.4485352768858121\n",
            "1      \t [ 7.96566073  7.15509535  7.          0.79906691 11.          0.34132075]. \t  -0.6502222800629637 \t -0.4485352768858121\n",
            "2      \t [ 1.98667885  1.35773177 13.          0.57199118  2.          0.39498908]. \t  -0.6661970064789862 \t -0.4485352768858121\n",
            "3      \t [ 3.00704909  2.42524876 14.          0.95062509 15.          0.83595087]. \t  -0.4577111911042998 \t -0.4485352768858121\n",
            "4      \t [0.52053109 3.24156501 6.         0.62694491 9.         0.62944167]. \t  -0.5198752102757443 \t -0.4485352768858121\n",
            "5      \t [8.0846212  5.99993376 6.         0.83941375 1.         0.46362124]. \t  -0.5681524533474447 \t -0.4485352768858121\n",
            "6      \t [0.61316554 1.36115087 5.         0.87944956 1.         0.21740227]. \t  -0.7332228664111685 \t -0.4485352768858121\n",
            "7      \t [ 0.72788527  2.26655356 10.          0.97273032 15.          0.85843758]. \t  -0.45084600130879665 \t -0.4485352768858121\n",
            "8      \t [ 7.29847873  0.61439441  5.          0.60353619 15.          0.628184  ]. \t  -0.5276788324413187 \t -0.4485352768858121\n",
            "9      \t [ 1.44692101  9.96202174 12.          0.76092884 18.          0.10523817]. \t  -0.731303133317551 \t -0.4485352768858121\n",
            "10     \t [ 1.01814405  9.81807131 14.          0.52908047  1.          0.89345697]. \t  -0.4570018264326601 \t -0.4485352768858121\n",
            "11     \t [ 9.05255372  2.37894322 12.          0.61307055 13.          0.78275789]. \t  -0.47723885637506297 \t -0.4485352768858121\n",
            "12     \t [ 9.73911039  8.37125563 14.          0.52973573 18.          0.18867177]. \t  -0.7327436822559269 \t -0.4485352768858121\n",
            "13     \t [ 8.98143836  8.7693897  12.          0.89468403 12.          0.66552576]. \t  -0.49793969514395464 \t -0.4485352768858121\n",
            "14     \t [ 8.46352627  3.51174184 14.          0.92570058  3.          0.89634733]. \t  \u001b[92m-0.44224691281124234\u001b[0m \t -0.44224691281124234\n",
            "15     \t [ 5.37444991  0.3056877   9.          0.99777328 15.          0.5903115 ]. \t  -0.5008636074552522 \t -0.44224691281124234\n",
            "16     \t [ 9.4780848   8.16138344  6.          0.79780509 18.          0.3785568 ]. \t  -0.651506546791354 \t -0.44224691281124234\n",
            "17     \t [ 0.0269108   5.63927067 14.          0.80391219 10.          0.28928667]. \t  -0.6555537944017766 \t -0.44224691281124234\n",
            "18     \t [0.12661394 7.44623666 6.         0.78474136 2.         0.7832835 ]. \t  -0.4714469009959118 \t -0.44224691281124234\n",
            "19     \t [ 0.19959794  7.04869492  7.          0.96984448 14.          0.74538627]. \t  -0.4742067223337808 \t -0.44224691281124234\n",
            "20     \t [ 8.27804787  0.86147068 14.          0.51158499 19.          0.40610893]. \t  -0.6503050783425212 \t -0.44224691281124234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.491951392659048"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1d_1LyydIfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed41662c-e572-4c4d-d766-d4d973283c22"
      },
      "source": [
        "end_zero = time.time()\n",
        "end_zero\n",
        "\n",
        "time_zero = end_zero - start_zero\n",
        "time_zero\n",
        "\n",
        "start_exact = time.time()\n",
        "start_exact"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1629368783.3808484"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyOw7XYVwAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6884d2-2c7f-4226-8d6f-1e1b35d3b567"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 1 \n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_exact_1 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_1 = dGPGO(surrogate_exact_1, Acquisition_new(util_exact), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_1 = exact_1.getResult()[0]\n",
        "params_exact_1['max_depth'] = int(params_exact_1['max_depth'])\n",
        "params_exact_1['min_child_weight'] = int(params_exact_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_exact_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_exact_1 = xgb.train(params_exact_1, dX_exact_train1)\n",
        "pred_exact_1 = model_exact_1.predict(dX_exact_test1)\n",
        "\n",
        "rmse_exact_1 = np.sqrt(mean_squared_error(pred_exact_1, y_test1))\n",
        "rmse_exact_1"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [5.48813504 7.15189366 8.         0.92897281 8.         0.48128932]. \t  -0.5626915313589341 \t -0.46143572360276275\n",
            "init   \t [ 6.45894113  4.37587211 11.          0.52835649 13.          0.44509737]. \t  -0.5772881365468763 \t -0.46143572360276275\n",
            "init   \t [ 7.91725038  5.2889492  13.          0.6963924  14.          0.40365654]. \t  -0.5870544636272766 \t -0.46143572360276275\n",
            "init   \t [ 6.48171872  3.6824154  10.          0.88907838 16.          0.88307853]. \t  -0.46143572360276275 \t -0.46143572360276275\n",
            "init   \t [4.73608045 8.00910752 8.         0.83943977 8.         0.67592892]. \t  -0.5051288806760134 \t -0.46143572360276275\n",
            "1      \t [ 0.96098408  9.76459465  7.          0.75481219 17.          0.64436097]. \t  -0.5149059967458438 \t -0.46143572360276275\n",
            "2      \t [ 5.13759733  2.22657933 12.          0.58106013  2.          0.92007745]. \t  -0.46868154430393166 \t -0.46143572360276275\n",
            "3      \t [9.58067178 9.65734278 7.         0.88193436 1.         0.38223155]. \t  -0.5888715285109799 \t -0.46143572360276275\n",
            "4      \t [ 0.90969339  9.80979401 14.          0.8665633   3.          0.55460209]. \t  -0.5632471108749298 \t -0.46143572360276275\n",
            "5      \t [0.3028841  4.069464   5.         0.51500749 1.         0.27359263]. \t  -0.6787735406244566 \t -0.46143572360276275\n",
            "6      \t [ 8.87166351  9.3367646   5.          0.58200219 19.          0.4055524 ]. \t  -0.6087566280502396 \t -0.46143572360276275\n",
            "7      \t [ 0.51228404  8.90605177 14.          0.7949699  11.          0.73913262]. \t  \u001b[92m-0.46104762612970374\u001b[0m \t -0.46104762612970374\n",
            "8      \t [ 2.05150398  0.53599727  5.          0.71551734 11.          0.37377971]. \t  -0.60261611749113 \t -0.46104762612970374\n",
            "9      \t [8.38797278 0.6003286  6.         0.6181346  1.         0.60370114]. \t  -0.526164863483156 \t -0.46104762612970374\n",
            "10     \t [ 9.06530919  9.40796401 14.          0.73452132  4.          0.22638202]. \t  -0.6737273666958219 \t -0.46104762612970374\n",
            "11     \t [ 2.92761431  0.02841708 13.          0.97950295  9.          0.31091424]. \t  -0.5822275926277118 \t -0.46104762612970374\n",
            "12     \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.4302853773034897\u001b[0m \t -0.4302853773034897\n",
            "13     \t [ 1.99180311  1.76156949  6.          0.52474573 18.          0.69397695]. \t  -0.5292680491699555 \t -0.4302853773034897\n",
            "14     \t [10.         10.         15.          1.         11.03652682  1.        ]. \t  \u001b[92m-0.42514855917982713\u001b[0m \t -0.42514855917982713\n",
            "15     \t [ 3.33998723  9.61997442 13.          0.87678219 17.          0.47112812]. \t  -0.562089749118206 \t -0.42514855917982713\n",
            "16     \t [ 0.89800437  1.82654173 13.          0.73602129 17.          0.1041576 ]. \t  -0.6740368890718571 \t -0.42514855917982713\n",
            "17     \t [ 7.96942817  9.76181769  7.          0.91046857 13.          0.32455735]. \t  -0.5870001390492389 \t -0.42514855917982713\n",
            "18     \t [0.45095418 9.96977628 8.         0.5330212  1.         0.30363698]. \t  -0.5984194428816372 \t -0.42514855917982713\n",
            "19     \t [ 8.55155756  0.36726655  8.          0.94666955 10.          0.23257137]. \t  -0.6706122625441684 \t -0.42514855917982713\n",
            "20     \t [3.69919707 0.85322265 6.         0.75904625 5.         0.97222422]. \t  -0.4807633478348844 \t -0.42514855917982713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.607787340872903"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDQbChpZ48F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9ed48d-e220-4ed7-e73b-195a09b89e5a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 2 \n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_exact_2 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_2 = dGPGO(surrogate_exact_2, Acquisition_new(util_exact), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_2 = exact_2.getResult()[0]\n",
        "params_exact_2['max_depth'] = int(params_exact_2['max_depth'])\n",
        "params_exact_2['min_child_weight'] = int(params_exact_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_exact_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_exact_2 = xgb.train(params_exact_2, dX_exact_train2)\n",
        "pred_exact_2 = model_exact_2.predict(dX_exact_test2)\n",
        "\n",
        "rmse_exact_2 = np.sqrt(mean_squared_error(pred_exact_2, y_test2))\n",
        "rmse_exact_2"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 4.35994902  0.25926232 11.          0.97386531 12.          0.47833102]. \t  -0.5147867600449748 \t -0.4765694615523879\n",
            "init   \t [ 3.30334821  2.04648634 10.          0.55997527  6.          0.71472339]. \t  -0.4765694615523879 \t -0.4765694615523879\n",
            "init   \t [ 4.9856117   5.86796978  8.          0.89266757 11.          0.59158659]. \t  -0.4946399399702889 \t -0.4765694615523879\n",
            "init   \t [ 4.07307832  1.76984624 13.          0.75262305  7.          0.35908193]. \t  -0.5884370816585467 \t -0.4765694615523879\n",
            "init   \t [ 1.16193318  1.81727038  9.          0.79837265 19.          0.29965165]. \t  -0.584982798458911 \t -0.4765694615523879\n",
            "1      \t [10.         10.         13.31650102  1.         20.          1.        ]. \t  \u001b[92m-0.3809881978047184\u001b[0m \t -0.3809881978047184\n",
            "2      \t [1.25559631 9.8394609  9.         0.52015567 2.         0.32958416]. \t  -0.5945813795974784 \t -0.3809881978047184\n",
            "3      \t [ 9.14946201  2.43697872  6.          0.997805   19.          0.45949208]. \t  -0.5305156688763031 \t -0.3809881978047184\n",
            "4      \t [9.94733027 1.06305306 6.         0.53557792 2.         0.26251983]. \t  -0.6747986207670527 \t -0.3809881978047184\n",
            "5      \t [ 9.49925654  7.97200083  8.          0.53193588 18.          0.45705681]. \t  -0.5269444956120974 \t -0.3809881978047184\n",
            "6      \t [ 9.61885664  9.78589131 14.          0.8903706   1.          0.14094172]. \t  -0.6776999663731892 \t -0.3809881978047184\n",
            "7      \t [ 0.05725091  9.56919109 14.          0.73355327 17.          0.65868771]. \t  -0.4840368839655496 \t -0.3809881978047184\n",
            "8      \t [7.36877801 8.87815651 5.         0.56972286 4.         0.80625064]. \t  -0.5008693406902159 \t -0.3809881978047184\n",
            "9      \t [ 5.76886466  6.30636441 11.          0.85075313  6.          0.94564556]. \t  -0.39766690308447894 \t -0.3809881978047184\n",
            "10     \t [ 9.532292    9.50049573 13.          0.6020865  12.          0.69858036]. \t  -0.4949335744560372 \t -0.3809881978047184\n",
            "11     \t [ 0.44071499  7.73764551  6.          0.95035779 15.          0.59534933]. \t  -0.5150284970246458 \t -0.3809881978047184\n",
            "12     \t [ 8.21799102  0.75840382 13.          0.74503318 19.          0.27131427]. \t  -0.6754902476470062 \t -0.3809881978047184\n",
            "13     \t [ 0.59751708  9.63364991 11.          0.90897061  9.          0.44731047]. \t  -0.5150962154652547 \t -0.3809881978047184\n",
            "14     \t [ 2.61078484  8.48438058 14.          0.79784401 12.          0.37480602]. \t  -0.5873563000820609 \t -0.3809881978047184\n",
            "15     \t [ 8.35914343  2.39349698 13.          0.9820231   1.          0.65048694]. \t  -0.4939446347578816 \t -0.3809881978047184\n",
            "16     \t [ 8.39253634  0.41975132  5.          0.53583299 10.          0.93462258]. \t  -0.4369912210415411 \t -0.3809881978047184\n",
            "17     \t [1.70838606 1.7073017  5.         0.64676695 1.         0.90384668]. \t  -0.4303260861905837 \t -0.3809881978047184\n",
            "18     \t [ 4.81917386  5.39014745 11.41344907  1.         16.41344907  1.        ]. \t  \u001b[92m-0.3777094511302258\u001b[0m \t -0.3777094511302258\n",
            "19     \t [10.          4.0050117  15.          1.          6.09854109  1.        ]. \t  \u001b[92m-0.3763198412022126\u001b[0m \t -0.3763198412022126\n",
            "20     \t [ 0.40164348  1.66439141 14.          0.96476321  1.          0.14148109]. \t  -0.6802555077917066 \t -0.3763198412022126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.556046943199982"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUPyXRfZ95Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383e81b4-dfa5-46a8-f27d-b158d127540f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 3 \n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_exact_3 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_3 = dGPGO(surrogate_exact_3, Acquisition_new(util_exact), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_3 = exact_3.getResult()[0]\n",
        "params_exact_3['max_depth'] = int(params_exact_3['max_depth'])\n",
        "params_exact_3['min_child_weight'] = int(params_exact_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_exact_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_exact_3 = xgb.train(params_exact_3, dX_exact_train3)\n",
        "pred_exact_3 = model_exact_3.predict(dX_exact_test3)\n",
        "\n",
        "rmse_exact_3 = np.sqrt(mean_squared_error(pred_exact_3, y_test3))\n",
        "rmse_exact_3"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.50797903  7.08147823 13.          0.56066429 11.          0.11687321]. \t  -0.7165783188757435 \t -0.6409647951145182\n",
            "init   \t [ 0.40630737  2.47888297 11.          0.72040492 13.          0.23083313]. \t  -0.7204431346766296 \t -0.6409647951145182\n",
            "init   \t [ 4.53172301  2.15577008 11.          0.74631796  2.          0.60296868]. \t  -0.6409647951145182 \t -0.6409647951145182\n",
            "init   \t [ 2.59252447  4.15101197 13.          0.79330998  8.          0.24118096]. \t  -0.7214290072967551 \t -0.6409647951145182\n",
            "init   \t [ 5.44649018  7.80314765 10.          0.62879264 18.          0.44917413]. \t  -0.6558401549443297 \t -0.6409647951145182\n",
            "1      \t [4.88873245 9.27936348 6.         0.94344906 8.         0.25949204]. \t  -0.7188700813687741 \t -0.6409647951145182\n",
            "2      \t [ 8.93142368  1.52910591 13.          0.84039318 17.          0.60846833]. \t  \u001b[92m-0.6320347814043803\u001b[0m \t -0.6320347814043803\n",
            "3      \t [ 6.38594331  1.19109066  5.          0.81189053 13.          0.59164768]. \t  \u001b[92m-0.6289274956252369\u001b[0m \t -0.6289274956252369\n",
            "4      \t [ 1.02918863  9.32189805 13.          0.88333707  1.          0.86998588]. \t  \u001b[92m-0.45190108244647254\u001b[0m \t -0.45190108244647254\n",
            "5      \t [ 9.74929058  1.51205926 11.          0.50025602  8.          0.46132437]. \t  -0.6593087000926661 \t -0.45190108244647254\n",
            "6      \t [ 9.45052852  8.62641484  7.          0.79615518 14.          0.32790361]. \t  -0.708730491334048 \t -0.45190108244647254\n",
            "7      \t [ 8.92744991  9.09956287 12.          0.74944313  3.          0.11030352]. \t  -0.7194264138705575 \t -0.45190108244647254\n",
            "8      \t [0.28002919 1.86471402 5.         0.90893429 3.         0.44808833]. \t  -0.653236808211726 \t -0.45190108244647254\n",
            "9      \t [ 9.02893081  7.64399312 14.          0.72894099 15.          0.70609928]. \t  -0.6325306398208942 \t -0.45190108244647254\n",
            "10     \t [4.37264513 7.83931591 6.         0.65864599 1.         0.34090671]. \t  -0.7108079171350814 \t -0.45190108244647254\n",
            "11     \t [ 2.84857043  0.57472701  5.          0.53705172 19.          0.51858228]. \t  -0.6589951838647294 \t -0.45190108244647254\n",
            "12     \t [ 0.63346059  9.87550877  6.          0.74382195 14.          0.39340576]. \t  -0.7092906536332471 \t -0.45190108244647254\n",
            "13     \t [2.99254427 2.69228882 7.         0.63915731 9.         0.21678027]. \t  -0.7194910621859361 \t -0.45190108244647254\n",
            "14     \t [8.8237369  0.04240955 5.         0.7158964  1.         0.99950517]. \t  -0.48215926032694006 \t -0.45190108244647254\n",
            "15     \t [ 5.7586577   1.42812945 10.          0.74022438  8.          0.79053147]. \t  -0.5205954164321709 \t -0.45190108244647254\n",
            "16     \t [9.25339855 3.80341124 5.         0.92679998 9.         0.56391072]. \t  -0.6536743943093699 \t -0.45190108244647254\n",
            "17     \t [ 0.94336997  1.29293405 12.          0.62917786 19.          0.81056172]. \t  -0.5192284078227264 \t -0.45190108244647254\n",
            "18     \t [ 0.31516471  7.97295608 10.          0.77489537  9.          0.25855149]. \t  -0.7193805891172754 \t -0.45190108244647254\n",
            "19     \t [ 9.8542409   3.34207335 14.          0.75035548  3.          0.6035725 ]. \t  -0.6317346576420496 \t -0.45190108244647254\n",
            "20     \t [ 3.60954441  5.81173439  5.          0.60415615 17.          0.56274483]. \t  -0.6561590588771492 \t -0.45190108244647254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.649527586702978"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKX_nfEaaAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd806ae4-bba7-4b22-de6b-2279d12ffc0f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 4 \n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_exact_4 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_4 = dGPGO(surrogate_exact_4, Acquisition_new(util_exact), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_4 = exact_4.getResult()[0]\n",
        "params_exact_4['max_depth'] = int(params_exact_4['max_depth'])\n",
        "params_exact_4['min_child_weight'] = int(params_exact_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_exact_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_exact_4 = xgb.train(params_exact_4, dX_exact_train4)\n",
        "pred_exact_4 = model_exact_4.predict(dX_exact_test4)\n",
        "\n",
        "rmse_exact_4 = np.sqrt(mean_squared_error(pred_exact_4, y_test4))\n",
        "rmse_exact_4"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.67029839 5.47232249 6.         0.92781047 9.         0.72795594]. \t  -0.5993772224326677 \t -0.4983304913999733\n",
            "init   \t [ 2.16089496  9.76274455 12.          0.62649118  9.          0.66966679]. \t  -0.6069567430422909 \t -0.4983304913999733\n",
            "init   \t [ 0.05159149  5.72356491  9.          0.99170034 10.          0.10808749]. \t  -0.7139334307278753 \t -0.4983304913999733\n",
            "init   \t [ 3.86571283  0.44160058 10.          0.90553105 18.          0.95407958]. \t  -0.4983304913999733 \t -0.4983304913999733\n",
            "init   \t [ 7.86305986  8.66289299  6.          0.53285477 14.          0.25117497]. \t  -0.7091576633146701 \t -0.4983304913999733\n",
            "1      \t [ 8.45443649  8.61014312 11.          0.83475494  1.          0.14018305]. \t  -0.7189305559932541 \t -0.4983304913999733\n",
            "2      \t [ 0.77431146  1.96668116 12.          0.50723361  3.          0.74768925]. \t  -0.6140173026050956 \t -0.4983304913999733\n",
            "3      \t [2.27858743 6.23199766 5.         0.58705984 2.         0.80794289]. \t  -0.5986031600147258 \t -0.4983304913999733\n",
            "4      \t [ 6.832625    9.87635293 14.          0.84450885 19.          0.20389666]. \t  -0.7153744507380464 \t -0.4983304913999733\n",
            "5      \t [ 7.37255369  2.03491596 13.          0.8921741   9.          0.46934318]. \t  -0.625550615237801 \t -0.4983304913999733\n",
            "6      \t [ 0.05992751  6.06320143 14.          0.89322475 17.          0.32211096]. \t  -0.6540035079472759 \t -0.4983304913999733\n",
            "7      \t [ 0.79250634  6.36332745  6.          0.84703891 17.          0.8628914 ]. \t  -0.5176542387032834 \t -0.4983304913999733\n",
            "8      \t [9.26767626 0.09691703 5.         0.59554562 3.         0.95249041]. \t  -0.524958204283032 \t -0.4983304913999733\n",
            "9      \t [ 9.93824172  2.34876498  7.          0.94210707 16.          0.6947317 ]. \t  -0.6069344861897712 \t -0.4983304913999733\n",
            "10     \t [3.43076773 0.51115291 6.         0.80398076 7.         0.163561  ]. \t  -0.7152347423610873 \t -0.4983304913999733\n",
            "11     \t [8.12039932 9.82838311 5.         0.6851891  3.         0.87462757]. \t  -0.5209212303565759 \t -0.4983304913999733\n",
            "12     \t [ 6.03647398  5.02077859  7.          0.51964174 12.          0.90396407]. \t  -0.5142832795776289 \t -0.4983304913999733\n",
            "13     \t [ 9.03174101  2.34471053 13.          0.52042845  3.          0.20136175]. \t  -0.7105427027924398 \t -0.4983304913999733\n",
            "14     \t [10.         10.         13.57473549  1.          9.57473549  1.        ]. \t  \u001b[92m-0.4791672055007812\u001b[0m \t -0.4791672055007812\n",
            "15     \t [ 6.05918846  4.34302105 13.          0.69548235 15.          0.51487907]. \t  -0.6222861048031127 \t -0.4791672055007812\n",
            "16     \t [ 2.02212851  9.9407933  10.          0.71009222  3.          0.59925198]. \t  -0.6118231860074616 \t -0.4791672055007812\n",
            "17     \t [ 9.31090633  0.03922389 13.          0.89687279 18.          0.65712575]. \t  -0.6054491997792859 \t -0.4791672055007812\n",
            "18     \t [ 0.84701744  1.60683479 14.          0.52191349  9.          0.13284287]. \t  -0.7118778609540446 \t -0.4791672055007812\n",
            "19     \t [0.43927529 9.60376826 5.         0.58040652 9.         0.31646261]. \t  -0.6626762857296217 \t -0.4791672055007812\n",
            "20     \t [ 0.65787882  1.52536753  7.          0.582989   13.          0.8348692 ]. \t  -0.5926194249900745 \t -0.4791672055007812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.742748709233146"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJmI9saAaEG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8512eb13-c2fa-4757-9a79-4e309f068ea9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 5 \n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_exact_5 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_5 = dGPGO(surrogate_exact_5, Acquisition_new(util_exact), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_5 = exact_5.getResult()[0]\n",
        "params_exact_5['max_depth'] = int(params_exact_5['max_depth'])\n",
        "params_exact_5['min_child_weight'] = int(params_exact_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_exact_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_exact_5 = xgb.train(params_exact_5, dX_exact_train5)\n",
        "pred_exact_5 = model_exact_5.predict(dX_exact_test5)\n",
        "\n",
        "rmse_exact_5 = np.sqrt(mean_squared_error(pred_exact_5, y_test5))\n",
        "rmse_exact_5"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.21993171  8.70732306 11.          0.68186845 10.          0.53957007]. \t  -0.5323233521622429 \t -0.48683475961332984\n",
            "init   \t [ 6.11743863  7.65907856  5.          0.64840025 16.          0.82745351]. \t  -0.48683475961332984 \t -0.48683475961332984\n",
            "init   \t [ 6.49458883  8.19472793  6.          0.93996852 19.          0.36647194]. \t  -0.586475577548452 \t -0.48683475961332984\n",
            "init   \t [ 6.28787909  5.7983781   6.          0.63290956 17.          0.18402673]. \t  -0.633609225551848 \t -0.48683475961332984\n",
            "init   \t [8.26554249 8.33492742 9.         0.97900675 3.         0.26957319]. \t  -0.6326249738244012 \t -0.48683475961332984\n",
            "1      \t [1.95474956 1.21548467 5.         0.65548996 6.         0.3261206 ]. \t  -0.5882279171378358 \t -0.48683475961332984\n",
            "2      \t [ 8.68915106  0.84881749 13.          0.9945373   7.          0.34624572]. \t  -0.5633370634302679 \t -0.48683475961332984\n",
            "3      \t [ 6.12310163  2.21013771 14.          0.74740665 18.          0.42989776]. \t  -0.5333068626906653 \t -0.48683475961332984\n",
            "4      \t [ 9.64635884  9.52633265 14.          0.67611826 10.          0.87506739]. \t  \u001b[92m-0.4423357727617705\u001b[0m \t -0.4423357727617705\n",
            "5      \t [ 0.42801231  0.53056997 14.          0.88001393  3.          0.60870677]. \t  -0.4956565139336144 \t -0.4423357727617705\n",
            "6      \t [10.         10.         14.76488616  1.         20.          1.        ]. \t  \u001b[92m-0.4315329340187998\u001b[0m \t -0.4315329340187998\n",
            "7      \t [9.67414353 2.68949571 5.         0.62350468 9.         0.83649418]. \t  -0.4893925661153425 \t -0.4315329340187998\n",
            "8      \t [ 0.5259471   9.7567347  14.          0.91097263  1.          0.64281486]. \t  -0.49657614671134453 \t -0.4315329340187998\n",
            "9      \t [9.64880583 0.85455793 9.         0.80366346 1.         0.54097439]. \t  -0.5442200644862888 \t -0.4315329340187998\n",
            "10     \t [1.24717977 9.40645683 5.         0.65383928 3.         0.68852908]. \t  -0.5172211356155465 \t -0.4315329340187998\n",
            "11     \t [ 1.7633545   9.18687735 14.          0.62483521 19.          0.67316164]. \t  -0.4977308231633463 \t -0.4315329340187998\n",
            "12     \t [ 1.13915888  1.42227157  5.          0.93948287 15.          0.24386626]. \t  -0.6356899457971215 \t -0.4315329340187998\n",
            "13     \t [ 1.33802527  0.56317443 13.          0.62013582 12.          0.977958  ]. \t  -0.4363186570405474 \t -0.4315329340187998\n",
            "14     \t [ 2.36281749  5.54578862 10.          0.61695313  4.          0.17816121]. \t  -0.6325214388686458 \t -0.4315329340187998\n",
            "15     \t [9.30969345 8.84356664 5.         0.89369445 8.         0.11148942]. \t  -0.6362192215439146 \t -0.4315329340187998\n",
            "16     \t [ 9.7108222   4.76509396 12.          0.6344947  14.          0.80559269]. \t  -0.4759315199959574 \t -0.4315329340187998\n",
            "17     \t [ 6.80615583  6.45556443 14.          0.61036314  1.          0.8555849 ]. \t  -0.4679937193324773 \t -0.4315329340187998\n",
            "18     \t [ 0.39655743  9.88537948  7.          0.8395494  15.          0.83639512]. \t  -0.46556996770739073 \t -0.4315329340187998\n",
            "19     \t [ 0.04353364  3.80292085 10.          0.99695718 18.          0.799674  ]. \t  -0.4560426797696765 \t -0.4315329340187998\n",
            "20     \t [ 9.94984248  2.37676311  9.          0.96621815 19.          0.48539202]. \t  -0.531856562647136 \t -0.4315329340187998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.742742703974025"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulhEolsxaG4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559f3e51-030f-4314-d689-91ff9978fae7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 6 \n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_exact_6 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror', eval_metric = 'rmse')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_6 = dGPGO(surrogate_exact_6, Acquisition_new(util_exact), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_6 = exact_6.getResult()[0]\n",
        "params_exact_6['max_depth'] = int(params_exact_6['max_depth'])\n",
        "params_exact_6['min_child_weight'] = int(params_exact_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_exact_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_exact_6 = xgb.train(params_exact_6, dX_exact_train6)\n",
        "pred_exact_6 = model_exact_6.predict(dX_exact_test6)\n",
        "\n",
        "rmse_exact_6 = np.sqrt(mean_squared_error(pred_exact_6, y_test6))\n",
        "rmse_exact_6"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5719256944003751 \t -0.5405445954433028\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.6047098118480896 \t -0.5405445954433028\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5405445954433028 \t -0.5405445954433028\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5678490550489279 \t -0.5405445954433028\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6145131146454834 \t -0.5405445954433028\n",
            "1      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.45961383649052767\u001b[0m \t -0.45961383649052767\n",
            "2      \t [ 9.24343066  2.50578958  7.          0.73023742 13.          0.6940436 ]. \t  -0.5767008292506094 \t -0.45961383649052767\n",
            "3      \t [8.18334854 9.97104849 8.         0.91869016 8.         0.37558961]. \t  -0.6144590591119746 \t -0.45961383649052767\n",
            "4      \t [ 2.04733214  5.62395529 10.          0.79098076 10.          0.19218812]. \t  -0.6920160120895364 \t -0.45961383649052767\n",
            "5      \t [ 1.35461816  3.68867636  7.          0.97358458 19.          0.99760691]. \t  -0.4813823942223731 \t -0.45961383649052767\n",
            "6      \t [ 9.79809506  4.05954978 14.          0.57601486  3.          0.61048927]. \t  -0.5747329707889864 \t -0.45961383649052767\n",
            "7      \t [ 1.7609747   9.34733292 14.          0.94873694  3.          0.31587639]. \t  -0.6202575508979221 \t -0.45961383649052767\n",
            "8      \t [ 9.32420466  6.39616005 13.          0.93300527 17.          0.34904443]. \t  -0.618513635024325 \t -0.45961383649052767\n",
            "9      \t [ 8.18088231  9.44781209  7.          0.95869221 19.          0.77401414]. \t  -0.5389010350952639 \t -0.45961383649052767\n",
            "10     \t [ 1.77214312  9.25420942  5.          0.67440912 15.          0.38002389]. \t  -0.6272073900971591 \t -0.45961383649052767\n",
            "11     \t [1.74386723 0.44633377 5.         0.62204812 9.         0.13189777]. \t  -0.6931813502859878 \t -0.45961383649052767\n",
            "12     \t [ 2.79340377  7.98121034 11.78897397  1.         20.          1.        ]. \t  \u001b[92m-0.459161351790203\u001b[0m \t -0.459161351790203\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5742354992220393 \t -0.459161351790203\n",
            "14     \t [ 0.47139237  0.24747681 14.          0.53291734 14.          0.24490793]. \t  -0.6947949784703452 \t -0.459161351790203\n",
            "15     \t [0.34300109 9.68403349 8.         0.54781067 3.         0.98074554]. \t  -0.483546606850085 \t -0.459161351790203\n",
            "16     \t [ 8.38603255  0.66031556 14.          0.89601822  8.          0.63597949]. \t  -0.5563153105609959 \t -0.459161351790203\n",
            "17     \t [ 8.4425376  10.         14.79626801  1.          6.79626801  1.        ]. \t  \u001b[92m-0.4526708734791033\u001b[0m \t -0.4526708734791033\n",
            "18     \t [ 0.1424991   9.83224829 14.          0.82001413 10.          0.36444993]. \t  -0.6201751120416013 \t -0.4526708734791033\n",
            "19     \t [ 8.21396627  2.65706382  6.          0.52082555 19.          0.80555848]. \t  -0.5578395133551297 \t -0.4526708734791033\n",
            "20     \t [ 0.99977857  3.51872667 14.          0.87616868  4.          0.76711714]. \t  -0.5299939150665656 \t -0.4526708734791033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.565813167045083"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYebx3RVaJ1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f1871e-28af-4c71-e74f-7669168f41d4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 7 \n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_exact_7 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_7 = dGPGO(surrogate_exact_7, Acquisition_new(util_exact), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_7 = exact_7.getResult()[0]\n",
        "params_exact_7['max_depth'] = int(params_exact_7['max_depth'])\n",
        "params_exact_7['min_child_weight'] = int(params_exact_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_exact_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_exact_7 = xgb.train(params_exact_7, dX_exact_train7)\n",
        "pred_exact_7 = model_exact_7.predict(dX_exact_test7)\n",
        "\n",
        "rmse_exact_7 = np.sqrt(mean_squared_error(pred_exact_7, y_test7))\n",
        "rmse_exact_7"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.76308289 7.79918792 8.         0.98911145 8.         0.98019056]. \t  -0.44500885348659536 \t -0.44173641078261416\n",
            "init   \t [ 5.3849587   5.01120464 13.          0.74994125  5.          0.88192131]. \t  -0.4488374676936292 \t -0.44173641078261416\n",
            "init   \t [ 3.30839249  3.9294231  12.          0.6440728  13.          0.41137564]. \t  -0.5669653799025498 \t -0.44173641078261416\n",
            "init   \t [9.29528191 2.6258377  5.         0.80027446 1.         0.86616513]. \t  -0.4661284696195417 \t -0.44173641078261416\n",
            "init   \t [ 1.74052764  7.90763512 14.          0.7244129   4.          0.77536887]. \t  -0.44173641078261416 \t -0.44173641078261416\n",
            "1      \t [3.43305102 3.00339076 8.         0.71322679 4.         0.33322219]. \t  -0.5720117286339644 \t -0.44173641078261416\n",
            "2      \t [ 7.6343627   1.31181598  5.          0.5769645  12.          0.84874959]. \t  -0.48924755963876193 \t -0.44173641078261416\n",
            "3      \t [ 9.12127254  9.64651695 14.          0.53624962  1.          0.38247449]. \t  -0.5784219709012162 \t -0.44173641078261416\n",
            "4      \t [ 4.51243396  9.79601217  8.          0.69773915 19.          0.69687222]. \t  -0.48670271968935647 \t -0.44173641078261416\n",
            "5      \t [ 8.06748781  9.6311716   5.          0.89165168 11.          0.90769253]. \t  -0.47467849504450826 \t -0.44173641078261416\n",
            "6      \t [ 9.84853722  9.76587477 12.          0.66808012 15.          0.83895675]. \t  -0.464300180546447 \t -0.44173641078261416\n",
            "7      \t [6.4915356  8.69600226 5.         0.66481282 2.         0.54976375]. \t  -0.5200045453863744 \t -0.44173641078261416\n",
            "8      \t [ 0.86712134  2.53401598  5.          0.75823741 18.          0.7884932 ]. \t  -0.4803186810834383 \t -0.44173641078261416\n",
            "9      \t [ 1.29932493  8.0055142  14.          0.51325501 19.          0.83654101]. \t  -0.4650670910068424 \t -0.44173641078261416\n",
            "10     \t [ 8.667653    2.8973594  14.          0.53825633 18.          0.10643456]. \t  -0.6861760180900373 \t -0.44173641078261416\n",
            "11     \t [ 9.89504759  2.29066357 10.          0.72562459  8.          0.65324681]. \t  -0.4791774660247169 \t -0.44173641078261416\n",
            "12     \t [ 9.94824081  0.39149062 12.          0.59113076  1.          0.49709751]. \t  -0.5081268521688408 \t -0.44173641078261416\n",
            "13     \t [ 9.60964399  8.52338044  5.          0.92796141 17.          0.69277707]. \t  -0.49365990657681785 \t -0.44173641078261416\n",
            "14     \t [ 3.34596156  0.67230605 11.          0.7366642  19.          0.29949997]. \t  -0.5628067831028366 \t -0.44173641078261416\n",
            "15     \t [ 9.60816545  8.33912452 11.          0.58307044  8.          0.36972424]. \t  -0.5786370523858145 \t -0.44173641078261416\n",
            "16     \t [ 0.47345619  1.31956961  7.          0.68987164 11.          0.9514247 ]. \t  -0.466265051270997 \t -0.44173641078261416\n",
            "17     \t [ 8.96081021  0.38413512 14.          0.90624871 12.          0.41560009]. \t  -0.565007973216504 \t -0.44173641078261416\n",
            "18     \t [ 9.71411962  1.71380043  5.          0.50726337 19.          0.71982498]. \t  -0.4912391351239423 \t -0.44173641078261416\n",
            "19     \t [ 0.18764716  0.58029521 13.          0.75995865  6.          0.69134836]. \t  -0.46855437085127594 \t -0.44173641078261416\n",
            "20     \t [ 1.77048754  9.0838881   6.          0.98375355 14.          0.13025417]. \t  -0.6815881776266557 \t -0.44173641078261416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.499924380514474"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0IPTSTbIl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174c3cdf-0730-49e4-b26a-671e58eac1af"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 8 \n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_exact_8 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_8 = dGPGO(surrogate_exact_8, Acquisition_new(util_exact), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_8 = exact_8.getResult()[0]\n",
        "params_exact_8['max_depth'] = int(params_exact_8['max_depth'])\n",
        "params_exact_8['min_child_weight'] = int(params_exact_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_exact_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_exact_8 = xgb.train(params_exact_8, dX_exact_train8)\n",
        "pred_exact_8 = model_exact_8.predict(dX_exact_test8)\n",
        "\n",
        "rmse_exact_8 = np.sqrt(mean_squared_error(pred_exact_8, y_test8))\n",
        "rmse_exact_8"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.5450023023990902 \t -0.47785117417083445\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.48415390639601685 \t -0.47785117417083445\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5514374023096014 \t -0.47785117417083445\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.509390123714371 \t -0.47785117417083445\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.47785117417083445 \t -0.47785117417083445\n",
            "1      \t [4.42022545 5.48487111 9.         0.97165909 3.         0.63617522]. \t  -0.4933129789895931 \t -0.47785117417083445\n",
            "2      \t [ 9.3432851   3.80536023 13.          0.82203895 19.          0.99569116]. \t  -0.4825289719254064 \t -0.47785117417083445\n",
            "3      \t [ 2.52429836  9.02824683 14.          0.59641093 17.          0.61934886]. \t  -0.5047477495526641 \t -0.47785117417083445\n",
            "4      \t [ 9.53473907  5.08424998 11.          0.50652828 18.          0.67121466]. \t  -0.5164144636241208 \t -0.47785117417083445\n",
            "5      \t [6.89072012 1.88822945 5.         0.9252956  8.         0.40577637]. \t  -0.5624845374478475 \t -0.47785117417083445\n",
            "6      \t [ 2.42575645  9.87357367  5.          0.61143882 19.          0.11833201]. \t  -0.6358778400175291 \t -0.47785117417083445\n",
            "7      \t [ 0.9931658   1.40870497 14.          0.50614818  6.          0.71944448]. \t  -0.48485914970894306 \t -0.47785117417083445\n",
            "8      \t [ 9.09148899  1.42093493  5.          0.88801001 17.          0.53231573]. \t  -0.5546570042431702 \t -0.47785117417083445\n",
            "9      \t [9.69554908 3.70013633 5.         0.72727157 1.         0.49530336]. \t  -0.5595421515943506 \t -0.47785117417083445\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.558869771245836 \t -0.47785117417083445\n",
            "11     \t [1.66316873 0.47469495 7.         0.60820298 1.         0.50689458]. \t  -0.5436576355709689 \t -0.47785117417083445\n",
            "12     \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.46870913378803636\u001b[0m \t -0.46870913378803636\n",
            "13     \t [5.63420638 9.7026496  5.         0.64869539 8.         0.22475566]. \t  -0.6389332388103492 \t -0.46870913378803636\n",
            "14     \t [ 0.24957112  3.26102415  5.          0.59065773 17.          0.91920206]. \t  -0.49962809668085056 \t -0.46870913378803636\n",
            "15     \t [ 1.12264609  9.56425304 13.          0.79308235  1.          0.93526753]. \t  -0.4740633204616465 \t -0.46870913378803636\n",
            "16     \t [10.         10.         15.          1.         12.51199706  1.        ]. \t  \u001b[92m-0.4678615693129947\u001b[0m \t -0.4678615693129947\n",
            "17     \t [ 9.3183355   3.29262794 14.          0.86350712  1.          0.18562794]. \t  -0.6359402471994396 \t -0.4678615693129947\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.6380286876071068 \t -0.4678615693129947\n",
            "19     \t [ 7.2771035   6.97077181  6.          0.96280365 18.          0.64814702]. \t  -0.5204979904417385 \t -0.4678615693129947\n",
            "20     \t [ 6.68744311  8.81728204 13.          0.79205386  1.          0.27645475]. \t  -0.6373135148611248 \t -0.4678615693129947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.487971093090443"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UroEj_RbLSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35826546-70f7-4f6b-ace9-25285d2ec13e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 9 \n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_exact_9 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_9 = dGPGO(surrogate_exact_9, Acquisition_new(util_exact), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_9 = exact_9.getResult()[0]\n",
        "params_exact_9['max_depth'] = int(params_exact_9['max_depth'])\n",
        "params_exact_9['min_child_weight'] = int(params_exact_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_exact_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_exact_9 = xgb.train(params_exact_9, dX_exact_train9)\n",
        "pred_exact_9 = model_exact_9.predict(dX_exact_test9)\n",
        "\n",
        "rmse_exact_9 = np.sqrt(mean_squared_error(pred_exact_9, y_test9))\n",
        "rmse_exact_9"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.10374154  5.01874592 11.          0.50377155  2.          0.29670281]. \t  -0.6545930802207814 \t -0.4584168030068045\n",
            "init   \t [ 4.18508181  2.48101168 13.          0.69794293  2.          0.25009871]. \t  -0.7166132091943936 \t -0.4584168030068045\n",
            "init   \t [ 8.78559086  9.50964032 13.          0.98395204 11.          0.90820641]. \t  -0.4584168030068045 \t -0.4584168030068045\n",
            "init   \t [ 6.66898973  5.47837783  6.          0.97165345 12.          0.72499481]. \t  -0.48839211091816903 \t -0.4584168030068045\n",
            "init   \t [ 8.24870465  4.65668475 13.          0.68760467  9.          0.98502332]. \t  -0.46354466019784824 \t -0.4584168030068045\n",
            "1      \t [6.73714319 2.39608167 5.         0.58130302 3.         0.163077  ]. \t  -0.7145926373770018 \t -0.4584168030068045\n",
            "2      \t [ 4.24955662  9.67331527 12.          0.64012695  7.          0.96617478]. \t  -0.4599552738922936 \t -0.4584168030068045\n",
            "3      \t [ 3.60566534  9.79805332 11.          0.62032576 16.          0.3578496 ]. \t  -0.6301914521165797 \t -0.4584168030068045\n",
            "4      \t [ 4.86601509  0.61279594  8.          0.72162785 18.          0.68911833]. \t  -0.4897803511428914 \t -0.4584168030068045\n",
            "5      \t [ 0.42678797  0.40430921 14.          0.96202194 10.          0.10119674]. \t  -0.7201378581044144 \t -0.4584168030068045\n",
            "6      \t [1.13863488 5.86180669 5.         0.9953054  3.         0.88639082]. \t  -0.4613511892865009 \t -0.4584168030068045\n",
            "7      \t [9.20185355 9.40235017 8.         0.60433558 1.         0.94540222]. \t  -0.4678077722812364 \t -0.4584168030068045\n",
            "8      \t [ 7.07313313  8.94084339  5.          0.99439529 19.          0.33798274]. \t  -0.6320298567588886 \t -0.4584168030068045\n",
            "9      \t [ 0.32747031  4.15373137 11.          0.86073055 16.          0.59344779]. \t  -0.4847822875702831 \t -0.4584168030068045\n",
            "10     \t [ 9.89680111  6.16985579 13.          0.58428363 17.          0.77251451]. \t  -0.4901785694142925 \t -0.4584168030068045\n",
            "11     \t [ 0.89628785  3.68529458  7.          0.8923295  10.          0.50355901]. \t  -0.4926976357635584 \t -0.4584168030068045\n",
            "12     \t [ 8.88256183  1.86768316 10.          0.88865081 13.          0.70876109]. \t  -0.4907324671255294 \t -0.4584168030068045\n",
            "13     \t [ 4.04453993  0.29784113 14.          0.62598795 17.          0.38851381]. \t  -0.6317392704296504 \t -0.4584168030068045\n",
            "14     \t [ 0.80844375  8.9121894   5.          0.56134557 18.          0.84901701]. \t  -0.496220520490058 \t -0.4584168030068045\n",
            "15     \t [4.62485155 9.65182222 5.         0.58896427 8.         0.36022331]. \t  -0.6322698857670199 \t -0.4584168030068045\n",
            "16     \t [ 8.77945628  5.51694736 12.          0.84613589  4.          0.64741755]. \t  -0.4798840189273464 \t -0.4584168030068045\n",
            "17     \t [ 3.89448326  9.77793126 11.          0.87312755  1.          0.25420157]. \t  -0.7204829959803113 \t -0.4584168030068045\n",
            "18     \t [9.96461831 1.05025842 5.         0.85654396 9.         0.88077949]. \t  -0.47633394276149926 \t -0.4584168030068045\n",
            "19     \t [ 2.87706677  6.16957719 14.          0.55246385 12.          0.92866915]. \t  -0.4653617556003965 \t -0.4584168030068045\n",
            "20     \t [ 2.2888102   0.47311819 10.          0.52676226  6.          0.86310876]. \t  -0.46328602169921157 \t -0.4584168030068045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6575353004618085"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VgaJOoJbOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22cfb2cc-fb60-4827-f6db-45a835005bd8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 10 \n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_exact_10 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_10 = dGPGO(surrogate_exact_10, Acquisition_new(util_exact), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_10 = exact_10.getResult()[0]\n",
        "params_exact_10['max_depth'] = int(params_exact_10['max_depth'])\n",
        "params_exact_10['min_child_weight'] = int(params_exact_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_exact_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_exact_10 = xgb.train(params_exact_10, dX_exact_train10)\n",
        "pred_exact_10 = model_exact_10.predict(dX_exact_test10)\n",
        "\n",
        "rmse_exact_10 = np.sqrt(mean_squared_error(pred_exact_10, y_test10))\n",
        "rmse_exact_10"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.71320643  0.20751949  5.          0.72150747 17.          0.12265456]. \t  -0.7090674967614334 \t -0.4737745634473992\n",
            "init   \t [ 7.0920801   2.65566127 13.          0.57518893 17.          0.83494165]. \t  -0.4737745634473992 \t -0.4737745634473992\n",
            "init   \t [ 3.36071584  8.90816531  6.          0.86087766 15.          0.75469196]. \t  -0.4755277191484213 \t -0.4737745634473992\n",
            "init   \t [ 5.40880931  1.31458152  8.          0.57108502 14.          0.62551123]. \t  -0.48811859212530173 \t -0.4737745634473992\n",
            "init   \t [1.82631436 8.26082248 6.         0.80888349 5.         0.15900694]. \t  -0.7057210222477256 \t -0.4737745634473992\n",
            "1      \t [8.31989768 3.09778055 7.         0.64798085 3.         0.98471878]. \t  \u001b[92m-0.46336171949490257\u001b[0m \t -0.46336171949490257\n",
            "2      \t [ 1.51483713  6.46720195 14.          0.87676044  8.          0.10934204]. \t  -0.7062921047588426 \t -0.46336171949490257\n",
            "3      \t [ 0.44494294  2.20797313 10.          0.76097539  2.          0.34290111]. \t  -0.6180922534763184 \t -0.46336171949490257\n",
            "4      \t [ 6.47425096  8.4482791  10.          0.69239539 11.          0.47622913]. \t  -0.5685254207537167 \t -0.46336171949490257\n",
            "5      \t [ 0.20896963  7.17600684 13.          0.63836859 16.          0.77244006]. \t  \u001b[92m-0.45403228727774253\u001b[0m \t -0.45403228727774253\n",
            "6      \t [ 9.38854854  7.91087361  8.          0.57475286 19.          0.33969987]. \t  -0.6147666257838221 \t -0.45403228727774253\n",
            "7      \t [ 9.16520307  0.72602801 12.          0.91999471  9.          0.54336218]. \t  -0.5572402945816037 \t -0.45403228727774253\n",
            "8      \t [ 8.33810851  9.8990204  14.          0.61893039  5.          0.69227045]. \t  -0.4851208634097359 \t -0.45403228727774253\n",
            "9      \t [ 0.12250572  0.33829451 12.          0.86512188 11.          0.46912604]. \t  -0.5583881911050124 \t -0.45403228727774253\n",
            "10     \t [ 0.8556149   1.34495008  5.          0.88655293 18.          0.15072152]. \t  -0.7048793212420641 \t -0.45403228727774253\n",
            "11     \t [9.53733075 9.92797623 5.         0.78642342 2.         0.9933049 ]. \t  -0.4656240229085077 \t -0.45403228727774253\n",
            "12     \t [1.69575137 2.28628662 5.         0.76046389 8.         0.60065268]. \t  -0.5134648983420089 \t -0.45403228727774253\n",
            "13     \t [ 6.15733989  0.33396443 13.          0.69796716  1.          0.92875212]. \t  \u001b[92m-0.4397628730770955\u001b[0m \t -0.4397628730770955\n",
            "14     \t [ 9.08727548  7.95071568  5.          0.91034544 13.          0.55647706]. \t  -0.5735578706821028 \t -0.4397628730770955\n",
            "15     \t [ 1.45996834  0.59889716 12.          0.82815203 19.          0.24922559]. \t  -0.7069273813967614 \t -0.4397628730770955\n",
            "16     \t [ 9.42421731  1.45800675  5.          0.89810349 11.          0.91467377]. \t  -0.4712218735320602 \t -0.4397628730770955\n",
            "17     \t [ 2.80210285  9.24943163 14.          0.85818339  2.          0.85428456]. \t  \u001b[92m-0.4387844304056118\u001b[0m \t -0.4387844304056118\n",
            "18     \t [7.60796195 7.8170794  5.         0.85145938 8.         0.57832563]. \t  -0.5106716993417335 \t -0.4387844304056118\n",
            "19     \t [ 8.10151611  9.78827602 14.          0.68048433 19.          0.19392154]. \t  -0.7116333725331636 \t -0.4387844304056118\n",
            "20     \t [ 1.11903492  9.93958695  9.          0.79898321 10.          0.44087381]. \t  -0.5625838427005065 \t -0.4387844304056118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.521324504618526"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51z87uHWbRGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10567b36-a1c6-4717-914e-fa4288fe2167"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 11 \n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_exact_11 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_11 = dGPGO(surrogate_exact_11, Acquisition_new(util_exact), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_11 = exact_11.getResult()[0]\n",
        "params_exact_11['max_depth'] = int(params_exact_11['max_depth'])\n",
        "params_exact_11['min_child_weight'] = int(params_exact_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_exact_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_exact_11 = xgb.train(params_exact_11, dX_exact_train11)\n",
        "pred_exact_11 = model_exact_11.predict(dX_exact_test11)\n",
        "\n",
        "rmse_exact_11 = np.sqrt(mean_squared_error(pred_exact_11, y_test11))\n",
        "rmse_exact_11"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.80269689  0.19475241  6.          0.59705781 13.          0.47818324]. \t  -0.5922349044250168 \t -0.49898623219170346\n",
            "init   \t [ 4.85427098  0.12780815  5.          0.91309068 14.          0.86571558]. \t  -0.49898623219170346 \t -0.49898623219170346\n",
            "init   \t [ 7.2996447   1.08736072 10.          0.92857712 18.          0.66910061]. \t  -0.5404544349803458 \t -0.49898623219170346\n",
            "init   \t [ 0.20483613  1.16737269  7.          0.57895615 16.          0.83644782]. \t  -0.5108833748715963 \t -0.49898623219170346\n",
            "init   \t [ 3.44624491  3.18798797 14.          0.54197657 15.          0.63958906]. \t  -0.5545314939891337 \t -0.49898623219170346\n",
            "1      \t [9.77136617 6.6548802  7.         0.51036649 9.         0.81011527]. \t  -0.5252491147925742 \t -0.49898623219170346\n",
            "2      \t [0.5279662  8.15331655 5.         0.83127487 9.         0.53242685]. \t  -0.5931601503405173 \t -0.49898623219170346\n",
            "3      \t [8.62555756 1.5478147  8.         0.99964468 2.         0.74874718]. \t  \u001b[92m-0.490360342610192\u001b[0m \t -0.490360342610192\n",
            "4      \t [ 0.90299561  9.42808632 14.          0.71344248  9.          0.5250902 ]. \t  -0.5778138976577691 \t -0.490360342610192\n",
            "5      \t [ 5.37271973  6.74878506 12.          0.69507758  3.          0.34109729]. \t  -0.5771395586534209 \t -0.490360342610192\n",
            "6      \t [10.         10.         14.84107387  1.          8.84107387  1.        ]. \t  \u001b[92m-0.4589029793726005\u001b[0m \t -0.4589029793726005\n",
            "7      \t [ 5.0402397   9.42026535  8.          0.71577    16.          0.52045581]. \t  -0.5873554197790632 \t -0.4589029793726005\n",
            "8      \t [10.          7.25651488 15.          1.         20.          1.        ]. \t  -0.46481753339047127 \t -0.4589029793726005\n",
            "9      \t [0.71954842 4.00816482 5.         0.76183414 1.         0.87022597]. \t  -0.49940652224028204 \t -0.4589029793726005\n",
            "10     \t [ 8.55410258  0.21556026 13.          0.58321534  8.          0.98283741]. \t  -0.4890790617970084 \t -0.4589029793726005\n",
            "11     \t [ 8.61765552  0.94349031 10.          0.85641829  4.          0.91576259]. \t  -0.47852786064513103 \t -0.4589029793726005\n",
            "12     \t [ 9.84847079  6.56311229  5.          0.73494874 19.          0.14302895]. \t  -0.6907115385769984 \t -0.4589029793726005\n",
            "13     \t [5.71344477 9.31745465 5.         0.62710446 1.         0.57350664]. \t  -0.56563042311184 \t -0.4589029793726005\n",
            "14     \t [ 8.40730244  5.3235921  14.          0.57824885  9.          0.96689527]. \t  -0.4867986869106529 \t -0.4589029793726005\n",
            "15     \t [ 0.23352034  0.69446734 12.          0.5389699   3.          0.65080983]. \t  -0.5571304657462128 \t -0.4589029793726005\n",
            "16     \t [ 3.71272268  4.57338191 11.          0.7134411  10.          0.83979859]. \t  -0.4880121906758131 \t -0.4589029793726005\n",
            "17     \t [ 0.80632707  5.39920626 11.          0.71823589 19.          0.37976587]. \t  -0.5836273896307068 \t -0.4589029793726005\n",
            "18     \t [ 6.19006075  3.41925871 11.          0.95286331  6.          0.39133224]. \t  -0.5745107435758626 \t -0.4589029793726005\n",
            "19     \t [0.77256475 1.22732493 7.         0.97692145 7.         0.9071396 ]. \t  -0.4728689341077363 \t -0.4589029793726005\n",
            "20     \t [7.46566309 0.73949694 7.         0.58224414 9.         0.25972328]. \t  -0.6904564119816002 \t -0.4589029793726005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.679812501694444"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8jZUeoWbTvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c542ce5e-feb2-4c4e-847f-cd6ccaf6d2a6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_exact_12 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_12 = dGPGO(surrogate_exact_12, Acquisition_new(util_exact), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_12 = exact_12.getResult()[0]\n",
        "params_exact_12['max_depth'] = int(params_exact_12['max_depth'])\n",
        "params_exact_12['min_child_weight'] = int(params_exact_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_exact_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_exact_12 = xgb.train(params_exact_12, dX_exact_train12)\n",
        "pred_exact_12 = model_exact_12.predict(dX_exact_test12)\n",
        "\n",
        "rmse_exact_12 = np.sqrt(mean_squared_error(pred_exact_12, y_test12))\n",
        "rmse_exact_12"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.54162842 7.40049697 6.         0.54321714 4.         0.11311747]. \t  -0.6840535846029854 \t -0.5032799564384677\n",
            "init   \t [ 9.18747008  9.00714854 14.          0.97847467 11.          0.35544552]. \t  -0.6305456734924068 \t -0.5032799564384677\n",
            "init   \t [ 6.06083184  9.44225136 14.          0.95626942  5.          0.56910342]. \t  -0.6144010633484512 \t -0.5032799564384677\n",
            "init   \t [ 5.52037633  4.85377414  7.          0.97886436 17.          0.78810441]. \t  -0.5032799564384677 \t -0.5032799564384677\n",
            "init   \t [ 0.20809798  1.35210178  5.          0.65494879 16.          0.36062811]. \t  -0.6516977337723864 \t -0.5032799564384677\n",
            "1      \t [9.46555822 8.57190559 5.         0.50164398 5.         0.71992807]. \t  -0.5227768283895782 \t -0.5032799564384677\n",
            "2      \t [9.04256367 2.61736915 8.         0.66026854 8.         0.14510453]. \t  -0.6776847411315782 \t -0.5032799564384677\n",
            "3      \t [6.03751892 2.08855857 8.         0.88966175 1.         0.6215545 ]. \t  -0.537741041779077 \t -0.5032799564384677\n",
            "4      \t [ 0.24796255  2.18203944 14.          0.56497025 17.          0.63132662]. \t  -0.5478949507371567 \t -0.5032799564384677\n",
            "5      \t [ 1.93384153  7.13950146  8.          0.85480597 18.          0.33734734]. \t  -0.6395317281169034 \t -0.5032799564384677\n",
            "6      \t [ 0.40359854  2.22527636 10.          0.60258213 10.          0.38255809]. \t  -0.6389275427144069 \t -0.5032799564384677\n",
            "7      \t [ 0.28427394  4.67296732 14.          0.84909523  4.          0.99176009]. \t  \u001b[92m-0.46369522867609564\u001b[0m \t -0.46369522867609564\n",
            "8      \t [ 7.63658847  0.39719075 14.          0.96199388 14.          0.84093877]. \t  -0.48258324270646796 \t -0.46369522867609564\n",
            "9      \t [ 7.0436136   8.68562161 15.          1.         19.35798404  1.        ]. \t  \u001b[92m-0.46093842220682946\u001b[0m \t -0.46093842220682946\n",
            "10     \t [ 4.27921374  9.2199845   6.          0.67076861 12.          0.56605459]. \t  -0.628155564863435 \t -0.46093842220682946\n",
            "11     \t [ 7.63578483  4.07501457 14.          0.97723751  1.          0.2033266 ]. \t  -0.6773954967709924 \t -0.46093842220682946\n",
            "12     \t [ 1.73522493  9.43858502 13.          0.55982395 15.          0.59746886]. \t  -0.5473582496522056 \t -0.46093842220682946\n",
            "13     \t [ 3.95801979  3.60127695  5.          0.58247512 11.          0.33687332]. \t  -0.6564398452227902 \t -0.46093842220682946\n",
            "14     \t [ 9.68766688  1.03665973  9.          0.58395515 19.          0.19787312]. \t  -0.6796636780038418 \t -0.46093842220682946\n",
            "15     \t [ 6.59943135  1.71178305 14.          0.69283152  8.          0.30459736]. \t  -0.635622313137239 \t -0.46093842220682946\n",
            "16     \t [5.99294447 7.0114806  5.         0.86352024 2.         0.47030879]. \t  -0.629371597372417 \t -0.46093842220682946\n",
            "17     \t [ 9.63630565  6.83547158  8.          0.6195521  13.          0.11712353]. \t  -0.6780822711639433 \t -0.46093842220682946\n",
            "18     \t [0.93465239 0.11560545 5.         0.52904926 2.         0.78190279]. \t  -0.5147104266754287 \t -0.46093842220682946\n",
            "19     \t [ 4.02953989  6.35914994 11.          0.73063178  8.          0.53793685]. \t  -0.6140059235484745 \t -0.46093842220682946\n",
            "20     \t [ 0.49442426  9.20422434 11.          0.70522852  1.          0.50168224]. \t  -0.62709850175482 \t -0.46093842220682946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.743572716730358"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snTrqE2RbWbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21130b5-cdbe-4d62-e785-8418a15e5ed3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 13 \n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_exact_13 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_13 = dGPGO(surrogate_exact_13, Acquisition_new(util_exact), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_13 = exact_13.getResult()[0]\n",
        "params_exact_13['max_depth'] = int(params_exact_13['max_depth'])\n",
        "params_exact_13['min_child_weight'] = int(params_exact_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_exact_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_exact_13 = xgb.train(params_exact_13, dX_exact_train13)\n",
        "pred_exact_13 = model_exact_13.predict(dX_exact_test13)\n",
        "\n",
        "rmse_exact_13 = np.sqrt(mean_squared_error(pred_exact_13, y_test13))\n",
        "rmse_exact_13"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.77702411  2.3754122  11.          0.94649135 13.          0.7827256 ]. \t  -0.5099204187421568 \t -0.5099204187421568\n",
            "init   \t [ 7.51661514  6.07343344 11.          0.69402149 11.          0.13153287]. \t  -0.7086153631136594 \t -0.5099204187421568\n",
            "init   \t [ 2.98449471  0.58512492 10.          0.73579614 12.          0.33065195]. \t  -0.6372461781857162 \t -0.5099204187421568\n",
            "init   \t [ 3.47581215  0.0941277  11.          0.86143432  8.          0.58454932]. \t  -0.5702017504451442 \t -0.5099204187421568\n",
            "init   \t [ 4.70137857  6.24432527 10.          0.8149145  18.          0.10784416]. \t  -0.7101125715665313 \t -0.5099204187421568\n",
            "1      \t [1.1119361  5.43221306 6.         0.56899303 8.         0.32100319]. \t  -0.6493427493033831 \t -0.5099204187421568\n",
            "2      \t [5.39023698 3.80105709 8.         0.54170057 1.         0.56798884]. \t  -0.621866591058205 \t -0.5099204187421568\n",
            "3      \t [ 0.5185863   5.23876151 13.          0.63798348  5.          0.7914799 ]. \t  \u001b[92m-0.5090480969691882\u001b[0m \t -0.5090480969691882\n",
            "4      \t [ 9.65518672  0.13040633  6.          0.63296628 18.          0.44133279]. \t  -0.632314879589983 \t -0.5090480969691882\n",
            "5      \t [ 9.80722669  7.22571611  7.          0.5026908  19.          0.92519376]. \t  -0.5113851690128051 \t -0.5090480969691882\n",
            "6      \t [ 6.75965929  9.42320667 14.          0.75932127  4.          0.51195623]. \t  -0.6206028595311828 \t -0.5090480969691882\n",
            "7      \t [9.95671825 0.88335607 5.         0.76593799 7.         0.60049246]. \t  -0.5952873237754599 \t -0.5090480969691882\n",
            "8      \t [ 1.88898055  9.92199995 14.          0.53783103 12.          0.63359705]. \t  -0.5738907345204808 \t -0.5090480969691882\n",
            "9      \t [0.32121091 9.03384774 6.         0.79493663 1.         0.29330571]. \t  -0.6544818879601484 \t -0.5090480969691882\n",
            "10     \t [ 9.64211232  3.05396831 11.          0.80784352  5.          0.67910498]. \t  -0.5747201434527256 \t -0.5090480969691882\n",
            "11     \t [ 1.60018805  0.54211528  7.          0.80910607 18.          0.55232873]. \t  -0.6209454018308087 \t -0.5090480969691882\n",
            "12     \t [10.        10.        15.         1.        14.9316955  1.       ]. \t  \u001b[92m-0.454006170940959\u001b[0m \t -0.454006170940959\n",
            "13     \t [7.99022981 8.23715587 5.         0.79172469 5.         0.39852784]. \t  -0.6543343020941466 \t -0.454006170940959\n",
            "14     \t [ 8.15066897  1.98276445 13.          0.64083395 19.          0.16153982]. \t  -0.7124294948180114 \t -0.454006170940959\n",
            "15     \t [ 2.27251819  8.81728667  7.          0.92054476 14.          0.74899609]. \t  -0.5198071138714226 \t -0.454006170940959\n",
            "16     \t [ 5.96896727  4.39812948  6.          0.6032279  13.          0.97913959]. \t  -0.5111397995838916 \t -0.454006170940959\n",
            "17     \t [ 9.08002101  9.03807456  5.          0.92565737 13.          0.80618835]. \t  -0.5365673692460949 \t -0.454006170940959\n",
            "18     \t [ 5.1333583   3.10658752 14.          0.74854361  1.          0.92539511]. \t  -0.4793091157473782 \t -0.454006170940959\n",
            "19     \t [ 0.65009462  9.91088996 13.          0.92448902  2.          0.37838494]. \t  -0.6501362550952002 \t -0.454006170940959\n",
            "20     \t [ 2.24744249  1.78720106 13.          0.8729686  19.          0.90941823]. \t  -0.47112056719100737 \t -0.454006170940959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.586816132339627"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAuEsXYbtOnC",
        "outputId": "6ad2392c-d852-4d0a-f6c7-d564c1d7d20c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 14 \n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_exact_14 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_14 = dGPGO(surrogate_exact_14, Acquisition_new(util_exact), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_14 = exact_14.getResult()[0]\n",
        "params_exact_14['max_depth'] = int(params_exact_14['max_depth'])\n",
        "params_exact_14['min_child_weight'] = int(params_exact_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_exact_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_exact_14 = xgb.train(params_exact_14, dX_exact_train14)\n",
        "pred_exact_14 = model_exact_14.predict(dX_exact_test14)\n",
        "\n",
        "rmse_exact_14 = np.sqrt(mean_squared_error(pred_exact_14, y_test14))\n",
        "rmse_exact_14"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.13943344  7.73165052 12.          0.6831412  11.          0.37876233]. \t  -0.558794499921046 \t -0.4448140077853998\n",
            "init   \t [ 9.57603739  5.13116712 14.          0.76959997 12.          0.71328228]. \t  -0.49979629433789113 \t -0.4448140077853998\n",
            "init   \t [5.34950319 2.47493539 5.         0.50293689 6.         0.29706373]. \t  -0.5741697988899073 \t -0.4448140077853998\n",
            "init   \t [ 2.94506579  3.45329697  8.          0.87620946 14.          0.9783044 ]. \t  -0.4448140077853998 \t -0.4448140077853998\n",
            "init   \t [ 1.11811929  1.73004086  5.          0.73745288 12.          0.20586008]. \t  -0.6214151152359092 \t -0.4448140077853998\n",
            "1      \t [ 6.50637223  2.67617722 14.          0.53562507  1.          0.16862152]. \t  -0.6294794339238933 \t -0.4448140077853998\n",
            "2      \t [ 9.97732733  0.9008687  13.          0.65397817 19.          0.96533011]. \t  -0.46274298441446626 \t -0.4448140077853998\n",
            "3      \t [ 0.28409124  4.13353348 13.          0.96339983  6.          0.90100709]. \t  \u001b[92m-0.42439805248956936\u001b[0m \t -0.42439805248956936\n",
            "4      \t [9.32373648 9.05676215 9.         0.53064322 3.         0.70657534]. \t  -0.5066999638361388 \t -0.42439805248956936\n",
            "5      \t [ 9.52454394  8.82757271  9.          0.90064956 19.          0.16022914]. \t  -0.6201852747998959 \t -0.42439805248956936\n",
            "6      \t [ 5.21920054  9.35580917 14.          0.81835368  4.          0.54800317]. \t  -0.4907240663685837 \t -0.42439805248956936\n",
            "7      \t [ 0.9687803   2.15143442 14.          0.51651811 18.          0.56051657]. \t  -0.5069463513331254 \t -0.42439805248956936\n",
            "8      \t [ 9.40430013  0.15700131  7.          0.99940272 12.          0.43771306]. \t  -0.50114980828739 \t -0.42439805248956936\n",
            "9      \t [0.38652312 9.54840602 9.         0.51100563 1.         0.36720837]. \t  -0.5713884526711975 \t -0.42439805248956936\n",
            "10     \t [ 2.08494663  9.70581169 14.          0.64307382  3.          0.10613693]. \t  -0.6314022013205511 \t -0.42439805248956936\n",
            "11     \t [ 1.55781918  0.46142569 10.          0.80240433 16.          0.22604037]. \t  -0.6223833972775541 \t -0.42439805248956936\n",
            "12     \t [ 8.32471637  8.64868248  5.          0.80719895 12.          0.64183859]. \t  -0.5064508572748014 \t -0.42439805248956936\n",
            "13     \t [ 1.40044045  8.51657075 10.          0.9376679  19.          0.10378975]. \t  -0.619787592651992 \t -0.42439805248956936\n",
            "14     \t [ 6.67907222  1.16181929 11.          0.57892751  8.          0.43787261]. \t  -0.5089510029010434 \t -0.42439805248956936\n",
            "15     \t [ 7.88588096  7.12943097 15.          1.         18.42182682  1.        ]. \t  -0.4295249225468953 \t -0.42439805248956936\n",
            "16     \t [3.08911491 9.05396381 5.         0.56731196 7.         0.30286489]. \t  -0.563723404842588 \t -0.42439805248956936\n",
            "17     \t [10.         10.         15.          1.          7.70471767  1.        ]. \t  \u001b[92m-0.42254397418764056\u001b[0m \t -0.42254397418764056\n",
            "18     \t [9.95902868 1.23916696 7.         0.60336262 2.         0.41962125]. \t  -0.5646831606854809 \t -0.42254397418764056\n",
            "19     \t [1.20508963 0.14448649 8.         0.50927505 2.         0.71549909]. \t  -0.48572060865355005 \t -0.42254397418764056\n",
            "20     \t [ 3.38602001  5.52665599  5.          0.6443898  19.          0.93037694]. \t  -0.4749460845017488 \t -0.42254397418764056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.567023503468326"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxvE7Irbbj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535b0ae1-fec6-4fe6-d344-b5c7c1a9069a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 15 \n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_exact_15 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_15 = dGPGO(surrogate_exact_15, Acquisition_new(util_exact), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_15 = exact_15.getResult()[0]\n",
        "params_exact_15['max_depth'] = int(params_exact_15['max_depth'])\n",
        "params_exact_15['min_child_weight'] = int(params_exact_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_exact_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_exact_15 = xgb.train(params_exact_15, dX_exact_train15)\n",
        "pred_exact_15 = model_exact_15.predict(dX_exact_test15)\n",
        "\n",
        "rmse_exact_15 = np.sqrt(mean_squared_error(pred_exact_15, y_test15))\n",
        "rmse_exact_15"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.48817697  1.78895925 12.          0.55549316  8.          0.93397854]. \t  -0.48943791400638287 \t -0.48943791400638287\n",
            "init   \t [ 0.24953032  8.22298097 12.          0.62494951 11.          0.12924598]. \t  -0.6992441679399787 \t -0.48943791400638287\n",
            "init   \t [ 5.02017228  5.50882771 11.          0.85295832 19.          0.13548008]. \t  -0.69945028775584 \t -0.48943791400638287\n",
            "init   \t [2.0023081  9.98543403 7.         0.6295772  2.         0.526127  ]. \t  -0.6264981748624211 \t -0.48943791400638287\n",
            "init   \t [ 5.09715306  9.45038417 11.          0.7388277  16.          0.22739973]. \t  -0.6964615643692806 \t -0.48943791400638287\n",
            "1      \t [ 0.29158961  4.9949242  12.          0.89124583  3.          0.67554049]. \t  -0.56524595824364 \t -0.48943791400638287\n",
            "2      \t [3.68214008 4.55748717 6.         0.60488381 8.         0.88973248]. \t  -0.494310985641296 \t -0.48943791400638287\n",
            "3      \t [9.75991344 6.15203198 6.         0.65490407 1.         0.73816291]. \t  -0.49903240523735964 \t -0.48943791400638287\n",
            "4      \t [ 0.04347405  0.16019908 11.          0.96902942  9.          0.40185268]. \t  -0.6876830293606538 \t -0.48943791400638287\n",
            "5      \t [ 8.02289731 10.         15.          1.          8.7668462   1.        ]. \t  \u001b[92m-0.4556272723840594\u001b[0m \t -0.4556272723840594\n",
            "6      \t [1.13261559 9.28349969 7.         0.98964629 7.         0.37032959]. \t  -0.6828004414762805 \t -0.4556272723840594\n",
            "7      \t [ 5.30770728  0.52044454  5.          0.75000665 15.          0.36204331]. \t  -0.683131978328503 \t -0.4556272723840594\n",
            "8      \t [ 7.76649344  1.163194   14.          0.89685392  1.          0.90591323]. \t  -0.46131653887574997 \t -0.4556272723840594\n",
            "9      \t [ 0.53269319  3.10786722 14.          0.59617872 16.          0.1338107 ]. \t  -0.700992505050063 \t -0.4556272723840594\n",
            "10     \t [ 0.40185461  0.03468168  9.          0.58775784 15.          0.13532287]. \t  -0.6983586430175102 \t -0.4556272723840594\n",
            "11     \t [ 2.97928851  7.73905112  6.          0.87870551 18.          0.36796416]. \t  -0.6830838005681892 \t -0.4556272723840594\n",
            "12     \t [3.02150875 0.49910736 5.         0.91633249 1.         0.11262301]. \t  -0.6990999780460975 \t -0.4556272723840594\n",
            "13     \t [ 9.52301108  9.56759639  5.          0.87397797 11.          0.23091707]. \t  -0.6978364801116691 \t -0.4556272723840594\n",
            "14     \t [ 8.83554548  9.97893931 14.          0.88835544  1.          0.8298738 ]. \t  -0.47714205578923014 \t -0.4556272723840594\n",
            "15     \t [ 7.70971055  5.9598343  10.          0.83735899 18.          0.36826156]. \t  -0.6853727999168309 \t -0.4556272723840594\n",
            "16     \t [10. 10. 15.  1. 20.  1.]. \t  -0.4662835424155933 \t -0.4556272723840594\n",
            "17     \t [ 8.79093169  0.86787487 14.          0.80222436 14.          0.17525565]. \t  -0.6977535247740003 \t -0.4556272723840594\n",
            "18     \t [ 3.51479622  9.74194901 13.          0.90442852  5.          0.62459375]. \t  -0.5589827061205567 \t -0.4556272723840594\n",
            "19     \t [ 5.64647885  9.07152022 10.          0.83229506 10.          0.27661589]. \t  -0.6978237750995275 \t -0.4556272723840594\n",
            "20     \t [ 3.70936727  2.056093   14.          0.59327467  6.          0.45934375]. \t  -0.6241376584484061 \t -0.4556272723840594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.564641872479156"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TaP6RoGuiNT",
        "outputId": "784329cd-e741-4277-a0ac-1c7fa324235b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 16 \n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_exact_16 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_16 = dGPGO(surrogate_exact_16, Acquisition_new(util_exact), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_16 = exact_16.getResult()[0]\n",
        "params_exact_16['max_depth'] = int(params_exact_16['max_depth'])\n",
        "params_exact_16['min_child_weight'] = int(params_exact_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_exact_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_exact_16 = xgb.train(params_exact_16, dX_exact_train16)\n",
        "pred_exact_16 = model_exact_16.predict(dX_exact_test16)\n",
        "\n",
        "rmse_exact_16 = np.sqrt(mean_squared_error(pred_exact_16, y_test16))\n",
        "rmse_exact_16"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.23291079 5.23163341 6.         0.65430839 5.         0.30077285]. \t  -0.6357813258069683 \t -0.6345701590947206\n",
            "init   \t [6.88726162 1.63731425 7.         0.97050543 2.         0.25392012]. \t  -0.7029752724132097 \t -0.6345701590947206\n",
            "init   \t [ 5.94328983  5.6393473   5.          0.67602695 19.          0.42538144]. \t  -0.6345701590947206 \t -0.6345701590947206\n",
            "init   \t [ 0.88741148  3.08148142 14.          0.56043938  9.          0.27515386]. \t  -0.7076230970293895 \t -0.6345701590947206\n",
            "init   \t [ 2.74631586  1.30996118 11.          0.52160786  8.          0.27956463]. \t  -0.7061563820165734 \t -0.6345701590947206\n",
            "1      \t [ 7.8937256   1.5972923  14.          0.61610774 17.          0.78739284]. \t  \u001b[92m-0.5498317738591506\u001b[0m \t -0.5498317738591506\n",
            "2      \t [ 9.01655783  8.21383177  9.          0.60772965 10.          0.9401803 ]. \t  \u001b[92m-0.4830822737254163\u001b[0m \t -0.4830822737254163\n",
            "3      \t [ 4.35132073  9.89698316 12.          0.94137984 16.          0.57741056]. \t  -0.5389659972108385 \t -0.4830822737254163\n",
            "4      \t [ 3.38377852  9.31285251 11.          0.88244942  1.          0.67627774]. \t  -0.5404220213450739 \t -0.4830822737254163\n",
            "5      \t [ 9.63904847  1.13624975 14.          0.67706869  2.          0.95236673]. \t  \u001b[92m-0.4641319096590556\u001b[0m \t -0.4641319096590556\n",
            "6      \t [ 0.19317903  0.6596816   6.          0.86233301 15.          0.41906313]. \t  -0.6308098904949025 \t -0.4641319096590556\n",
            "7      \t [ 3.86147645  9.68905939  5.          0.65198689 12.          0.68426055]. \t  -0.5557826482727706 \t -0.4641319096590556\n",
            "8      \t [ 1.22130867  0.64008351 12.          0.59515137 19.          0.65193522]. \t  -0.5486769508942775 \t -0.4641319096590556\n",
            "9      \t [8.31596139 8.08775071 5.         0.5698323  1.         0.11769544]. \t  -0.708927036050009 \t -0.4641319096590556\n",
            "10     \t [ 9.39421065  1.15238895 10.          0.86257926 10.          0.82976642]. \t  -0.5380808068994449 \t -0.4641319096590556\n",
            "11     \t [ 8.63952355  8.87914214 14.          0.57943185  3.          0.83429859]. \t  -0.5495434658820855 \t -0.4641319096590556\n",
            "12     \t [ 2.23523952  3.62733473 12.          0.51556206  2.          0.2036675 ]. \t  -0.7124687886931153 \t -0.4641319096590556\n",
            "13     \t [10.         10.         15.          1.         11.03041881  1.        ]. \t  \u001b[92m-0.4538384944380411\u001b[0m \t -0.4538384944380411\n",
            "14     \t [ 5.94853197  4.92024227  5.          0.75769119 13.          0.52349427]. \t  -0.5762899773358159 \t -0.4538384944380411\n",
            "15     \t [10. 10. 15.  1. 20.  1.]. \t  -0.46362391476067477 \t -0.4538384944380411\n",
            "16     \t [ 9.28336661  6.57187836 11.          0.54297007 17.          0.56210295]. \t  -0.5785906968518683 \t -0.4538384944380411\n",
            "17     \t [ 0.03255143  5.62206767 10.          0.695628   16.          0.90161277]. \t  -0.4734497149203408 \t -0.4538384944380411\n",
            "18     \t [ 3.32190637  9.36212837 13.          0.54407927 10.          0.12468989]. \t  -0.7062938512175606 \t -0.4538384944380411\n",
            "19     \t [ 9.76289081  0.88879743  6.          0.50453768 15.          0.73005629]. \t  -0.5522738715689526 \t -0.4538384944380411\n",
            "20     \t [ 7.43573851  4.55544477 14.          0.86207767  8.          0.45183313]. \t  -0.5660380755246425 \t -0.4538384944380411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.568091354119152"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiOaMUmgulbx",
        "outputId": "b245c94b-2bdc-4a2c-8821-a335a6cbb988"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 17 \n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_exact_17 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_17 = dGPGO(surrogate_exact_17, Acquisition_new(util_exact), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_17 = exact_17.getResult()[0]\n",
        "params_exact_17['max_depth'] = int(params_exact_17['max_depth'])\n",
        "params_exact_17['min_child_weight'] = int(params_exact_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_exact_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_exact_17 = xgb.train(params_exact_17, dX_exact_train17)\n",
        "pred_exact_17 = model_exact_17.predict(dX_exact_test17)\n",
        "\n",
        "rmse_exact_17 = np.sqrt(mean_squared_error(pred_exact_17, y_test17))\n",
        "rmse_exact_17"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.94665003  5.30586756 11.          0.94443241 14.          0.80828691]. \t  -0.48092361225642916 \t -0.48092361225642916\n",
            "init   \t [ 6.56333522  6.37520896 12.          0.81487881 18.          0.42203224]. \t  -0.634455605137701 \t -0.48092361225642916\n",
            "init   \t [ 9.45683187  0.6004468  11.          0.5171566  10.          0.53881211]. \t  -0.6046684392629649 \t -0.48092361225642916\n",
            "init   \t [2.72705857 1.19063434 6.         0.74176431 6.         0.10101151]. \t  -0.6562801618178493 \t -0.48092361225642916\n",
            "init   \t [ 4.77631812  5.24671297 13.          0.66254476 19.          0.36708086]. \t  -0.6415104035419145 \t -0.48092361225642916\n",
            "1      \t [ 0.65702322  5.79284078 13.          0.75136902  1.          0.30306068]. \t  -0.6424064809093688 \t -0.48092361225642916\n",
            "2      \t [ 6.93446178  8.68032298 13.          0.78195789  7.          0.91906958]. \t  \u001b[92m-0.47529593119031155\u001b[0m \t -0.47529593119031155\n",
            "3      \t [9.72843652 3.88893279 9.         0.6901555  1.         0.31608219]. \t  -0.6407338037654788 \t -0.47529593119031155\n",
            "4      \t [ 9.65057736  8.52725784  5.          0.68420234 13.          0.40008732]. \t  -0.6446152251367371 \t -0.47529593119031155\n",
            "5      \t [ 4.97204887  2.40072226  5.          0.54268748 19.          0.30995407]. \t  -0.6473150344064219 \t -0.47529593119031155\n",
            "6      \t [0.12174033 8.73496008 5.         0.89827646 5.         0.85354798]. \t  -0.4993728869548043 \t -0.47529593119031155\n",
            "7      \t [ 2.91443079  0.16723755 13.          0.598201    6.          0.91729605]. \t  -0.48363736808522184 \t -0.47529593119031155\n",
            "8      \t [7.20615247 9.36901627 6.         0.85465034 7.         0.6878262 ]. \t  -0.5223074457605629 \t -0.47529593119031155\n",
            "9      \t [ 9.02586164  0.59354638 10.          0.86038693 18.          0.90794111]. \t  -0.48184301693459747 \t -0.47529593119031155\n",
            "10     \t [ 1.66641474  7.47633023  5.          0.68203645 17.          0.15512069]. \t  -0.6612773320145335 \t -0.47529593119031155\n",
            "11     \t [ 0.12410542  7.60180472 13.          0.97425003  8.          0.76245421]. \t  \u001b[92m-0.47195842108145214\u001b[0m \t -0.47195842108145214\n",
            "12     \t [ 8.39240649 10.         15.          1.         13.24506875  1.        ]. \t  \u001b[92m-0.47173619960497104\u001b[0m \t -0.47173619960497104\n",
            "13     \t [9.59190042 1.26233772 5.         0.55398722 6.         0.46259714]. \t  -0.6127976173878007 \t -0.47173619960497104\n",
            "14     \t [ 9.06002681  9.03779351 14.          0.60614154  1.          0.16421461]. \t  -0.6627557424336108 \t -0.47173619960497104\n",
            "15     \t [ 0.59590325  0.30071901  8.          0.9148403  16.          0.62633871]. \t  -0.507330476870976 \t -0.47173619960497104\n",
            "16     \t [ 8.69529605  0.27226865 14.          0.90636352  1.          0.39638219]. \t  -0.6352506435607296 \t -0.47173619960497104\n",
            "17     \t [ 5.5997101   2.80089542 11.          0.91397635  4.          0.58148876]. \t  -0.49267256921662134 \t -0.47173619960497104\n",
            "18     \t [10. 10. 15.  1. 20.  1.]. \t  -0.478716870694923 \t -0.47173619960497104\n",
            "19     \t [ 2.76368647  8.96162764  6.          0.57935111 11.          0.22903211]. \t  -0.6599396664254092 \t -0.47173619960497104\n",
            "20     \t [ 7.9822505   0.1930237   5.          0.88184327 12.          0.30139498]. \t  -0.6375373096353469 \t -0.47173619960497104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.661205621669708"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H4MWSXFcZjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da42656-14f5-48b1-d7db-f71686057f05"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 18 \n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_exact_18 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_18 = dGPGO(surrogate_exact_18, Acquisition_new(util_exact), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_18 = exact_18.getResult()[0]\n",
        "params_exact_18['max_depth'] = int(params_exact_18['max_depth'])\n",
        "params_exact_18['min_child_weight'] = int(params_exact_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_exact_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_exact_18 = xgb.train(params_exact_18, dX_exact_train18)\n",
        "pred_exact_18 = model_exact_18.predict(dX_exact_test18)\n",
        "\n",
        "rmse_exact_18 = np.sqrt(mean_squared_error(pred_exact_18, y_test18))\n",
        "rmse_exact_18"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.50374242 5.05453374 6.         0.59092011 3.         0.28357516]. \t  -0.6408812889056655 \t -0.45563901799042095\n",
            "init   \t [0.11506734 4.26891483 9.         0.81785956 5.         0.63489043]. \t  -0.5015618704267137 \t -0.45563901799042095\n",
            "init   \t [ 2.8861259   6.35547834 11.          0.64267955 14.          0.27877092]. \t  -0.6438813467576416 \t -0.45563901799042095\n",
            "init   \t [6.57189031 6.99655629 8.         0.63235896 4.         0.52894035]. \t  -0.5434165841486521 \t -0.45563901799042095\n",
            "init   \t [ 6.66600348  2.11312037 14.          0.74363461  4.          0.73174558]. \t  -0.45563901799042095 \t -0.45563901799042095\n",
            "1      \t [ 8.67093232  0.11649132  5.          0.92962202 15.          0.53672863]. \t  -0.5446894741691042 \t -0.45563901799042095\n",
            "2      \t [ 8.43851229  2.41114508 13.          0.75771586 19.          0.86905071]. \t  \u001b[92m-0.4388122674991237\u001b[0m \t -0.4388122674991237\n",
            "3      \t [ 9.44281001  9.01534322  7.          0.99142432 16.          0.37631199]. \t  -0.6085057891370991 \t -0.4388122674991237\n",
            "4      \t [10.         10.         13.95119006  1.          8.95119006  1.        ]. \t  \u001b[92m-0.4266957287400768\u001b[0m \t -0.4266957287400768\n",
            "5      \t [ 1.97643014  8.37982471  5.          0.63246176 17.          0.45403539]. \t  -0.5418357136590688 \t -0.4266957287400768\n",
            "6      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.42588161300140354\u001b[0m \t -0.42588161300140354\n",
            "7      \t [ 0.02961082  2.38062589  5.          0.60457904 13.          0.70655364]. \t  -0.5083627676422857 \t -0.42588161300140354\n",
            "8      \t [ 6.63763513  1.21594617 14.          0.92753462 12.          0.25002233]. \t  -0.6447227465520562 \t -0.42588161300140354\n",
            "9      \t [ 4.32027405  9.96851926 14.          0.70719552  1.          0.80495431]. \t  -0.44551025370789715 \t -0.42588161300140354\n",
            "10     \t [ 0.89534385  1.39878838 10.          0.85395723 19.          0.70708679]. \t  -0.49227382562309224 \t -0.42588161300140354\n",
            "11     \t [ 2.69640701  8.7583803   5.          0.72267254 10.          0.8528077 ]. \t  -0.4752181963523114 \t -0.42588161300140354\n",
            "12     \t [9.0137211  3.37251846 8.         0.99529789 9.         0.14915462]. \t  -0.6435423820932967 \t -0.42588161300140354\n",
            "13     \t [0.58655573 8.89860432 5.         0.62593909 1.         0.46819727]. \t  -0.5449038923555584 \t -0.42588161300140354\n",
            "14     \t [ 1.64238572  0.62580745 11.          0.79203735 11.          0.21517242]. \t  -0.643614453864466 \t -0.42588161300140354\n",
            "15     \t [ 2.72240994  9.88599464 12.          0.79869012  8.          0.52915179]. \t  -0.5404306975471715 \t -0.42588161300140354\n",
            "16     \t [2.5556895  0.91787864 5.         0.57129972 6.         0.40466558]. \t  -0.6123895871923584 \t -0.42588161300140354\n",
            "17     \t [ 2.88988266  8.62215245 13.          0.75211659 19.          0.58952824]. \t  -0.4917333227604216 \t -0.42588161300140354\n",
            "18     \t [ 7.16872281  3.36587781  7.          0.60098211 19.          0.72044147]. \t  -0.46380371702866086 \t -0.42588161300140354\n",
            "19     \t [ 7.53809313  9.75652362 13.          0.61919632 14.          0.91992568]. \t  -0.4388210663948877 \t -0.42588161300140354\n",
            "20     \t [ 1.47346798  3.42280397 13.          0.87173303  1.          0.71719115]. \t  -0.4411780295132647 \t -0.42588161300140354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.787850327309379"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-zaPbk2uuzH",
        "outputId": "f89cb8df-ca0d-44e6-dcbf-d4f80f659a00"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 19 \n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_exact_19 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_19 = dGPGO(surrogate_exact_19, Acquisition_new(util_exact), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_19 = exact_19.getResult()[0]\n",
        "params_exact_19['max_depth'] = int(params_exact_19['max_depth'])\n",
        "params_exact_19['min_child_weight'] = int(params_exact_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_exact_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_exact_19 = xgb.train(params_exact_19, dX_exact_train19)\n",
        "pred_exact_19 = model_exact_19.predict(dX_exact_test19)\n",
        "\n",
        "rmse_exact_19 = np.sqrt(mean_squared_error(pred_exact_19, y_test19))\n",
        "rmse_exact_19"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.97533602  7.61249717 13.          0.85765469 11.          0.39830191]. \t  -0.5851081958447377 \t -0.4870287725699859\n",
            "init   \t [ 0.82999565  6.71977081  6.          0.50407413 19.          0.67209466]. \t  -0.5245729886945251 \t -0.4870287725699859\n",
            "init   \t [ 2.15923256  5.49027432 12.          0.52588686 10.          0.20235326]. \t  -0.677999989539271 \t -0.4870287725699859\n",
            "init   \t [4.99659267 1.52108422 6.         0.73481085 4.         0.71949465]. \t  -0.4940235599803803 \t -0.4870287725699859\n",
            "init   \t [ 3.72927156  9.46160045  5.          0.80554614 18.          0.97708466]. \t  -0.4870287725699859 \t -0.4870287725699859\n",
            "1      \t [ 8.33060043  1.42030563  8.          0.92863724 14.          0.78606141]. \t  \u001b[92m-0.48699206254226385\u001b[0m \t -0.48699206254226385\n",
            "2      \t [ 7.89674065  9.31460122 11.          0.96158687  5.          0.67754853]. \t  -0.49785049104905016 \t -0.48699206254226385\n",
            "3      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.45072062300002613\u001b[0m \t -0.45072062300002613\n",
            "4      \t [ 2.48547521  0.16821047 14.          0.50102007  1.          0.10104193]. \t  -0.6865863454987299 \t -0.45072062300002613\n",
            "5      \t [0.25768796 8.33414072 6.         0.99948369 4.         0.66680619]. \t  -0.5074502274890225 \t -0.45072062300002613\n",
            "6      \t [ 1.10650842  9.77537077 14.          0.83431584  1.          0.5884296 ]. \t  -0.5035756891739214 \t -0.45072062300002613\n",
            "7      \t [ 9.97057985  1.83320304 14.          0.52757417  6.          0.12389204]. \t  -0.6775116303533313 \t -0.45072062300002613\n",
            "8      \t [ 8.56732982  8.91986104 14.          0.62397749 13.          0.90657477]. \t  -0.4704331127149441 \t -0.45072062300002613\n",
            "9      \t [7.08488952 8.05904216 5.         0.93088713 1.         0.30662699]. \t  -0.5910777842699757 \t -0.45072062300002613\n",
            "10     \t [ 0.60329519  1.30823827 14.          0.81928223 17.          0.54931634]. \t  -0.5614358389874498 \t -0.45072062300002613\n",
            "11     \t [ 9.47453996  8.80927889  6.          0.60975678 10.          0.22761105]. \t  -0.6786997262456852 \t -0.45072062300002613\n",
            "12     \t [ 7.79929955  2.13050742 14.          0.83877886 17.          0.7783859 ]. \t  -0.48184598472537166 \t -0.45072062300002613\n",
            "13     \t [ 2.37824388  4.47272578  5.          0.97943263 10.          0.125395  ]. \t  -0.6793722154876207 \t -0.45072062300002613\n",
            "14     \t [ 0.11534005  0.08986117  8.          0.95341643 19.          0.48014014]. \t  -0.5639400804347694 \t -0.45072062300002613\n",
            "15     \t [ 2.45146849  8.32898472 14.          0.63227537 18.          0.3341326 ]. \t  -0.5774093941622171 \t -0.45072062300002613\n",
            "16     \t [ 8.69943878  7.36890408  7.          0.86891549 19.          0.36276071]. \t  -0.5930096394832391 \t -0.45072062300002613\n",
            "17     \t [ 1.9120179   0.52119504 12.          0.71043482  7.          0.33341372]. \t  -0.5854472398210584 \t -0.45072062300002613\n",
            "18     \t [9.32488714 1.90315632 6.         0.7513195  8.         0.80108048]. \t  -0.49835441465198543 \t -0.45072062300002613\n",
            "19     \t [ 1.50133196  1.19414112  9.          0.60450761 13.          0.34743418]. \t  -0.5811044554703819 \t -0.45072062300002613\n",
            "20     \t [ 3.82670531  7.06245628  8.          0.7747773  14.          0.11384763]. \t  -0.67893464439286 \t -0.45072062300002613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.72073900839424"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvkuHKlQuxRy",
        "outputId": "06b245e8-e0c9-449b-d4f6-95b57f45f365"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 20 \n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_exact_20 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_20 = dGPGO(surrogate_exact_20, Acquisition_new(util_exact), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_20 = exact_20.getResult()[0]\n",
        "params_exact_20['max_depth'] = int(params_exact_20['max_depth'])\n",
        "params_exact_20['min_child_weight'] = int(params_exact_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_exact_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_exact_20 = xgb.train(params_exact_20, dX_exact_train20)\n",
        "pred_exact_20 = model_exact_20.predict(dX_exact_test20)\n",
        "\n",
        "rmse_exact_20 = np.sqrt(mean_squared_error(pred_exact_20, y_test20))\n",
        "rmse_exact_20"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.88130801  8.97713728 14.          0.81074445  8.          0.95540649]. \t  -0.4485352768858121 \t -0.4485352768858121\n",
            "init   \t [6.72865655 0.41173329 8.         0.6361582  7.         0.76174061]. \t  -0.47208091450542966 \t -0.4485352768858121\n",
            "init   \t [ 4.77387703  8.66202323 10.          0.51833215  7.          0.10123387]. \t  -0.7316473600840852 \t -0.4485352768858121\n",
            "init   \t [ 5.75489985  4.74524381  8.          0.78084343 15.          0.26643049]. \t  -0.7314226542252507 \t -0.4485352768858121\n",
            "init   \t [ 4.53444     4.47342833  8.          0.91974896 18.          0.35997552]. \t  -0.6494230116583573 \t -0.4485352768858121\n",
            "1      \t [ 7.96566073  7.15509535  7.          0.79906691 11.          0.34132075]. \t  -0.6502222800629637 \t -0.4485352768858121\n",
            "2      \t [ 1.98667885  1.35773177 13.          0.57199118  2.          0.39498908]. \t  -0.6661970064789862 \t -0.4485352768858121\n",
            "3      \t [ 3.00704909  2.42524876 14.          0.95062509 15.          0.83595087]. \t  -0.4577111911042998 \t -0.4485352768858121\n",
            "4      \t [ 8.94683774  7.07743462 15.          1.         17.10232747  1.        ]. \t  \u001b[92m-0.43833021580133885\u001b[0m \t -0.43833021580133885\n",
            "5      \t [8.0846212  5.99993376 6.         0.83941375 1.         0.46362124]. \t  -0.5681524533474447 \t -0.43833021580133885\n",
            "6      \t [0.61316554 1.36115087 5.         0.87944956 1.         0.21740227]. \t  -0.7332228664111685 \t -0.43833021580133885\n",
            "7      \t [ 0.72788527  2.26655356 10.          0.97273032 15.          0.85843758]. \t  -0.45084600130879665 \t -0.43833021580133885\n",
            "8      \t [4.62702633 9.6876243  5.         0.9923406  1.         0.38132515]. \t  -0.6522172440468724 \t -0.43833021580133885\n",
            "9      \t [ 1.44692101  9.96202174 12.          0.76092884 18.          0.10523817]. \t  -0.731303133317551 \t -0.43833021580133885\n",
            "10     \t [ 1.01814405  9.81807131 14.          0.52908047  1.          0.89345697]. \t  -0.4570018264326601 \t -0.43833021580133885\n",
            "11     \t [ 0.81648933  6.00906202 12.          0.6106182   6.          0.17068603]. \t  -0.7370223446090218 \t -0.43833021580133885\n",
            "12     \t [ 9.62878188  0.47045758 13.          0.73408624  1.          0.43054124]. \t  -0.5746597113890792 \t -0.43833021580133885\n",
            "13     \t [ 8.98143836  8.7693897  12.          0.89468403 12.          0.66552576]. \t  -0.49793969514395464 \t -0.43833021580133885\n",
            "14     \t [ 9.55715055  0.18843871 11.          0.63478984 17.          0.93511154]. \t  -0.4708818143147438 \t -0.43833021580133885\n",
            "15     \t [ 5.37444991  0.3056877   9.          0.99777328 15.          0.5903115 ]. \t  -0.5008636074552522 \t -0.43833021580133885\n",
            "16     \t [ 8.81579675  8.3667984  12.          0.69895873  2.          0.28897175]. \t  -0.656636562688776 \t -0.43833021580133885\n",
            "17     \t [ 8.2503326   1.61920179 14.          0.80641713 11.          0.5764476 ]. \t  -0.4970739671390835 \t -0.43833021580133885\n",
            "18     \t [ 4.4224035   9.83430007  5.          0.89681877 17.          0.40799802]. \t  -0.6495318911805994 \t -0.43833021580133885\n",
            "19     \t [ 9.64898386  6.20494423  6.          0.8469719  19.          0.57533849]. \t  -0.5209730469573338 \t -0.43833021580133885\n",
            "20     \t [ 1.39513001  1.37149056  5.          0.56200696 12.          0.29219492]. \t  -0.6531158385825743 \t -0.43833021580133885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.532516410062584"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFKuwvS3uzrs",
        "outputId": "20199abc-6b41-4918-9c2b-6d68815b1616"
      },
      "source": [
        "end_exact = time.time()\n",
        "end_exact\n",
        "\n",
        "time_exact = end_exact - start_exact\n",
        "time_exact"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1684.176251411438"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2FlhY4vHUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d42a8a-e50f-4974-f445-3a444fdc8806"
      },
      "source": [
        "rmse_zero = [rmse_zero_1,\n",
        "rmse_zero_2,\n",
        "rmse_zero_3,\n",
        "rmse_zero_4,\n",
        "rmse_zero_5,\n",
        "rmse_zero_6,\n",
        "rmse_zero_7,\n",
        "rmse_zero_8,\n",
        "rmse_zero_9,\n",
        "rmse_zero_10,\n",
        "rmse_zero_11,\n",
        "rmse_zero_12,\n",
        "rmse_zero_13,\n",
        "rmse_zero_14,\n",
        "rmse_zero_15,\n",
        "rmse_zero_16,\n",
        "rmse_zero_17,\n",
        "rmse_zero_18,\n",
        "rmse_zero_19,\n",
        "rmse_zero_20]\n",
        "\n",
        "np.mean(rmse_zero)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.697604334700215"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ53FsWXu3J1",
        "outputId": "4be60074-2a8c-4d86-bb57-20bca6d422c3"
      },
      "source": [
        "rmse_exact = [rmse_exact_1,\n",
        "rmse_exact_2,\n",
        "rmse_exact_3,\n",
        "rmse_exact_4,\n",
        "rmse_exact_5,\n",
        "rmse_exact_6,\n",
        "rmse_exact_7,\n",
        "rmse_exact_8,\n",
        "rmse_exact_9,\n",
        "rmse_exact_10,\n",
        "rmse_exact_11,\n",
        "rmse_exact_12,\n",
        "rmse_exact_13,\n",
        "rmse_exact_14,\n",
        "rmse_exact_15,\n",
        "rmse_exact_16,\n",
        "rmse_exact_17,\n",
        "rmse_exact_18,\n",
        "rmse_exact_19,\n",
        "rmse_exact_20]\n",
        "\n",
        "np.mean(rmse_exact)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.622184558899018"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9FOyoH8u5Wx",
        "outputId": "2a2c5e55-8111-40e9-bad8-277ed0a12380"
      },
      "source": [
        "min_rmse_zero = min_max_array(rmse_zero)\n",
        "min_rmse_zero, len(min_rmse_zero)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.667222167746296,\n",
              "  4.600295852493361,\n",
              "  4.600295852493361,\n",
              "  4.600295852493361,\n",
              "  4.600295852493361,\n",
              "  4.600295852493361,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.491951392659048],\n",
              " 20)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXOpKHcvO15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c6b3f9-291f-4944-b61e-b340044a06bb"
      },
      "source": [
        "min_rmse_exact = min_max_array(rmse_exact)\n",
        "min_rmse_exact, len(min_rmse_exact)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.607787340872903,\n",
              "  4.556046943199982,\n",
              "  4.556046943199982,\n",
              "  4.556046943199982,\n",
              "  4.556046943199982,\n",
              "  4.556046943199982,\n",
              "  4.499924380514474,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443],\n",
              " 20)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxo85-HEvRPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "53246bed-fbe9-4779-b839-24d28bba473d"
      },
      "source": [
        "### Visualise!\n",
        "\n",
        "title = obj_func\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(min_rmse_exact, color = 'Green', label='RMSE: GP EI ExactGrad', ls='--')# r'($\\nu$' ' = {})'.format(df))\n",
        "plt.plot(min_rmse_zero, color = 'Red', label='RMSE: GP EI ZeroGrad')\n",
        "\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\n",
        "plt.xlabel('Experiment(s)', weight = 'bold', family = 'Arial') # x-axis label\n",
        "plt.ylabel('RMSE (US Dollars $)', weight = 'bold', family = 'Arial') # y-axis label\n",
        "plt.legend(loc=0) # add plot legend\n",
        "\n",
        "### Make the x-ticks integers, not floats:\n",
        "count = len(min_rmse_zero)\n",
        "plt.xticks(np.arange(count), np.arange(1, count + 1))\n",
        "plt.grid(b=None)\n",
        "plt.show() #visualize!\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwyO7_iZvT7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f1ffad-2d7e-473d-9362-3fce93929938"
      },
      "source": [
        "time_zero, time_exact\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(374.0735580921173, 1684.176251411438)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHLA-0DnVXxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58900084-51a0-4b11-82e5-5c2c91dd31ea"
      },
      "source": [
        "min(min_rmse_exact), min(min_rmse_zero)\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.487971093090443, 4.491951392659048)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iUNBRy3W0GY"
      },
      "source": [
        ""
      ],
      "execution_count": 106,
      "outputs": []
    }
  ]
}