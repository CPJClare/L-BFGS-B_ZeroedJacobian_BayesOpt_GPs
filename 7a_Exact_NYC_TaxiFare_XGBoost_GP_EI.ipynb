{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7a. Exact_NYC_TaxiFare_XGBoost_GP_EI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wHutsqZUcn"
      },
      "source": [
        "XGBoost Regression - 'real-world' example: NYC Taxi-Fare Predictor\n",
        "\n",
        "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7PwmXsgZO8D",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5753f21d-5522-49a0-96bc-cb6cb6c87b52"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a84b53b8-4c45-4e85-858d-d3d00a1290fc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a84b53b8-4c45-4e85-858d-d3d00a1290fc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"conorc2006\",\"key\":\"c5c5a6382a7d50c022aab991694fc17f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwbJ6hjZltI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c01d093-ca15-43f3-ff75-31f7e5c60b21"
      },
      "source": [
        "## Ensure the kaggle.json file is present:\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Aug 19 08:58 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Pu-UlWZovH"
      },
      "source": [
        "## Next, install the Kaggle API client:\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOQ4SE7Zuj3"
      },
      "source": [
        "## The Kaggle API Client expects this file to be ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcEztjCZxOn"
      },
      "source": [
        "## Permissions' change\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-u4Tmj7ZUD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd21bfdb-8bc5-408f-8f03-0a2e80c050b4"
      },
      "source": [
        "!kaggle competitions download -c new-york-city-taxi-fare-prediction"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "sample_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "GCP-Coupons-Instructions.rtf: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-0Pe1i4Z2R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7e98d9-db9a-466e-c8ec-ed1d8f46b4ef"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyGPGO in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.11.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: Theano-PyMC in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (2019.0)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl->pyGPGO) (2021.3.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.13.0)\n",
            "Requirement already satisfied: arviz>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.11.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.3.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (3.7.4.3)\n",
            "Requirement already satisfied: cachetools>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.2.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from Theano-PyMC->pyGPGO) (3.0.12)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (3.2.2)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (0.18.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (21.0)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (1.5.7)\n",
            "Requirement already satisfied: setuptools>=38.4 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pyMC3->pyGPGO) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0->arviz>=0.11.0->pyMC3->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.7/dist-packages (from netcdf4->arviz>=0.11.0->pyMC3->pyGPGO) (1.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zDTf1naBsH"
      },
      "source": [
        "# Load some default Python modules:\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import time\n",
        "\n",
        "from matplotlib.pyplot import rc\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
        "rc('text', usetex=False)\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "from collections import OrderedDict\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.special import gamma\n",
        "from scipy.stats import norm, t\n",
        "from joblib import Parallel, delayed\n",
        "import itertools\n",
        "\n",
        "from pyGPGO.logger import EventLogger\n",
        "from pyGPGO.GPGO import GPGO\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
        "from pyGPGO.acquisition import Acquisition\n",
        "from pyGPGO.covfunc import squaredExponential\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from pandas_datareader import data\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXicekJhaE0P"
      },
      "source": [
        "# Read data in pandas dataframe:\n",
        "\n",
        "df_train =  pd.read_csv('/content/train.csv.zip', nrows = 1_000_000, parse_dates=[\"pickup_datetime\"])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0mDzt_cBmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4f73072b-dc96-4379-f022-4e9b9146838e"
      },
      "source": [
        "# List first rows:\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-06-15 17:26:21.0000001</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2009-06-15 17:26:21+00:00</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-05 16:52:16.0000002</td>\n",
              "      <td>16.9</td>\n",
              "      <td>2010-01-05 16:52:16+00:00</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-08-18 00:35:00.00000049</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2011-08-18 00:35:00+00:00</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-04-21 04:30:42.0000001</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2012-04-21 04:30:42+00:00</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-03-09 07:51:00.000000135</td>\n",
              "      <td>5.3</td>\n",
              "      <td>2010-03-09 07:51:00+00:00</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             key  ...  passenger_count\n",
              "0    2009-06-15 17:26:21.0000001  ...                1\n",
              "1    2010-01-05 16:52:16.0000002  ...                1\n",
              "2   2011-08-18 00:35:00.00000049  ...                2\n",
              "3    2012-04-21 04:30:42.0000001  ...                1\n",
              "4  2010-03-09 07:51:00.000000135  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9fZujMycFMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d99566-946d-47aa-d028-8a37f01c48a7"
      },
      "source": [
        "# Format 'pickup_datetime' variable:\n",
        "\n",
        "df_train['pickup_datetime'] =  pd.to_datetime(df_train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
        "df_train['pickup_datetime'].head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   2009-06-15 17:26:21+00:00\n",
              "1   2010-01-05 16:52:16+00:00\n",
              "2   2011-08-18 00:35:00+00:00\n",
              "3   2012-04-21 04:30:42+00:00\n",
              "4   2010-03-09 07:51:00+00:00\n",
              "Name: pickup_datetime, dtype: datetime64[ns, UTC]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nReKu62HcVFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1f05a411-703f-4202-f1b8-c7cb19764602"
      },
      "source": [
        "df_train.sort_values(by = 'pickup_datetime').tail() ### June 2015 the final month\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>286276</th>\n",
              "      <td>2015-06-30 23:38:21.0000003</td>\n",
              "      <td>26.5</td>\n",
              "      <td>2015-06-30 23:38:21+00:00</td>\n",
              "      <td>-74.008385</td>\n",
              "      <td>40.711571</td>\n",
              "      <td>-73.884071</td>\n",
              "      <td>40.737385</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955575</th>\n",
              "      <td>2015-06-30 23:45:57.0000003</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015-06-30 23:45:57+00:00</td>\n",
              "      <td>-74.002342</td>\n",
              "      <td>40.739819</td>\n",
              "      <td>-74.005829</td>\n",
              "      <td>40.745239</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915826</th>\n",
              "      <td>2015-06-30 23:48:35.0000005</td>\n",
              "      <td>30.5</td>\n",
              "      <td>2015-06-30 23:48:35+00:00</td>\n",
              "      <td>-73.983826</td>\n",
              "      <td>40.729546</td>\n",
              "      <td>-73.927917</td>\n",
              "      <td>40.661186</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751350</th>\n",
              "      <td>2015-06-30 23:53:23.0000002</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2015-06-30 23:53:23+00:00</td>\n",
              "      <td>-73.978020</td>\n",
              "      <td>40.757439</td>\n",
              "      <td>-73.980705</td>\n",
              "      <td>40.753544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785182</th>\n",
              "      <td>2015-06-30 23:53:49.0000003</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2015-06-30 23:53:49+00:00</td>\n",
              "      <td>-73.959969</td>\n",
              "      <td>40.762405</td>\n",
              "      <td>-73.953064</td>\n",
              "      <td>40.782688</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                key  ...  passenger_count\n",
              "286276  2015-06-30 23:38:21.0000003  ...                5\n",
              "955575  2015-06-30 23:45:57.0000003  ...                1\n",
              "915826  2015-06-30 23:48:35.0000005  ...                2\n",
              "751350  2015-06-30 23:53:23.0000002  ...                1\n",
              "785182  2015-06-30 23:53:49.0000003  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9j9LnIfcXcX"
      },
      "source": [
        "# Add time variables:\n",
        "\n",
        "df_train['hour'] = df_train['pickup_datetime'].dt.hour\n",
        "df_train['weekday'] = df_train['pickup_datetime'].dt.weekday\n",
        "df_train['month'] = df_train['pickup_datetime'].dt.month\n",
        "df_train['year'] = df_train['pickup_datetime'].dt.year\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVyFZIVIcaj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5e866d22-d180-4f66-9bd1-0196b101f7ec"
      },
      "source": [
        "df_train = df_train.drop(['pickup_datetime','key'], axis = 1)\n",
        "df_train.head()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.5</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.9</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.7</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.7</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.3</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "0          4.5        -73.844311        40.721319  ...        0      6  2009\n",
              "1         16.9        -74.016048        40.711303  ...        1      1  2010\n",
              "2          5.7        -73.982738        40.761270  ...        3      8  2011\n",
              "3          7.7        -73.987130        40.733143  ...        5      4  2012\n",
              "4          5.3        -73.968095        40.768008  ...        1      3  2010\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfm-KSqcdVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67cc4b0-2a40-49d8-d67e-43d30f5e601c"
      },
      "source": [
        "# Remove negative fares and postive outliers:\n",
        "\n",
        "df_train = df_train[df_train.fare_amount>=0]\n",
        "df_train = df_train[df_train.fare_amount<=60]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTVDAD2KchTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cedb4dd8-472d-44d2-966d-1014ab4542c2"
      },
      "source": [
        "# Remove missing data:\n",
        "\n",
        "df_train = df_train.dropna(how = 'any', axis = 'rows')\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYksJ2cclVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeebf82d-9fad-4e51-f8e2-7d70143244d0"
      },
      "source": [
        "# June 2015 NYC taxi data (Wu et al, 2017):\n",
        "\n",
        "df_train = df_train[df_train.month==6]\n",
        "df_train = df_train[df_train.year==2015]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 11269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgSHPyYcnuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "f919cdc9-9c75-4893-83a6-83c268f74bae"
      },
      "source": [
        "# Histogram fare plot:\n",
        "\n",
        "df_train[df_train.fare_amount<15].fare_amount.hist(bins=100, figsize=(16,5), color = \"red\")\n",
        "plt.xlabel('$ US Dollars', weight = 'bold', family = 'Arial')\n",
        "plt.title('June 2015 Fares', weight = 'bold', family = 'Arial')\n",
        "plt.grid(b=None)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMSdAAjcr4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a21d06-54ca-42c2-8488-24eb593fff4c"
      },
      "source": [
        "y = df_train.fare_amount.values + 1e-10\n",
        "y ### for supervised learning: output vector y"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.54,  8.  , 34.  , ...,  4.5 ,  6.5 ,  7.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOeHvi3cu1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "06a226e5-d875-49f2-b3f6-eb3d18bbbef1"
      },
      "source": [
        "# List first rows (post-cleaning):\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>22.54</td>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>34.00</td>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>11.50</td>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "31         22.54        -74.010483        40.717667  ...        6      6  2015\n",
              "310         8.00        -74.010727        40.710091  ...        5      6  2015\n",
              "314        34.00        -73.974899        40.751095  ...        1      6  2015\n",
              "321         8.00        -73.961784        40.759579  ...        0      6  2015\n",
              "486        11.50        -73.957443        40.761703  ...        0      6  2015\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lT9BBicw4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8f511bf6-119a-4eef-9a30-beb63e873a53"
      },
      "source": [
        "X = df_train.drop(['fare_amount', 'month', 'year'], axis = 1)\n",
        "X.head() ### for supervised learning: input matrix X"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pickup_longitude  pickup_latitude  ...  hour  weekday\n",
              "31         -74.010483        40.717667  ...    21        6\n",
              "310        -74.010727        40.710091  ...     9        5\n",
              "314        -73.974899        40.751095  ...    23        1\n",
              "321        -73.961784        40.759579  ...    21        0\n",
              "486        -73.957443        40.761703  ...    19        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eC8SDPzczNY"
      },
      "source": [
        "### Optimum rmse: regression model objective function is Root Mean Square Error (RMSE); \n",
        "### Should be minimized (as close to zero as possible):\n",
        "\n",
        "y_global_orig = 0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTmWEhSc1qQ"
      },
      "source": [
        "### Bayesian Optimization - inputs:\n",
        "\n",
        "obj_func = 'XGBoost'\n",
        "n_start_AcqFunc = 100\n",
        "n_test = 500 # test points\n",
        "df = 3 # nu\n",
        "\n",
        "util_approx = 'ExpectedImprovement'\n",
        "util_exact = 'dEI_GP'\n",
        "n_init = 5 # random initialisations\n",
        "opt = True\n",
        "\n",
        "test_perc = 0.667\n",
        "train_perc = 1 - test_perc\n",
        "\n",
        "n_test = int(len(df_train) * test_perc)\n",
        "n_train = int(len(df_train) - n_test)\n",
        "\n",
        "eps = 1e-08"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ngnRxbc7cg"
      },
      "source": [
        "### Objective function:\n",
        "\n",
        "if obj_func == 'XGBoost': # 6-D\n",
        "            \n",
        "    # Constraints:\n",
        "    param_lb_alpha = 0\n",
        "    param_ub_alpha = 10\n",
        "    \n",
        "    param_lb_gamma = 0\n",
        "    param_ub_gamma = 10\n",
        "    \n",
        "    param_lb_max_depth = 5\n",
        "    param_ub_max_depth = 15\n",
        "    \n",
        "    param_lb_min_child_weight = 1\n",
        "    param_ub_min_child_weight = 20\n",
        "    \n",
        "    param_lb_subsample = .5\n",
        "    param_ub_subsample = 1\n",
        "    \n",
        "    param_lb_colsample = .1\n",
        "    param_ub_colsample = 1\n",
        "    \n",
        "    # 6-D inputs' parameter bounds:\n",
        "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
        "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
        "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
        "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
        "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
        "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
        "        }\n",
        "       \n",
        "    # True y bounds:\n",
        "    dim = 6\n",
        "    \n",
        "    max_iter = 20  # iterations of Bayesian optimization\n",
        "    \n",
        "    operator = 1 \n",
        "    \n",
        "    n_est = 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJsNX29c_xA"
      },
      "source": [
        "### Surrogate derivatives: \n",
        "\n",
        "cov_func = squaredExponential()\n",
        "\n",
        "def kronDelta(X, Xstar):\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\n",
        "\n",
        "def se(X, Xstar, sigmaf, l, sigman):\n",
        "    return sigmaf * np.exp(-0.5 * cdist(X, Xstar) ** 2 / l ** 2) + sigman * kronDelta(X, Xstar)\n",
        "\n",
        "def deriv_se(X, Xstar, sigmaf, l, sigman):\n",
        "    return cdist(X, Xstar) / (l ** 2) * se(X, Xstar, sigmaf, l, sigman)\n",
        "\n",
        "def der_covmat(X, Xstar, sigmaf, l, sigman):\n",
        "      nx = len(X)\n",
        "      ny = len(Xstar)\n",
        "      return np.round(np.array([deriv_se(np.atleast_2d(i), np.atleast_2d(j), sigmaf, l, sigman) for (i, j) in itertools.product(X, Xstar)]).reshape(nx, ny), 8)\n",
        "\n",
        "class dGaussianProcess(GaussianProcess):\n",
        "    l = GaussianProcess(cov_func, optimize=opt).getcovparams()['l']\n",
        "    sigmaf = GaussianProcess(cov_func, optimize=opt).getcovparams()['sigmaf']\n",
        "    sigman = GaussianProcess(cov_func, optimize=opt).getcovparams()['sigman']\n",
        "\n",
        "    def AcqGrad(self, Xstar):\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = self.covfunc.K(self.X, Xstar).T\n",
        "        dKstar = der_covmat(self.X, Xstar, self.sigmaf, self.l, self.sigman).T\n",
        "        \n",
        "        alpha_Kstar = np.dot(np.linalg.inv(self.K + (self.sigman**2) * np.eye(len(self.X))), Kstar.T)\n",
        "        \n",
        "        dm = np.dot(-dKstar, self.alpha)\n",
        "        ds = -2 * np.dot(-dKstar, alpha_Kstar)\n",
        "        \n",
        "        return dm, ds\n",
        "        "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ZuEB2VdE0W"
      },
      "source": [
        "### Set-seeds:\n",
        "\n",
        "run_num_1 = 0\n",
        "run_num_2 = 2\n",
        "run_num_3 = 3\n",
        "run_num_4 = 4\n",
        "run_num_5 = 5\n",
        "run_num_6 = 6\n",
        "run_num_7 = 7\n",
        "run_num_8 = 8\n",
        "run_num_9 = 9\n",
        "run_num_10 = 10\n",
        "run_num_11 = 11\n",
        "run_num_12 = 12\n",
        "run_num_13 = 13\n",
        "run_num_14 = 14\n",
        "run_num_15 = 15\n",
        "run_num_16 = 16\n",
        "run_num_17 = 17\n",
        "run_num_18 = 18\n",
        "run_num_19 = 19\n",
        "run_num_20 = 20\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHMFEyPdCk4"
      },
      "source": [
        "### Cumulative Regret Calculator:\n",
        "\n",
        "def min_max_array(x):\n",
        "    new_list = []\n",
        "    for i, num in enumerate(x):\n",
        "            new_list.append(np.min(x[0:i+1]))\n",
        "    return new_list\n",
        "    "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJMhL70fdHz_"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \n",
        "    def __init__(self, mode, eps=eps, **params):\n",
        "        \n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "\n",
        "        mode_dict = {\n",
        "            'dEI_GP': self.dEI_GP\n",
        "        }\n",
        "\n",
        "        self.f = mode_dict[mode]\n",
        "    \n",
        "    def dEI_GP(self, tau, mean, std, ds, dm):\n",
        "        gamma = (mean - tau - self.eps) / (std + self.eps)\n",
        "        gamma_h = (mean - tau) / (std + self.eps)\n",
        "        dsdx = ds / (2 * (std + self.eps))\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (gamma * norm.cdf(gamma) + norm.pdf(gamma))\n",
        "        df1 = f / (std + self.eps) * dsdx \n",
        "        df2 = (std + self.eps) * norm.cdf(gamma) * dmdx\n",
        "        df = (df1 + df2)[0]\n",
        "        df_arr = []\n",
        "\n",
        "        for j in range(0, dim):\n",
        "          df_arr.append(df)\n",
        "        return f, np.asarray(df_arr).transpose()\n",
        "        \n",
        "    def d_eval(self, tau, mean, std, ds, dm):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, **self.params)\n",
        "        "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S422jNLsdIMm"
      },
      "source": [
        "## dGPGO:\n",
        "\n",
        "grad = 1\n",
        "\n",
        "class dGPGO(GPGO):\n",
        "    n_start = n_start_AcqFunc\n",
        "\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start_AcqFunc):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.acqfunc,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "        self.start_points_arr = start_points_arr\n",
        "\n",
        "        return x_best, f_best\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)\n",
        "\n",
        "    def acqfunc(self, xnew, n_start=n_start_AcqFunc):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + eps)\n",
        "        dm, ds = self.GP.AcqGrad(xnew)\n",
        "        f, df = self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm)\n",
        "\n",
        "        return -f, -df * grad\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlilveEgdIR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629bfe4e-bf71-4031-f29b-b26edae03c69"
      },
      "source": [
        "start_approx = time.time()\n",
        "start_approx"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1629367633.6552827"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlzDSHbUG-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8d94c6-62c8-451a-81d4-3b198bed0aed"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 1\n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_approx_1 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_1 = GPGO(surrogate_approx_1, Acquisition(util_approx), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
        "approx_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_1 = approx_1.getResult()[0]\n",
        "params_approx_1['max_depth'] = int(params_approx_1['max_depth'])\n",
        "params_approx_1['min_child_weight'] = int(params_approx_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_approx_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_approx_1 = xgb.train(params_approx_1, dX_approx_train1)\n",
        "pred_approx_1 = model_approx_1.predict(dX_approx_test1)\n",
        "\n",
        "rmse_approx_1 = np.sqrt(mean_squared_error(pred_approx_1, y_test1))\n",
        "rmse_approx_1"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [5.48813504 7.15189366 8.         0.92897281 8.         0.48128932]. \t  -0.5626915313589341 \t -0.46143572360276275\n",
            "init   \t [ 6.45894113  4.37587211 11.          0.52835649 13.          0.44509737]. \t  -0.5772881365468763 \t -0.46143572360276275\n",
            "init   \t [ 7.91725038  5.2889492  13.          0.6963924  14.          0.40365654]. \t  -0.5870544636272766 \t -0.46143572360276275\n",
            "init   \t [ 6.48171872  3.6824154  10.          0.88907838 16.          0.88307853]. \t  -0.46143572360276275 \t -0.46143572360276275\n",
            "init   \t [4.73608045 8.00910752 8.         0.83943977 8.         0.67592892]. \t  -0.5051288806760134 \t -0.46143572360276275\n",
            "1      \t [ 0.96098408  9.76459465  7.          0.75481219 17.          0.64436097]. \t  -0.5149059967458438 \t -0.46143572360276275\n",
            "2      \t [ 5.13759733  2.22657933 12.          0.58106013  2.          0.92007745]. \t  -0.46868154430393166 \t -0.46143572360276275\n",
            "3      \t [9.58067178 9.65734278 7.         0.88193436 1.         0.38223155]. \t  -0.5888715285109799 \t -0.46143572360276275\n",
            "4      \t [ 0.90969339  9.80979401 14.          0.8665633   3.          0.55460209]. \t  -0.5632471108749298 \t -0.46143572360276275\n",
            "5      \t [0.3028841  4.069464   5.         0.51500749 1.         0.27359263]. \t  -0.6787735406244566 \t -0.46143572360276275\n",
            "6      \t [ 8.87166351  9.3367646   5.          0.58200219 19.          0.4055524 ]. \t  -0.6087566280502396 \t -0.46143572360276275\n",
            "7      \t [ 0.51228404  8.90605177 14.          0.7949699  11.          0.73913262]. \t  \u001b[92m-0.46104762612970374\u001b[0m \t -0.46104762612970374\n",
            "8      \t [ 2.05150398  0.53599727  5.          0.71551734 11.          0.37377971]. \t  -0.60261611749113 \t -0.46104762612970374\n",
            "9      \t [8.38797278 0.6003286  6.         0.6181346  1.         0.60370114]. \t  -0.526164863483156 \t -0.46104762612970374\n",
            "10     \t [ 9.06530919  9.40796401 14.          0.73452132  4.          0.22638202]. \t  -0.6737273666958219 \t -0.46104762612970374\n",
            "11     \t [ 2.92761431  0.02841708 13.          0.97950295  9.          0.31091424]. \t  -0.5822275926277118 \t -0.46104762612970374\n",
            "12     \t [9.14092793 0.64739809 8.         0.90967035 8.         0.70435457]. \t  -0.5029298264030027 \t -0.46104762612970374\n",
            "13     \t [ 1.99180311  1.76156949  6.          0.52474573 18.          0.69397695]. \t  -0.5292680491699555 \t -0.46104762612970374\n",
            "14     \t [ 1.47165443  0.32474578 13.          0.93757169 15.          0.40230457]. \t  -0.5795930586132758 \t -0.46104762612970374\n",
            "15     \t [ 3.33998723  9.61997442 13.          0.87678219 17.          0.47112812]. \t  -0.562089749118206 \t -0.46104762612970374\n",
            "16     \t [ 0.59268574  5.88934857  8.          0.90451848 12.          0.19426621]. \t  -0.6701635295211161 \t -0.46104762612970374\n",
            "17     \t [ 7.96942817  9.76181769  7.          0.91046857 13.          0.32455735]. \t  -0.5870001390492389 \t -0.46104762612970374\n",
            "18     \t [0.45095418 9.96977628 8.         0.5330212  1.         0.30363698]. \t  -0.5984194428816372 \t -0.46104762612970374\n",
            "19     \t [ 9.20351788  2.8632943   5.          0.5        20.          1.        ]. \t  -0.4769039550156672 \t -0.46104762612970374\n",
            "20     \t [3.69919707 0.85322265 6.         0.75904625 5.         0.97222422]. \t  -0.4807633478348844 \t -0.46104762612970374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.667222167746296"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJ9rN2KUJzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e024e4-1925-49c3-9e90-40038e7d985f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 2\n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_approx_2 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_2 = GPGO(surrogate_approx_2, Acquisition(util_approx), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
        "approx_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_2 = approx_2.getResult()[0]\n",
        "params_approx_2['max_depth'] = int(params_approx_2['max_depth'])\n",
        "params_approx_2['min_child_weight'] = int(params_approx_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_approx_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_approx_2 = xgb.train(params_approx_2, dX_approx_train2)\n",
        "pred_approx_2 = model_approx_2.predict(dX_approx_test2)\n",
        "\n",
        "rmse_approx_2 = np.sqrt(mean_squared_error(pred_approx_2, y_test2))\n",
        "rmse_approx_2"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 4.35994902  0.25926232 11.          0.97386531 12.          0.47833102]. \t  -0.5147867600449748 \t -0.4765694615523879\n",
            "init   \t [ 3.30334821  2.04648634 10.          0.55997527  6.          0.71472339]. \t  -0.4765694615523879 \t -0.4765694615523879\n",
            "init   \t [ 4.9856117   5.86796978  8.          0.89266757 11.          0.59158659]. \t  -0.4946399399702889 \t -0.4765694615523879\n",
            "init   \t [ 4.07307832  1.76984624 13.          0.75262305  7.          0.35908193]. \t  -0.5884370816585467 \t -0.4765694615523879\n",
            "init   \t [ 1.16193318  1.81727038  9.          0.79837265 19.          0.29965165]. \t  -0.584982798458911 \t -0.4765694615523879\n",
            "1      \t [9.68290573 5.74953535 8.         0.93445831 2.         0.81872709]. \t  -0.47692913742153503 \t -0.4765694615523879\n",
            "2      \t [ 8.78180153  6.61060882 12.          0.91523653 18.          0.29687212]. \t  -0.5840366351865409 \t -0.4765694615523879\n",
            "3      \t [ 0.66591974  9.26661294 14.          0.96342421 18.          0.94909068]. \t  \u001b[92m-0.3879430352217851\u001b[0m \t -0.3879430352217851\n",
            "4      \t [0.53023554 8.79041977 6.         0.85342606 1.         0.93968064]. \t  -0.41489686189456554 \t -0.3879430352217851\n",
            "5      \t [ 1.33044502  7.67244135 10.          0.96757444  7.          0.49535892]. \t  -0.5161313584690508 \t -0.3879430352217851\n",
            "6      \t [ 8.57602235  9.83360074 12.          0.99133635  7.          0.25584574]. \t  -0.6763928241669385 \t -0.3879430352217851\n",
            "7      \t [ 9.77744834  2.26597384  5.          0.98618685 19.          0.58642903]. \t  -0.520384169717879 \t -0.3879430352217851\n",
            "8      \t [ 4.9591915   9.55230935 12.          0.62018763  1.          0.81205874]. \t  -0.47644483906838386 \t -0.3879430352217851\n",
            "9      \t [ 5.76886466  6.30636441 11.          0.85075313  6.          0.94564556]. \t  -0.39766690308447894 \t -0.3879430352217851\n",
            "10     \t [ 0.92680624  0.80829442  5.          0.99163751 10.          0.48273491]. \t  -0.5317089151903325 \t -0.3879430352217851\n",
            "11     \t [ 8.66397635  9.94226364  5.          0.61754568 19.          0.46861962]. \t  -0.5385752501499373 \t -0.3879430352217851\n",
            "12     \t [ 2.21750241  8.1937508   6.          0.88756269 17.          0.73205426]. \t  -0.4886010805216155 \t -0.3879430352217851\n",
            "13     \t [ 9.78109337  2.52726344  5.          0.62902817 11.          0.70291169]. \t  -0.5193465670402935 \t -0.3879430352217851\n",
            "14     \t [ 2.61078484  8.48438058 14.          0.79784401 12.          0.37480602]. \t  -0.5873563000820609 \t -0.3879430352217851\n",
            "15     \t [0.12798644 2.8640124  5.         0.70953362 4.         0.75142572]. \t  -0.49559690638982534 \t -0.3879430352217851\n",
            "16     \t [ 9.16837918  2.32035478 14.          0.50291805  1.          0.25746282]. \t  -0.6778946275066893 \t -0.3879430352217851\n",
            "17     \t [ 9.26580723  2.30642223 15.          1.         13.58385464  0.21220249]. \t  -0.6371480649888038 \t -0.3879430352217851\n",
            "18     \t [6.0348008  1.05838904 5.         0.60659353 6.         0.66244736]. \t  -0.5184813195331227 \t -0.3879430352217851\n",
            "19     \t [10.          0.81114757 12.38919881  0.5        20.          0.1       ]. \t  -0.6752754033615737 \t -0.3879430352217851\n",
            "20     \t [ 0.40164348  1.66439141 14.          0.96476321  1.          0.14148109]. \t  -0.6802555077917066 \t -0.3879430352217851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.600295852493361"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-45l3NU4UNiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c884dc-a541-43cf-808d-22448ff09787"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 3\n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_approx_3 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_3 = GPGO(surrogate_approx_3, Acquisition(util_approx), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
        "approx_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_3 = approx_3.getResult()[0]\n",
        "params_approx_3['max_depth'] = int(params_approx_3['max_depth'])\n",
        "params_approx_3['min_child_weight'] = int(params_approx_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_approx_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_approx_3 = xgb.train(params_approx_3, dX_approx_train3)\n",
        "pred_approx_3 = model_approx_3.predict(dX_approx_test3)\n",
        "\n",
        "rmse_approx_3 = np.sqrt(mean_squared_error(pred_approx_3, y_test3))\n",
        "rmse_approx_3"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.50797903  7.08147823 13.          0.56066429 11.          0.11687321]. \t  -0.7165783188757435 \t -0.6409647951145182\n",
            "init   \t [ 0.40630737  2.47888297 11.          0.72040492 13.          0.23083313]. \t  -0.7204431346766296 \t -0.6409647951145182\n",
            "init   \t [ 4.53172301  2.15577008 11.          0.74631796  2.          0.60296868]. \t  -0.6409647951145182 \t -0.6409647951145182\n",
            "init   \t [ 2.59252447  4.15101197 13.          0.79330998  8.          0.24118096]. \t  -0.7214290072967551 \t -0.6409647951145182\n",
            "init   \t [ 5.44649018  7.80314765 10.          0.62879264 18.          0.44917413]. \t  -0.6558401549443297 \t -0.6409647951145182\n",
            "1      \t [1.56262424 9.7795241  5.         0.91450054 5.         0.53102391]. \t  -0.652766690473656 \t -0.6409647951145182\n",
            "2      \t [ 8.93142368  1.52910591 13.          0.84039318 17.          0.60846833]. \t  \u001b[92m-0.6320347814043803\u001b[0m \t -0.6320347814043803\n",
            "3      \t [ 6.38594331  1.19109066  5.          0.81189053 13.          0.59164768]. \t  \u001b[92m-0.6289274956252369\u001b[0m \t -0.6289274956252369\n",
            "4      \t [ 1.02918863  9.32189805 13.          0.88333707  1.          0.86998588]. \t  \u001b[92m-0.45190108244647254\u001b[0m \t -0.45190108244647254\n",
            "5      \t [ 9.74929058  1.51205926 11.          0.50025602  8.          0.46132437]. \t  -0.6593087000926661 \t -0.45190108244647254\n",
            "6      \t [ 9.45052852  8.62641484  7.          0.79615518 14.          0.32790361]. \t  -0.708730491334048 \t -0.45190108244647254\n",
            "7      \t [ 8.92744991  9.09956287 12.          0.74944313  3.          0.11030352]. \t  -0.7194264138705575 \t -0.45190108244647254\n",
            "8      \t [6.90239429 4.38257693 5.         0.77543741 6.         0.209803  ]. \t  -0.7198213013052538 \t -0.45190108244647254\n",
            "9      \t [ 9.02893081  7.64399312 14.          0.72894099 15.          0.70609928]. \t  -0.6325306398208942 \t -0.45190108244647254\n",
            "10     \t [9.43215663 9.14652183 5.         0.95117899 1.         0.516629  ]. \t  -0.657104878100571 \t -0.45190108244647254\n",
            "11     \t [ 2.84857043  0.57472701  5.          0.53705172 19.          0.51858228]. \t  -0.6589951838647294 \t -0.45190108244647254\n",
            "12     \t [ 0.63346059  9.87550877  6.          0.74382195 14.          0.39340576]. \t  -0.7092906536332471 \t -0.45190108244647254\n",
            "13     \t [ 0.63366318  7.00411349 14.          0.89817768 18.          0.82387154]. \t  -0.5088999933243639 \t -0.45190108244647254\n",
            "14     \t [ 9.9224789   2.12085607  6.          0.82807413 19.          0.27539821]. \t  -0.7196366943027934 \t -0.45190108244647254\n",
            "15     \t [ 8.38669041  0.14009263  7.          0.51525209 10.          0.83685663]. \t  -0.5364793690384302 \t -0.45190108244647254\n",
            "16     \t [ 7.41867878  4.36581543 14.          0.99315379  6.          0.89117614]. \t  -0.4557702743530953 \t -0.45190108244647254\n",
            "17     \t [0.58114767 0.49394646 7.         0.87185676 5.         0.42563352]. \t  -0.7090305881085761 \t -0.45190108244647254\n",
            "18     \t [ 2.77081064  4.33888713  7.          0.86748866 10.          0.91605352]. \t  -0.4747430674172078 \t -0.45190108244647254\n",
            "19     \t [ 0.2848593   7.24262219  6.          0.70597553 19.          0.97089957]. \t  -0.4870655187666578 \t -0.45190108244647254\n",
            "20     \t [4.27034104 6.89504758 8.         0.52242333 1.         0.93013039]. \t  -0.4688807931917104 \t -0.45190108244647254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.649527586702978"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voPfk1UDUQU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd32db2-a315-4bed-e41f-7821b1199c3b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 4\n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_approx_4 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_4 = GPGO(surrogate_approx_4, Acquisition(util_approx), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
        "approx_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_4 = approx_4.getResult()[0]\n",
        "params_approx_4['max_depth'] = int(params_approx_4['max_depth'])\n",
        "params_approx_4['min_child_weight'] = int(params_approx_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_approx_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_approx_4 = xgb.train(params_approx_4, dX_approx_train4)\n",
        "pred_approx_4 = model_approx_4.predict(dX_approx_test4)\n",
        "\n",
        "rmse_approx_4 = np.sqrt(mean_squared_error(pred_approx_4, y_test4))\n",
        "rmse_approx_4"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.67029839 5.47232249 6.         0.92781047 9.         0.72795594]. \t  -0.5993772224326677 \t -0.4983304913999733\n",
            "init   \t [ 2.16089496  9.76274455 12.          0.62649118  9.          0.66966679]. \t  -0.6069567430422909 \t -0.4983304913999733\n",
            "init   \t [ 0.05159149  5.72356491  9.          0.99170034 10.          0.10808749]. \t  -0.7139334307278753 \t -0.4983304913999733\n",
            "init   \t [ 3.86571283  0.44160058 10.          0.90553105 18.          0.95407958]. \t  -0.4983304913999733 \t -0.4983304913999733\n",
            "init   \t [ 7.86305986  8.66289299  6.          0.53285477 14.          0.25117497]. \t  -0.7091576633146701 \t -0.4983304913999733\n",
            "1      \t [ 8.45443649  8.61014312 11.          0.83475494  1.          0.14018305]. \t  -0.7189305559932541 \t -0.4983304913999733\n",
            "2      \t [ 0.77431146  1.96668116 12.          0.50723361  3.          0.74768925]. \t  -0.6140173026050956 \t -0.4983304913999733\n",
            "3      \t [2.27858743 6.23199766 5.         0.58705984 2.         0.80794289]. \t  -0.5986031600147258 \t -0.4983304913999733\n",
            "4      \t [ 6.832625    9.87635293 14.          0.84450885 19.          0.20389666]. \t  -0.7153744507380464 \t -0.4983304913999733\n",
            "5      \t [ 7.37255369  2.03491596 13.          0.8921741   9.          0.46934318]. \t  -0.625550615237801 \t -0.4983304913999733\n",
            "6      \t [ 0.05992751  6.06320143 14.          0.89322475 17.          0.32211096]. \t  -0.6540035079472759 \t -0.4983304913999733\n",
            "7      \t [ 0.79250634  6.36332745  6.          0.84703891 17.          0.8628914 ]. \t  -0.5176542387032834 \t -0.4983304913999733\n",
            "8      \t [9.26767626 0.09691703 5.         0.59554562 3.         0.95249041]. \t  -0.524958204283032 \t -0.4983304913999733\n",
            "9      \t [ 9.93824172  2.34876498  7.          0.94210707 16.          0.6947317 ]. \t  -0.6069344861897712 \t -0.4983304913999733\n",
            "10     \t [3.43076773 0.51115291 6.         0.80398076 7.         0.163561  ]. \t  -0.7152347423610873 \t -0.4983304913999733\n",
            "11     \t [8.12039932 9.82838311 5.         0.6851891  3.         0.87462757]. \t  -0.5209212303565759 \t -0.4983304913999733\n",
            "12     \t [ 6.03647398  5.02077859  7.          0.51964174 12.          0.90396407]. \t  -0.5142832795776289 \t -0.4983304913999733\n",
            "13     \t [ 9.03174101  2.34471053 13.          0.52042845  3.          0.20136175]. \t  -0.7105427027924398 \t -0.4983304913999733\n",
            "14     \t [ 9.29091353  0.24848851 14.          0.65742606 17.          0.33836791]. \t  -0.655558370799832 \t -0.4983304913999733\n",
            "15     \t [3.19488299 9.73527155 6.         0.65266597 9.         0.60657999]. \t  -0.6104166590688074 \t -0.4983304913999733\n",
            "16     \t [ 2.02212851  9.9407933  10.          0.71009222  3.          0.59925198]. \t  -0.6118231860074616 \t -0.4983304913999733\n",
            "17     \t [ 7.85427932  8.81949282 14.          0.936894    6.          0.6768425 ]. \t  -0.6008015870134431 \t -0.4983304913999733\n",
            "18     \t [ 6.65760625  2.35253217 11.          0.58483338 14.          0.21088543]. \t  -0.7100640025661658 \t -0.4983304913999733\n",
            "19     \t [ 0.9141999   0.89707594 12.          0.91410151 11.          0.69348379]. \t  -0.6076939108347638 \t -0.4983304913999733\n",
            "20     \t [ 6.39825002  5.88858918  9.          0.67432081 18.          0.813834  ]. \t  -0.5921986299757287 \t -0.4983304913999733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.04081354533672"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kEnTd7MUdlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a45bc9-6bcd-46dc-d5ed-ad3750bb97b8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 5\n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_approx_5 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_5 = GPGO(surrogate_approx_5, Acquisition(util_approx), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
        "approx_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_5 = approx_5.getResult()[0]\n",
        "params_approx_5['max_depth'] = int(params_approx_5['max_depth'])\n",
        "params_approx_5['min_child_weight'] = int(params_approx_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_approx_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_approx_5 = xgb.train(params_approx_5, dX_approx_train5)\n",
        "pred_approx_5 = model_approx_5.predict(dX_approx_test5)\n",
        "\n",
        "rmse_approx_5 = np.sqrt(mean_squared_error(pred_approx_5, y_test5))\n",
        "rmse_approx_5"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.21993171  8.70732306 11.          0.68186845 10.          0.53957007]. \t  -0.5323233521622429 \t -0.48683475961332984\n",
            "init   \t [ 6.11743863  7.65907856  5.          0.64840025 16.          0.82745351]. \t  -0.48683475961332984 \t -0.48683475961332984\n",
            "init   \t [ 6.49458883  8.19472793  6.          0.93996852 19.          0.36647194]. \t  -0.586475577548452 \t -0.48683475961332984\n",
            "init   \t [ 6.28787909  5.7983781   6.          0.63290956 17.          0.18402673]. \t  -0.633609225551848 \t -0.48683475961332984\n",
            "init   \t [8.26554249 8.33492742 9.         0.97900675 3.         0.26957319]. \t  -0.6326249738244012 \t -0.48683475961332984\n",
            "1      \t [1.95474956 1.21548467 5.         0.65548996 6.         0.3261206 ]. \t  -0.5882279171378358 \t -0.48683475961332984\n",
            "2      \t [ 0.09956678  5.3014067  14.          0.6298658  17.          0.94976952]. \t  \u001b[92m-0.43497892773908886\u001b[0m \t -0.43497892773908886\n",
            "3      \t [ 8.96005069  0.40158558 13.          0.99993231  1.          0.57714848]. \t  -0.49454570308727447 \t -0.43497892773908886\n",
            "4      \t [ 9.87326458  3.08165071 13.          0.89537491 17.          0.48678657]. \t  -0.5314874018711363 \t -0.43497892773908886\n",
            "5      \t [ 0.42801231  0.53056997 14.          0.88001393  3.          0.60870677]. \t  -0.4956565139336144 \t -0.43497892773908886\n",
            "6      \t [ 9.41789049  5.24678791 11.          0.78426601 10.          0.31189288]. \t  -0.5666618582956817 \t -0.43497892773908886\n",
            "7      \t [ 0.15773168  2.32643207  7.          0.74406654 13.          0.24500907]. \t  -0.6300236282521434 \t -0.43497892773908886\n",
            "8      \t [ 0.5259471   9.7567347  14.          0.91097263  1.          0.64281486]. \t  -0.49657614671134453 \t -0.43497892773908886\n",
            "9      \t [ 7.68019847  0.14272251  5.          0.84271757 13.          0.46064437]. \t  -0.5609457236924665 \t -0.43497892773908886\n",
            "10     \t [1.24717977 9.40645683 5.         0.65383928 3.         0.68852908]. \t  -0.5172211356155465 \t -0.43497892773908886\n",
            "11     \t [8.5765283  1.77730566 7.         0.63994635 4.         0.42153661]. \t  -0.5747310336255558 \t -0.43497892773908886\n",
            "12     \t [7.9980796  9.13466128 6.         0.81085598 9.         0.18640376]. \t  -0.6345727275625916 \t -0.43497892773908886\n",
            "13     \t [ 5.39228464  0.17784539 13.          0.57001227  9.          0.60313835]. \t  -0.5028236705242316 \t -0.43497892773908886\n",
            "14     \t [ 3.9042494   1.34731655 10.          0.83969498 18.          0.52845925]. \t  -0.5302205217955697 \t -0.43497892773908886\n",
            "15     \t [2.03762865 4.34961943 8.         0.53797581 1.         0.36410352]. \t  -0.5730025877297991 \t -0.43497892773908886\n",
            "16     \t [ 7.87772257  8.71340044 12.          0.8589136  15.          0.7305315 ]. \t  -0.4642522085611378 \t -0.43497892773908886\n",
            "17     \t [ 6.80615583  6.45556443 14.          0.61036314  1.          0.8555849 ]. \t  -0.4679937193324773 \t -0.43497892773908886\n",
            "18     \t [ 0.39655743  9.88537948  7.          0.8395494  15.          0.83639512]. \t  -0.46556996770739073 \t -0.43497892773908886\n",
            "19     \t [0.86243549 8.92584837 5.         0.6740487  9.         0.55305648]. \t  -0.5632466130939408 \t -0.43497892773908886\n",
            "20     \t [ 0.08863462  6.57371192 11.          0.96556049  5.          0.14575448]. \t  -0.6366949409441233 \t -0.43497892773908886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.866353439583042"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVSH6caUgyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af39ce05-9b98-4e71-981b-cc0f68175be2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 6\n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_approx_6 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_6 = GPGO(surrogate_approx_6, Acquisition(util_approx), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
        "approx_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_6 = approx_6.getResult()[0]\n",
        "params_approx_6['max_depth'] = int(params_approx_6['max_depth'])\n",
        "params_approx_6['min_child_weight'] = int(params_approx_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_approx_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_approx_6 = xgb.train(params_approx_6, dX_approx_train6)\n",
        "pred_approx_6 = model_approx_6.predict(dX_approx_test6)\n",
        "\n",
        "rmse_approx_6 = np.sqrt(mean_squared_error(pred_approx_6, y_test6))\n",
        "rmse_approx_6"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5719256944003751 \t -0.5405445954433028\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.6047098118480896 \t -0.5405445954433028\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5405445954433028 \t -0.5405445954433028\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5678490550489279 \t -0.5405445954433028\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6145131146454834 \t -0.5405445954433028\n",
            "1      \t [ 2.61343239  0.80193947  5.          0.83898129 13.          0.84644718]. \t  -0.5477332582915869 \t -0.5405445954433028\n",
            "2      \t [ 9.72322443  9.21177696  5.          0.87917074 11.          0.86681736]. \t  \u001b[92m-0.4929247454390112\u001b[0m \t -0.4929247454390112\n",
            "3      \t [ 6.6253488   3.09318222  6.          0.95687426 16.          0.13946484]. \t  -0.690368704953711 \t -0.4929247454390112\n",
            "4      \t [ 8.54451719  6.55901746 13.          0.7507391   5.          0.45426156]. \t  -0.6083435565147685 \t -0.4929247454390112\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6192420483468785 \t -0.4929247454390112\n",
            "6      \t [ 1.15162056  8.90964142  5.          0.51235759 16.          0.30257234]. \t  -0.6348710910727494 \t -0.4929247454390112\n",
            "7      \t [ 0.85068891  9.82901317 11.          0.8341869   1.          0.33880369]. \t  -0.6276360919465666 \t -0.4929247454390112\n",
            "8      \t [ 9.32420466  6.39616005 13.          0.93300527 17.          0.34904443]. \t  -0.618513635024325 \t -0.4929247454390112\n",
            "9      \t [ 2.5352414   9.31776001 11.          0.69913965  8.          0.81482819]. \t  -0.5386758711280498 \t -0.4929247454390112\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.6983911087025447 \t -0.4929247454390112\n",
            "11     \t [ 7.01659189  0.12368581 13.          0.73757646 10.          0.2486598 ]. \t  -0.6917483076985717 \t -0.4929247454390112\n",
            "12     \t [ 1.90877122  0.88669904 11.          0.72802974 19.          0.27519153]. \t  -0.6920469274142503 \t -0.4929247454390112\n",
            "13     \t [8.90989579 3.88913177 8.         0.86631672 9.         0.59686825]. \t  -0.5703702118029026 \t -0.4929247454390112\n",
            "14     \t [ 6.8359928   9.89182285  7.          0.83897916 17.          0.34169562]. \t  -0.618337528663939 \t -0.4929247454390112\n",
            "15     \t [ 3.68500147  6.04160102  8.          0.55594848 12.          0.2508545 ]. \t  -0.6923666983699771 \t -0.4929247454390112\n",
            "16     \t [ 0.05509359  8.13873516 11.          0.83199834 17.          0.87257216]. \t  \u001b[92m-0.4656680239098458\u001b[0m \t -0.4656680239098458\n",
            "17     \t [1.20916044 1.67996637 8.         0.95434584 8.         0.65713107]. \t  -0.560720771561226 \t -0.4656680239098458\n",
            "18     \t [3.01249996 9.72062503 5.         0.82932198 9.         0.65554665]. \t  -0.572026808612368 \t -0.4656680239098458\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.537139253301325 \t -0.4656680239098458\n",
            "20     \t [ 1.59392983  0.4187331   5.          0.53325118 19.          0.2829738 ]. \t  -0.6911833881800746 \t -0.4656680239098458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.934873443584281"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WsphKSUj19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11dac4c-0633-4340-96ee-639007ea82ed"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 7\n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_approx_7 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_7 = GPGO(surrogate_approx_7, Acquisition(util_approx), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
        "approx_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_7 = approx_7.getResult()[0]\n",
        "params_approx_7['max_depth'] = int(params_approx_7['max_depth'])\n",
        "params_approx_7['min_child_weight'] = int(params_approx_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_approx_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_approx_7 = xgb.train(params_approx_7, dX_approx_train7)\n",
        "pred_approx_7 = model_approx_7.predict(dX_approx_test7)\n",
        "\n",
        "rmse_approx_7 = np.sqrt(mean_squared_error(pred_approx_7, y_test7))\n",
        "rmse_approx_7"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.76308289 7.79918792 8.         0.98911145 8.         0.98019056]. \t  -0.44500885348659536 \t -0.44173641078261416\n",
            "init   \t [ 5.3849587   5.01120464 13.          0.74994125  5.          0.88192131]. \t  -0.4488374676936292 \t -0.44173641078261416\n",
            "init   \t [ 3.30839249  3.9294231  12.          0.6440728  13.          0.41137564]. \t  -0.5669653799025498 \t -0.44173641078261416\n",
            "init   \t [9.29528191 2.6258377  5.         0.80027446 1.         0.86616513]. \t  -0.4661284696195417 \t -0.44173641078261416\n",
            "init   \t [ 1.74052764  7.90763512 14.          0.7244129   4.          0.77536887]. \t  -0.44173641078261416 \t -0.44173641078261416\n",
            "1      \t [3.43305102 3.00339076 8.         0.71322679 4.         0.33322219]. \t  -0.5720117286339644 \t -0.44173641078261416\n",
            "2      \t [ 7.6343627   1.31181598  5.          0.5769645  12.          0.84874959]. \t  -0.48924755963876193 \t -0.44173641078261416\n",
            "3      \t [ 9.12127254  9.64651695 14.          0.53624962  1.          0.38247449]. \t  -0.5784219709012162 \t -0.44173641078261416\n",
            "4      \t [ 4.51243396  9.79601217  8.          0.69773915 19.          0.69687222]. \t  -0.48670271968935647 \t -0.44173641078261416\n",
            "5      \t [ 8.06748781  9.6311716   5.          0.89165168 11.          0.90769253]. \t  -0.47467849504450826 \t -0.44173641078261416\n",
            "6      \t [ 9.84853722  9.76587477 12.          0.66808012 15.          0.83895675]. \t  -0.464300180546447 \t -0.44173641078261416\n",
            "7      \t [6.4915356  8.69600226 5.         0.66481282 2.         0.54976375]. \t  -0.5200045453863744 \t -0.44173641078261416\n",
            "8      \t [ 0.86712134  2.53401598  5.          0.75823741 18.          0.7884932 ]. \t  -0.4803186810834383 \t -0.44173641078261416\n",
            "9      \t [ 1.29932493  8.0055142  14.          0.51325501 19.          0.83654101]. \t  -0.4650670910068424 \t -0.44173641078261416\n",
            "10     \t [ 8.667653    2.8973594  14.          0.53825633 18.          0.10643456]. \t  -0.6861760180900373 \t -0.44173641078261416\n",
            "11     \t [ 9.89504759  2.29066357 10.          0.72562459  8.          0.65324681]. \t  -0.4791774660247169 \t -0.44173641078261416\n",
            "12     \t [ 9.94824081  0.39149062 12.          0.59113076  1.          0.49709751]. \t  -0.5081268521688408 \t -0.44173641078261416\n",
            "13     \t [ 9.60964399  8.52338044  5.          0.92796141 17.          0.69277707]. \t  -0.49365990657681785 \t -0.44173641078261416\n",
            "14     \t [ 3.34596156  0.67230605 11.          0.7366642  19.          0.29949997]. \t  -0.5628067831028366 \t -0.44173641078261416\n",
            "15     \t [ 9.60816545  8.33912452 11.          0.58307044  8.          0.36972424]. \t  -0.5786370523858145 \t -0.44173641078261416\n",
            "16     \t [ 0.47345619  1.31956961  7.          0.68987164 11.          0.9514247 ]. \t  -0.466265051270997 \t -0.44173641078261416\n",
            "17     \t [ 8.96081021  0.38413512 14.          0.90624871 12.          0.41560009]. \t  -0.565007973216504 \t -0.44173641078261416\n",
            "18     \t [ 9.71411962  1.71380043  5.          0.50726337 19.          0.71982498]. \t  -0.4912391351239423 \t -0.44173641078261416\n",
            "19     \t [ 0.18764716  0.58029521 13.          0.75995865  6.          0.69134836]. \t  -0.46855437085127594 \t -0.44173641078261416\n",
            "20     \t [ 1.77048754  9.0838881   6.          0.98375355 14.          0.13025417]. \t  -0.6815881776266557 \t -0.44173641078261416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.499924380514474"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI8sFP4ZUmOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f9ccd1-5b5f-4146-9ee1-8b1cefea5e7d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 8\n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_approx_8 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_8 = GPGO(surrogate_approx_8, Acquisition(util_approx), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
        "approx_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_8 = approx_8.getResult()[0]\n",
        "params_approx_8['max_depth'] = int(params_approx_8['max_depth'])\n",
        "params_approx_8['min_child_weight'] = int(params_approx_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_approx_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_approx_8 = xgb.train(params_approx_8, dX_approx_train8)\n",
        "pred_approx_8 = model_approx_8.predict(dX_approx_test8)\n",
        "\n",
        "rmse_approx_8 = np.sqrt(mean_squared_error(pred_approx_8, y_test8))\n",
        "rmse_approx_8"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.5450023023990902 \t -0.47785117417083445\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.48415390639601685 \t -0.47785117417083445\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5514374023096014 \t -0.47785117417083445\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.509390123714371 \t -0.47785117417083445\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.47785117417083445 \t -0.47785117417083445\n",
            "1      \t [4.42022545 5.48487111 9.         0.97165909 3.         0.63617522]. \t  -0.4933129789895931 \t -0.47785117417083445\n",
            "2      \t [ 9.3432851   3.80536023 13.          0.82203895 19.          0.99569116]. \t  -0.4825289719254064 \t -0.47785117417083445\n",
            "3      \t [ 2.52429836  9.02824683 14.          0.59641093 17.          0.61934886]. \t  -0.5047477495526641 \t -0.47785117417083445\n",
            "4      \t [ 9.53473907  5.08424998 11.          0.50652828 18.          0.67121466]. \t  -0.5164144636241208 \t -0.47785117417083445\n",
            "5      \t [6.89072012 1.88822945 5.         0.9252956  8.         0.40577637]. \t  -0.5624845374478475 \t -0.47785117417083445\n",
            "6      \t [ 2.42575645  9.87357367  5.          0.61143882 19.          0.11833201]. \t  -0.6358778400175291 \t -0.47785117417083445\n",
            "7      \t [ 0.9931658   1.40870497 14.          0.50614818  6.          0.71944448]. \t  -0.48485914970894306 \t -0.47785117417083445\n",
            "8      \t [ 9.09148899  1.42093493  5.          0.88801001 17.          0.53231573]. \t  -0.5546570042431702 \t -0.47785117417083445\n",
            "9      \t [9.69554908 3.70013633 5.         0.72727157 1.         0.49530336]. \t  -0.5595421515943506 \t -0.47785117417083445\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.558869771245836 \t -0.47785117417083445\n",
            "11     \t [1.66316873 0.47469495 7.         0.60820298 1.         0.50689458]. \t  -0.5436576355709689 \t -0.47785117417083445\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.5062409032591381 \t -0.47785117417083445\n",
            "13     \t [5.63420638 9.7026496  5.         0.64869539 8.         0.22475566]. \t  -0.6389332388103492 \t -0.47785117417083445\n",
            "14     \t [ 0.63194856  9.08760061 12.          0.51277257  1.          0.85579144]. \t  -0.4885511515989219 \t -0.47785117417083445\n",
            "15     \t [ 8.35277365  5.17929354 14.          0.67305838  1.          0.70101595]. \t  -0.5154357312421961 \t -0.47785117417083445\n",
            "16     \t [ 8.62413803  1.63568526 10.          0.78670071 12.          0.5099252 ]. \t  -0.5367709050823309 \t -0.47785117417083445\n",
            "17     \t [ 3.5121134   5.24088616  9.          0.68973249 19.          0.78159968]. \t  -0.4838024291467368 \t -0.47785117417083445\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.6380286876071068 \t -0.47785117417083445\n",
            "19     \t [ 3.45271174  0.75931377  5.          0.81313484 19.          0.57119025]. \t  -0.5573456183368956 \t -0.47785117417083445\n",
            "20     \t [10.          0.         10.00902395  0.5         1.          0.1       ]. \t  -0.6418158390314135 \t -0.47785117417083445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.797573101898328"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5IYus6UpAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a4fd00-f312-4864-9985-8eb66eec2576"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 9\n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_approx_9 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_9 = GPGO(surrogate_approx_9, Acquisition(util_approx), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
        "approx_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_9 = approx_9.getResult()[0]\n",
        "params_approx_9['max_depth'] = int(params_approx_9['max_depth'])\n",
        "params_approx_9['min_child_weight'] = int(params_approx_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_approx_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_approx_9 = xgb.train(params_approx_9, dX_approx_train9)\n",
        "pred_approx_9 = model_approx_9.predict(dX_approx_test9)\n",
        "\n",
        "rmse_approx_9 = np.sqrt(mean_squared_error(pred_approx_9, y_test9))\n",
        "rmse_approx_9"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.10374154  5.01874592 11.          0.50377155  2.          0.29670281]. \t  -0.6545930802207814 \t -0.4584168030068045\n",
            "init   \t [ 4.18508181  2.48101168 13.          0.69794293  2.          0.25009871]. \t  -0.7166132091943936 \t -0.4584168030068045\n",
            "init   \t [ 8.78559086  9.50964032 13.          0.98395204 11.          0.90820641]. \t  -0.4584168030068045 \t -0.4584168030068045\n",
            "init   \t [ 6.66898973  5.47837783  6.          0.97165345 12.          0.72499481]. \t  -0.48839211091816903 \t -0.4584168030068045\n",
            "init   \t [ 8.24870465  4.65668475 13.          0.68760467  9.          0.98502332]. \t  -0.46354466019784824 \t -0.4584168030068045\n",
            "1      \t [6.73714319 2.39608167 5.         0.58130302 3.         0.163077  ]. \t  -0.7145926373770018 \t -0.4584168030068045\n",
            "2      \t [ 4.24955662  9.67331527 12.          0.64012695  7.          0.96617478]. \t  -0.4599552738922936 \t -0.4584168030068045\n",
            "3      \t [ 3.60566534  9.79805332 11.          0.62032576 16.          0.3578496 ]. \t  -0.6301914521165797 \t -0.4584168030068045\n",
            "4      \t [ 4.86601509  0.61279594  8.          0.72162785 18.          0.68911833]. \t  -0.4897803511428914 \t -0.4584168030068045\n",
            "5      \t [ 0.42678797  0.40430921 14.          0.96202194 10.          0.10119674]. \t  -0.7201378581044144 \t -0.4584168030068045\n",
            "6      \t [1.13863488 5.86180669 5.         0.9953054  3.         0.88639082]. \t  -0.4613511892865009 \t -0.4584168030068045\n",
            "7      \t [9.20185355 9.40235017 8.         0.60433558 1.         0.94540222]. \t  -0.4678077722812364 \t -0.4584168030068045\n",
            "8      \t [ 7.07313313  8.94084339  5.          0.99439529 19.          0.33798274]. \t  -0.6320298567588886 \t -0.4584168030068045\n",
            "9      \t [ 0.32747031  4.15373137 11.          0.86073055 16.          0.59344779]. \t  -0.4847822875702831 \t -0.4584168030068045\n",
            "10     \t [ 9.89680111  6.16985579 13.          0.58428363 17.          0.77251451]. \t  -0.4901785694142925 \t -0.4584168030068045\n",
            "11     \t [ 8.41810681  0.28363289 14.          0.70665052 14.          0.98883394]. \t  -0.47311243562864586 \t -0.4584168030068045\n",
            "12     \t [ 9.36118271  4.59163065 12.          0.62854998  1.          0.54782313]. \t  -0.4908537986889264 \t -0.4584168030068045\n",
            "13     \t [9.80536934 0.66854735 7.         0.82928527 8.         0.6146037 ]. \t  -0.4874891178593158 \t -0.4584168030068045\n",
            "14     \t [ 2.10168671  1.41938081  7.          0.9145822  10.          0.99863976]. \t  -0.4611721300311338 \t -0.4584168030068045\n",
            "15     \t [4.62485155 9.65182222 5.         0.58896427 8.         0.36022331]. \t  -0.6322698857670199 \t -0.4584168030068045\n",
            "16     \t [ 8.77945628  5.51694736 12.          0.84613589  4.          0.64741755]. \t  -0.4798840189273464 \t -0.4584168030068045\n",
            "17     \t [ 0.  10.   5.   0.5 19.   1. ]. \t  -0.4849297045622178 \t -0.4584168030068045\n",
            "18     \t [ 0.2991245   8.82042246  6.          0.51451421 13.          0.59889561]. \t  -0.5099547877139965 \t -0.4584168030068045\n",
            "19     \t [ 2.87706677  6.16957719 14.          0.55246385 12.          0.92866915]. \t  -0.4653617556003965 \t -0.4584168030068045\n",
            "20     \t [ 9.87808543  0.40042764  7.          0.61947699 16.          0.7283732 ]. \t  -0.4990720195733469 \t -0.4584168030068045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6575353004618085"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD494io_Ur7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c630a5f0-e02d-42e5-a995-f726aa9efad4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 10\n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_approx_10 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_10 = GPGO(surrogate_approx_10, Acquisition(util_approx), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
        "approx_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_10 = approx_10.getResult()[0]\n",
        "params_approx_10['max_depth'] = int(params_approx_10['max_depth'])\n",
        "params_approx_10['min_child_weight'] = int(params_approx_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_approx_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_approx_10 = xgb.train(params_approx_10, dX_approx_train10)\n",
        "pred_approx_10 = model_approx_10.predict(dX_approx_test10)\n",
        "\n",
        "rmse_approx_10 = np.sqrt(mean_squared_error(pred_approx_10, y_test10))\n",
        "rmse_approx_10"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.71320643  0.20751949  5.          0.72150747 17.          0.12265456]. \t  -0.7090674967614334 \t -0.4737745634473992\n",
            "init   \t [ 7.0920801   2.65566127 13.          0.57518893 17.          0.83494165]. \t  -0.4737745634473992 \t -0.4737745634473992\n",
            "init   \t [ 3.36071584  8.90816531  6.          0.86087766 15.          0.75469196]. \t  -0.4755277191484213 \t -0.4737745634473992\n",
            "init   \t [ 5.40880931  1.31458152  8.          0.57108502 14.          0.62551123]. \t  -0.48811859212530173 \t -0.4737745634473992\n",
            "init   \t [1.82631436 8.26082248 6.         0.80888349 5.         0.15900694]. \t  -0.7057210222477256 \t -0.4737745634473992\n",
            "1      \t [8.31989768 3.09778055 7.         0.64798085 3.         0.98471878]. \t  \u001b[92m-0.46336171949490257\u001b[0m \t -0.46336171949490257\n",
            "2      \t [ 1.51483713  6.46720195 14.          0.87676044  8.          0.10934204]. \t  -0.7062921047588426 \t -0.46336171949490257\n",
            "3      \t [ 0.44494294  2.20797313 10.          0.76097539  2.          0.34290111]. \t  -0.6180922534763184 \t -0.46336171949490257\n",
            "4      \t [ 6.47425096  8.4482791  10.          0.69239539 11.          0.47622913]. \t  -0.5685254207537167 \t -0.46336171949490257\n",
            "5      \t [ 0.20896963  7.17600684 13.          0.63836859 16.          0.77244006]. \t  \u001b[92m-0.45403228727774253\u001b[0m \t -0.45403228727774253\n",
            "6      \t [ 9.38854854  7.91087361  8.          0.57475286 19.          0.33969987]. \t  -0.6147666257838221 \t -0.45403228727774253\n",
            "7      \t [ 9.16520307  0.72602801 12.          0.91999471  9.          0.54336218]. \t  -0.5572402945816037 \t -0.45403228727774253\n",
            "8      \t [ 8.33810851  9.8990204  14.          0.61893039  5.          0.69227045]. \t  -0.4851208634097359 \t -0.45403228727774253\n",
            "9      \t [ 0.12250572  0.33829451 12.          0.86512188 11.          0.46912604]. \t  -0.5583881911050124 \t -0.45403228727774253\n",
            "10     \t [ 0.8556149   1.34495008  5.          0.88655293 18.          0.15072152]. \t  -0.7048793212420641 \t -0.45403228727774253\n",
            "11     \t [9.53733075 9.92797623 5.         0.78642342 2.         0.9933049 ]. \t  -0.4656240229085077 \t -0.45403228727774253\n",
            "12     \t [1.69575137 2.28628662 5.         0.76046389 8.         0.60065268]. \t  -0.5134648983420089 \t -0.45403228727774253\n",
            "13     \t [ 6.15733989  0.33396443 13.          0.69796716  1.          0.92875212]. \t  \u001b[92m-0.4397628730770955\u001b[0m \t -0.4397628730770955\n",
            "14     \t [ 9.08727548  7.95071568  5.          0.91034544 13.          0.55647706]. \t  -0.5735578706821028 \t -0.4397628730770955\n",
            "15     \t [ 1.45996834  0.59889716 12.          0.82815203 19.          0.24922559]. \t  -0.7069273813967614 \t -0.4397628730770955\n",
            "16     \t [ 9.42421731  1.45800675  5.          0.89810349 11.          0.91467377]. \t  -0.4712218735320602 \t -0.4397628730770955\n",
            "17     \t [ 2.80210285  9.24943163 14.          0.85818339  2.          0.85428456]. \t  \u001b[92m-0.4387844304056118\u001b[0m \t -0.4387844304056118\n",
            "18     \t [7.60796195 7.8170794  5.         0.85145938 8.         0.57832563]. \t  -0.5106716993417335 \t -0.4387844304056118\n",
            "19     \t [ 8.10151611  9.78827602 14.          0.68048433 19.          0.19392154]. \t  -0.7116333725331636 \t -0.4387844304056118\n",
            "20     \t [ 1.11903492  9.93958695  9.          0.79898321 10.          0.44087381]. \t  -0.5625838427005065 \t -0.4387844304056118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.521324504618526"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03Sq0TvUuhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03bbac4-d591-4d83-c267-a357a831dd04"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 11\n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_approx_11 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_11 = GPGO(surrogate_approx_11, Acquisition(util_approx), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
        "approx_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_11 = approx_11.getResult()[0]\n",
        "params_approx_11['max_depth'] = int(params_approx_11['max_depth'])\n",
        "params_approx_11['min_child_weight'] = int(params_approx_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_approx_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_approx_11 = xgb.train(params_approx_11, dX_approx_train11)\n",
        "pred_approx_11 = model_approx_11.predict(dX_approx_test11)\n",
        "\n",
        "rmse_approx_11 = np.sqrt(mean_squared_error(pred_approx_11, y_test11))\n",
        "rmse_approx_11"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.80269689  0.19475241  6.          0.59705781 13.          0.47818324]. \t  -0.5922349044250168 \t -0.49898623219170346\n",
            "init   \t [ 4.85427098  0.12780815  5.          0.91309068 14.          0.86571558]. \t  -0.49898623219170346 \t -0.49898623219170346\n",
            "init   \t [ 7.2996447   1.08736072 10.          0.92857712 18.          0.66910061]. \t  -0.5404544349803458 \t -0.49898623219170346\n",
            "init   \t [ 0.20483613  1.16737269  7.          0.57895615 16.          0.83644782]. \t  -0.5108833748715963 \t -0.49898623219170346\n",
            "init   \t [ 3.44624491  3.18798797 14.          0.54197657 15.          0.63958906]. \t  -0.5545314939891337 \t -0.49898623219170346\n",
            "1      \t [9.77136617 6.6548802  7.         0.51036649 9.         0.81011527]. \t  -0.5252491147925742 \t -0.49898623219170346\n",
            "2      \t [0.5279662  8.15331655 5.         0.83127487 9.         0.53242685]. \t  -0.5931601503405173 \t -0.49898623219170346\n",
            "3      \t [8.62555756 1.5478147  8.         0.99964468 2.         0.74874718]. \t  \u001b[92m-0.490360342610192\u001b[0m \t -0.490360342610192\n",
            "4      \t [ 0.90299561  9.42808632 14.          0.71344248  9.          0.5250902 ]. \t  -0.5778138976577691 \t -0.490360342610192\n",
            "5      \t [ 5.37271973  6.74878506 12.          0.69507758  3.          0.34109729]. \t  -0.5771395586534209 \t -0.490360342610192\n",
            "6      \t [ 2.80563958  8.51387637  8.          0.72474118 19.          0.16647242]. \t  -0.6892058127918498 \t -0.490360342610192\n",
            "7      \t [ 0.77265856  1.6177329  13.          0.54135345  6.          0.64546795]. \t  -0.5525358359299481 \t -0.490360342610192\n",
            "8      \t [0.47065357 6.80536856 5.         0.94482943 1.         0.93677618]. \t  -0.5032158363867651 \t -0.490360342610192\n",
            "9      \t [ 8.74582539  1.09960491 14.          0.69981761  9.          0.73083826]. \t  -0.4904447717887598 \t -0.490360342610192\n",
            "10     \t [ 9.62795963  5.53773092  5.          0.74613073 18.          0.44845649]. \t  -0.6039998092641087 \t -0.490360342610192\n",
            "11     \t [ 8.61765552  0.94349031 10.          0.85641829  4.          0.91576259]. \t  \u001b[92m-0.47852786064513103\u001b[0m \t -0.47852786064513103\n",
            "12     \t [3.50049766 0.06230783 7.         0.82711607 1.         0.94515625]. \t  -0.4818438762403437 \t -0.47852786064513103\n",
            "13     \t [ 9.79448361  9.59732472 14.          0.70844799  8.          0.1372886 ]. \t  -0.689949911392042 \t -0.47852786064513103\n",
            "14     \t [ 8.40730244  5.3235921  14.          0.57824885  9.          0.96689527]. \t  -0.4867986869106529 \t -0.47852786064513103\n",
            "15     \t [ 7.38761283 10.          5.00000002  0.5         1.00000002  1.        ]. \t  -0.5035162153337096 \t -0.47852786064513103\n",
            "16     \t [ 9.7602473   8.08368399 12.          0.65958526 18.          0.9544452 ]. \t  -0.4831334802506264 \t -0.47852786064513103\n",
            "17     \t [ 3.84543028  9.7496114  12.          0.92871344 15.          0.33846707]. \t  -0.5715361401218819 \t -0.47852786064513103\n",
            "18     \t [ 6.19006075  3.41925871 11.          0.95286331  6.          0.39133224]. \t  -0.5745107435758626 \t -0.47852786064513103\n",
            "19     \t [0.77256475 1.22732493 7.         0.97692145 7.         0.9071396 ]. \t  \u001b[92m-0.4728689341077363\u001b[0m \t -0.4728689341077363\n",
            "20     \t [ 4.72034282  5.05300371  8.          0.61154598 11.          0.30373276]. \t  -0.5863184080550365 \t -0.4728689341077363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.084919441252919"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_nP9lQjUztV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9279e8-d313-459c-a162-96787ad47fc7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_approx_12 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_12 = GPGO(surrogate_approx_12, Acquisition(util_approx), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
        "approx_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_12 = approx_12.getResult()[0]\n",
        "params_approx_12['max_depth'] = int(params_approx_12['max_depth'])\n",
        "params_approx_12['min_child_weight'] = int(params_approx_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_approx_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_approx_12 = xgb.train(params_approx_12, dX_approx_train12)\n",
        "pred_approx_12 = model_approx_12.predict(dX_approx_test12)\n",
        "\n",
        "rmse_approx_12 = np.sqrt(mean_squared_error(pred_approx_12, y_test12))\n",
        "rmse_approx_12"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.54162842 7.40049697 6.         0.54321714 4.         0.11311747]. \t  -0.6840535846029854 \t -0.5032799564384677\n",
            "init   \t [ 9.18747008  9.00714854 14.          0.97847467 11.          0.35544552]. \t  -0.6305456734924068 \t -0.5032799564384677\n",
            "init   \t [ 6.06083184  9.44225136 14.          0.95626942  5.          0.56910342]. \t  -0.6144010633484512 \t -0.5032799564384677\n",
            "init   \t [ 5.52037633  4.85377414  7.          0.97886436 17.          0.78810441]. \t  -0.5032799564384677 \t -0.5032799564384677\n",
            "init   \t [ 0.20809798  1.35210178  5.          0.65494879 16.          0.36062811]. \t  -0.6516977337723864 \t -0.5032799564384677\n",
            "1      \t [9.46555822 8.57190559 5.         0.50164398 5.         0.71992807]. \t  -0.5227768283895782 \t -0.5032799564384677\n",
            "2      \t [9.04256367 2.61736915 8.         0.66026854 8.         0.14510453]. \t  -0.6776847411315782 \t -0.5032799564384677\n",
            "3      \t [6.03751892 2.08855857 8.         0.88966175 1.         0.6215545 ]. \t  -0.537741041779077 \t -0.5032799564384677\n",
            "4      \t [ 0.24796255  2.18203944 14.          0.56497025 17.          0.63132662]. \t  -0.5478949507371567 \t -0.5032799564384677\n",
            "5      \t [ 1.93384153  7.13950146  8.          0.85480597 18.          0.33734734]. \t  -0.6395317281169034 \t -0.5032799564384677\n",
            "6      \t [ 0.40359854  2.22527636 10.          0.60258213 10.          0.38255809]. \t  -0.6389275427144069 \t -0.5032799564384677\n",
            "7      \t [ 0.28427394  4.67296732 14.          0.84909523  4.          0.99176009]. \t  \u001b[92m-0.46369522867609564\u001b[0m \t -0.46369522867609564\n",
            "8      \t [ 7.63658847  0.39719075 14.          0.96199388 14.          0.84093877]. \t  -0.48258324270646796 \t -0.46369522867609564\n",
            "9      \t [ 0.50213582  8.87075    12.          0.71856843 12.          0.66120412]. \t  -0.5367112816028642 \t -0.46369522867609564\n",
            "10     \t [ 4.27921374  9.2199845   6.          0.67076861 12.          0.56605459]. \t  -0.628155564863435 \t -0.46369522867609564\n",
            "11     \t [ 7.63578483  4.07501457 14.          0.97723751  1.          0.2033266 ]. \t  -0.6773954967709924 \t -0.46369522867609564\n",
            "12     \t [ 9.06259994  1.4172751   5.          0.81347212 19.          0.38881712]. \t  -0.650363142837499 \t -0.46369522867609564\n",
            "13     \t [ 9.38461996  5.62749581 12.          0.98689844 17.          0.89710846]. \t  -0.4762695359794075 \t -0.46369522867609564\n",
            "14     \t [ 4.55964005  0.30434123  5.          0.52399968 11.          0.14256504]. \t  -0.6841482061412913 \t -0.46369522867609564\n",
            "15     \t [ 6.59943135  1.71178305 14.          0.69283152  8.          0.30459736]. \t  -0.635622313137239 \t -0.46369522867609564\n",
            "16     \t [5.99294447 7.0114806  5.         0.86352024 2.         0.47030879]. \t  -0.629371597372417 \t -0.46369522867609564\n",
            "17     \t [ 9.63630565  6.83547158  8.          0.6195521  13.          0.11712353]. \t  -0.6780822711639433 \t -0.46369522867609564\n",
            "18     \t [0.93465239 0.11560545 5.         0.52904926 2.         0.78190279]. \t  -0.5147104266754287 \t -0.46369522867609564\n",
            "19     \t [ 4.02953989  6.35914994 11.          0.73063178  8.          0.53793685]. \t  -0.6140059235484745 \t -0.46369522867609564\n",
            "20     \t [ 0.49442426  9.20422434 11.          0.70522852  1.          0.50168224]. \t  -0.62709850175482 \t -0.46369522867609564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.548920198104285"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDI2Bi9vU05U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5693ae2a-98ee-49d3-bf4b-84f7872abe31"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 13\n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_approx_13 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_13 = GPGO(surrogate_approx_13, Acquisition(util_approx), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
        "approx_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_13 = approx_13.getResult()[0]\n",
        "params_approx_13['max_depth'] = int(params_approx_13['max_depth'])\n",
        "params_approx_13['min_child_weight'] = int(params_approx_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_approx_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_approx_13 = xgb.train(params_approx_13, dX_approx_train13)\n",
        "pred_approx_13 = model_approx_13.predict(dX_approx_test13)\n",
        "\n",
        "rmse_approx_13 = np.sqrt(mean_squared_error(pred_approx_13, y_test13))\n",
        "rmse_approx_13"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.77702411  2.3754122  11.          0.94649135 13.          0.7827256 ]. \t  -0.5099204187421568 \t -0.5099204187421568\n",
            "init   \t [ 7.51661514  6.07343344 11.          0.69402149 11.          0.13153287]. \t  -0.7086153631136594 \t -0.5099204187421568\n",
            "init   \t [ 2.98449471  0.58512492 10.          0.73579614 12.          0.33065195]. \t  -0.6372461781857162 \t -0.5099204187421568\n",
            "init   \t [ 3.47581215  0.0941277  11.          0.86143432  8.          0.58454932]. \t  -0.5702017504451442 \t -0.5099204187421568\n",
            "init   \t [ 4.70137857  6.24432527 10.          0.8149145  18.          0.10784416]. \t  -0.7101125715665313 \t -0.5099204187421568\n",
            "1      \t [1.1119361  5.43221306 6.         0.56899303 8.         0.32100319]. \t  -0.6493427493033831 \t -0.5099204187421568\n",
            "2      \t [5.39023698 3.80105709 8.         0.54170057 1.         0.56798884]. \t  -0.621866591058205 \t -0.5099204187421568\n",
            "3      \t [ 0.5185863   5.23876151 13.          0.63798348  5.          0.7914799 ]. \t  \u001b[92m-0.5090480969691882\u001b[0m \t -0.5090480969691882\n",
            "4      \t [ 9.65518672  0.13040633  6.          0.63296628 18.          0.44133279]. \t  -0.632314879589983 \t -0.5090480969691882\n",
            "5      \t [ 9.80722669  7.22571611  7.          0.5026908  19.          0.92519376]. \t  -0.5113851690128051 \t -0.5090480969691882\n",
            "6      \t [ 6.75965929  9.42320667 14.          0.75932127  4.          0.51195623]. \t  -0.6206028595311828 \t -0.5090480969691882\n",
            "7      \t [9.95671825 0.88335607 5.         0.76593799 7.         0.60049246]. \t  -0.5952873237754599 \t -0.5090480969691882\n",
            "8      \t [ 1.88898055  9.92199995 14.          0.53783103 12.          0.63359705]. \t  -0.5738907345204808 \t -0.5090480969691882\n",
            "9      \t [0.32121091 9.03384774 6.         0.79493663 1.         0.29330571]. \t  -0.6544818879601484 \t -0.5090480969691882\n",
            "10     \t [ 9.64211232  3.05396831 11.          0.80784352  5.          0.67910498]. \t  -0.5747201434527256 \t -0.5090480969691882\n",
            "11     \t [ 1.60018805  0.54211528  7.          0.80910607 18.          0.55232873]. \t  -0.6209454018308087 \t -0.5090480969691882\n",
            "12     \t [ 6.1829314   9.53799326  5.          0.66916589 13.          0.57013155]. \t  -0.6295351865431291 \t -0.5090480969691882\n",
            "13     \t [7.99022981 8.23715587 5.         0.79172469 5.         0.39852784]. \t  -0.6543343020941466 \t -0.5090480969691882\n",
            "14     \t [ 8.15066897  1.98276445 13.          0.64083395 19.          0.16153982]. \t  -0.7124294948180114 \t -0.5090480969691882\n",
            "15     \t [ 2.48128047  9.70146472  6.          0.59664322 19.          0.9781653 ]. \t  -0.5140838980696801 \t -0.5090480969691882\n",
            "16     \t [ 9.08793394  9.83715934 14.          0.60607659 14.          0.5136415 ]. \t  -0.6216172534981154 \t -0.5090480969691882\n",
            "17     \t [ 1.80857777  4.26197    14.          0.84061579 16.          0.40224246]. \t  -0.637460469665298 \t -0.5090480969691882\n",
            "18     \t [ 5.1333583   3.10658752 14.          0.74854361  1.          0.92539511]. \t  \u001b[92m-0.4793091157473782\u001b[0m \t -0.4793091157473782\n",
            "19     \t [ 0.65009462  9.91088996 13.          0.92448902  2.          0.37838494]. \t  -0.6501362550952002 \t -0.4793091157473782\n",
            "20     \t [ 0.18073363  6.71350851  7.          0.80658597 14.          0.6264676 ]. \t  -0.5752866688439547 \t -0.4793091157473782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.601162573826364"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2F_Q194U3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b96351-ce95-4f9d-8809-ec843415946f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 14\n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_approx_14 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_14 = GPGO(surrogate_approx_14, Acquisition(util_approx), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
        "approx_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_14 = approx_14.getResult()[0]\n",
        "params_approx_14['max_depth'] = int(params_approx_14['max_depth'])\n",
        "params_approx_14['min_child_weight'] = int(params_approx_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_approx_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_approx_14 = xgb.train(params_approx_14, dX_approx_train14)\n",
        "pred_approx_14 = model_approx_14.predict(dX_approx_test14)\n",
        "\n",
        "rmse_approx_14 = np.sqrt(mean_squared_error(pred_approx_14, y_test14))\n",
        "rmse_approx_14"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.13943344  7.73165052 12.          0.6831412  11.          0.37876233]. \t  -0.558794499921046 \t -0.4448140077853998\n",
            "init   \t [ 9.57603739  5.13116712 14.          0.76959997 12.          0.71328228]. \t  -0.49979629433789113 \t -0.4448140077853998\n",
            "init   \t [5.34950319 2.47493539 5.         0.50293689 6.         0.29706373]. \t  -0.5741697988899073 \t -0.4448140077853998\n",
            "init   \t [ 2.94506579  3.45329697  8.          0.87620946 14.          0.9783044 ]. \t  -0.4448140077853998 \t -0.4448140077853998\n",
            "init   \t [ 1.11811929  1.73004086  5.          0.73745288 12.          0.20586008]. \t  -0.6214151152359092 \t -0.4448140077853998\n",
            "1      \t [ 6.50637223  2.67617722 14.          0.53562507  1.          0.16862152]. \t  -0.6294794339238933 \t -0.4448140077853998\n",
            "2      \t [ 7.31258536  0.20867922 13.          0.74156619 18.          0.34815138]. \t  -0.5581312723611839 \t -0.4448140077853998\n",
            "3      \t [ 0.28409124  4.13353348 13.          0.96339983  6.          0.90100709]. \t  \u001b[92m-0.42439805248956936\u001b[0m \t -0.42439805248956936\n",
            "4      \t [9.32373648 9.05676215 9.         0.53064322 3.         0.70657534]. \t  -0.5066999638361388 \t -0.42439805248956936\n",
            "5      \t [ 9.52454394  8.82757271  9.          0.90064956 19.          0.16022914]. \t  -0.6201852747998959 \t -0.42439805248956936\n",
            "6      \t [ 3.61508571  7.70718733 10.          0.62042707  5.          0.12186292]. \t  -0.6265638054423092 \t -0.42439805248956936\n",
            "7      \t [ 0.05037086  7.50461284  5.          0.70076046 19.          0.10543546]. \t  -0.62079470471519 \t -0.42439805248956936\n",
            "8      \t [ 9.40430013  0.15700131  7.          0.99940272 12.          0.43771306]. \t  -0.50114980828739 \t -0.42439805248956936\n",
            "9      \t [ 0.08962521  4.02073781 13.          0.7602714  18.          0.37402763]. \t  -0.5480362115976656 \t -0.42439805248956936\n",
            "10     \t [ 2.08494663  9.70581169 14.          0.64307382  3.          0.10613693]. \t  -0.6314022013205511 \t -0.42439805248956936\n",
            "11     \t [ 1.55781918  0.46142569 10.          0.80240433 16.          0.22604037]. \t  -0.6223833972775541 \t -0.42439805248956936\n",
            "12     \t [ 8.32471637  8.64868248  5.          0.80719895 12.          0.64183859]. \t  -0.5064508572748014 \t -0.42439805248956936\n",
            "13     \t [8.83263595 5.04727069 5.         0.78314986 1.         0.35219474]. \t  -0.5593356200473169 \t -0.42439805248956936\n",
            "14     \t [0.73443351 4.9474333  6.         0.76758914 1.         0.38158758]. \t  -0.5544761292575502 \t -0.42439805248956936\n",
            "15     \t [9.46744067 0.0339838  9.         0.74305504 6.         0.26947831]. \t  -0.621447358328987 \t -0.42439805248956936\n",
            "16     \t [ 6.43832075  1.19978471 13.          0.97485923  9.          0.89110744]. \t  -0.43457885965616294 \t -0.42439805248956936\n",
            "17     \t [ 7.58744711  5.19042144  5.          0.89542819 17.          0.12960574]. \t  -0.6210679687986883 \t -0.42439805248956936\n",
            "18     \t [ 1.39364497  9.10181008 11.          0.74858439 19.          0.91824055]. \t  -0.4509731488166258 \t -0.42439805248956936\n",
            "19     \t [ 6.6896573   9.49786322 14.          0.98343455 18.          0.89331088]. \t  -0.44269918266909547 \t -0.42439805248956936\n",
            "20     \t [ 0.17539185  8.79567306  9.          0.56601724 13.          0.83976647]. \t  -0.48014414278881856 \t -0.42439805248956936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.562117790499774"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po5wImJaU6VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e65edf-3e31-4822-f21f-7fed8af2b3c6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 15\n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_approx_15 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_15 = GPGO(surrogate_approx_15, Acquisition(util_approx), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
        "approx_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_15 = approx_15.getResult()[0]\n",
        "params_approx_15['max_depth'] = int(params_approx_15['max_depth'])\n",
        "params_approx_15['min_child_weight'] = int(params_approx_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_approx_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_approx_15 = xgb.train(params_approx_15, dX_approx_train15)\n",
        "pred_approx_15 = model_approx_15.predict(dX_approx_test15)\n",
        "\n",
        "rmse_approx_15 = np.sqrt(mean_squared_error(pred_approx_15, y_test15))\n",
        "rmse_approx_15"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.48817697  1.78895925 12.          0.55549316  8.          0.93397854]. \t  -0.48943791400638287 \t -0.48943791400638287\n",
            "init   \t [ 0.24953032  8.22298097 12.          0.62494951 11.          0.12924598]. \t  -0.6992441679399787 \t -0.48943791400638287\n",
            "init   \t [ 5.02017228  5.50882771 11.          0.85295832 19.          0.13548008]. \t  -0.69945028775584 \t -0.48943791400638287\n",
            "init   \t [2.0023081  9.98543403 7.         0.6295772  2.         0.526127  ]. \t  -0.6264981748624211 \t -0.48943791400638287\n",
            "init   \t [ 5.09715306  9.45038417 11.          0.7388277  16.          0.22739973]. \t  -0.6964615643692806 \t -0.48943791400638287\n",
            "1      \t [ 0.29158961  4.9949242  12.          0.89124583  3.          0.67554049]. \t  -0.56524595824364 \t -0.48943791400638287\n",
            "2      \t [3.68214008 4.55748717 6.         0.60488381 8.         0.88973248]. \t  -0.494310985641296 \t -0.48943791400638287\n",
            "3      \t [9.75991344 6.15203198 6.         0.65490407 1.         0.73816291]. \t  -0.49903240523735964 \t -0.48943791400638287\n",
            "4      \t [ 9.51793103  9.55070381  6.          0.93315157 11.          0.40416985]. \t  -0.688799230055169 \t -0.48943791400638287\n",
            "5      \t [ 6.65116837  8.16324548 14.          0.95750787  1.          0.74925927]. \t  \u001b[92m-0.4743330993644729\u001b[0m \t -0.4743330993644729\n",
            "6      \t [ 0.41861043  0.05076637  5.          0.78940316 12.          0.77619725]. \t  -0.5149313800202362 \t -0.4743330993644729\n",
            "7      \t [ 0.44758083  1.21361479 14.          0.72053858 16.          0.73785377]. \t  -0.48610261627787 \t -0.4743330993644729\n",
            "8      \t [ 9.36080884  1.0313168   5.          0.6330142  14.          0.51274968]. \t  -0.6459684741407418 \t -0.4743330993644729\n",
            "9      \t [ 3.89599673  7.59173972  5.          0.94509004 14.          0.68417344]. \t  -0.5757237316898676 \t -0.4743330993644729\n",
            "10     \t [4.3552321  3.34830177 9.         0.76229473 1.         0.13073039]. \t  -0.6974379414299371 \t -0.4743330993644729\n",
            "11     \t [ 0.22313453  7.46320923  9.          0.98956429 19.          0.22753615]. \t  -0.6992681026416028 \t -0.4743330993644729\n",
            "12     \t [ 3.91025291  0.94746935  6.          0.83827135 18.          0.68489482]. \t  -0.5761732475509362 \t -0.4743330993644729\n",
            "13     \t [ 1.41127451  0.02455301 14.          0.99438541  9.          0.56009967]. \t  -0.6155173676416096 \t -0.4743330993644729\n",
            "14     \t [ 7.09557213  0.02461437 14.          0.76122797 15.          0.64719348]. \t  -0.5671440080891083 \t -0.4743330993644729\n",
            "15     \t [ 7.70971055  5.9598343  10.          0.83735899 18.          0.36826156]. \t  -0.6853727999168309 \t -0.4743330993644729\n",
            "16     \t [0.         0.         5.         0.68017419 4.56917351 0.67131931]. \t  -0.5783977707433609 \t -0.4743330993644729\n",
            "17     \t [9.52146177 3.75389952 5.         0.5049997  7.         0.95818795]. \t  -0.5077587114042463 \t -0.4743330993644729\n",
            "18     \t [ 8.77636678  8.74391722 14.          0.79691628 11.          0.9639423 ]. \t  \u001b[92m-0.47422405894109615\u001b[0m \t -0.47422405894109615\n",
            "19     \t [ 2.27839391  3.48085254  9.          0.82265329 13.          0.31218776]. \t  -0.6838527397579454 \t -0.47422405894109615\n",
            "20     \t [10.   0.   5.   0.5  1.   1. ]. \t  -0.48694397576155424 \t -0.47422405894109615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.589957692331628"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrAQN-pU9Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e966dd-10b8-406b-b051-3e61171fdfc3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 16\n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_approx_16 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_16 = GPGO(surrogate_approx_16, Acquisition(util_approx), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
        "approx_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_16 = approx_16.getResult()[0]\n",
        "params_approx_16['max_depth'] = int(params_approx_16['max_depth'])\n",
        "params_approx_16['min_child_weight'] = int(params_approx_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_approx_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_approx_16 = xgb.train(params_approx_16, dX_approx_train16)\n",
        "pred_approx_16 = model_approx_16.predict(dX_approx_test16)\n",
        "\n",
        "rmse_approx_16 = np.sqrt(mean_squared_error(pred_approx_16, y_test16))\n",
        "rmse_approx_16"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.23291079 5.23163341 6.         0.65430839 5.         0.30077285]. \t  -0.6357813258069683 \t -0.6345701590947206\n",
            "init   \t [6.88726162 1.63731425 7.         0.97050543 2.         0.25392012]. \t  -0.7029752724132097 \t -0.6345701590947206\n",
            "init   \t [ 5.94328983  5.6393473   5.          0.67602695 19.          0.42538144]. \t  -0.6345701590947206 \t -0.6345701590947206\n",
            "init   \t [ 0.88741148  3.08148142 14.          0.56043938  9.          0.27515386]. \t  -0.7076230970293895 \t -0.6345701590947206\n",
            "init   \t [ 2.74631586  1.30996118 11.          0.52160786  8.          0.27956463]. \t  -0.7061563820165734 \t -0.6345701590947206\n",
            "1      \t [ 7.8937256   1.5972923  14.          0.61610774 17.          0.78739284]. \t  \u001b[92m-0.5498317738591506\u001b[0m \t -0.5498317738591506\n",
            "2      \t [ 9.01655783  8.21383177  9.          0.60772965 10.          0.9401803 ]. \t  \u001b[92m-0.4830822737254163\u001b[0m \t -0.4830822737254163\n",
            "3      \t [ 4.35132073  9.89698316 12.          0.94137984 16.          0.57741056]. \t  -0.5389659972108385 \t -0.4830822737254163\n",
            "4      \t [ 3.38377852  9.31285251 11.          0.88244942  1.          0.67627774]. \t  -0.5404220213450739 \t -0.4830822737254163\n",
            "5      \t [ 9.63904847  1.13624975 14.          0.67706869  2.          0.95236673]. \t  \u001b[92m-0.4641319096590556\u001b[0m \t -0.4641319096590556\n",
            "6      \t [ 0.19317903  0.6596816   6.          0.86233301 15.          0.41906313]. \t  -0.6308098904949025 \t -0.4641319096590556\n",
            "7      \t [ 3.86147645  9.68905939  5.          0.65198689 12.          0.68426055]. \t  -0.5557826482727706 \t -0.4641319096590556\n",
            "8      \t [ 1.22130867  0.64008351 12.          0.59515137 19.          0.65193522]. \t  -0.5486769508942775 \t -0.4641319096590556\n",
            "9      \t [8.31596139 8.08775071 5.         0.5698323  1.         0.11769544]. \t  -0.708927036050009 \t -0.4641319096590556\n",
            "10     \t [ 9.39421065  1.15238895 10.          0.86257926 10.          0.82976642]. \t  -0.5380808068994449 \t -0.4641319096590556\n",
            "11     \t [ 8.63952355  8.87914214 14.          0.57943185  3.          0.83429859]. \t  -0.5495434658820855 \t -0.4641319096590556\n",
            "12     \t [ 2.23523952  3.62733473 12.          0.51556206  2.          0.2036675 ]. \t  -0.7124687886931153 \t -0.4641319096590556\n",
            "13     \t [ 0.55900516  9.18229404  7.          0.95696256 17.          0.1903245 ]. \t  -0.701856248538269 \t -0.4641319096590556\n",
            "14     \t [ 5.94853197  4.92024227  5.          0.75769119 13.          0.52349427]. \t  -0.5762899773358159 \t -0.4641319096590556\n",
            "15     \t [ 3.22380477  8.8210053  12.          0.91827732  7.          0.1365336 ]. \t  -0.7049504170537606 \t -0.4641319096590556\n",
            "16     \t [ 9.80203814  5.08834263 14.          0.60567693  8.          0.92815593]. \t  -0.4763161418293736 \t -0.4641319096590556\n",
            "17     \t [ 0.27313091  7.4504504  11.          0.83119788 12.          0.13163591]. \t  -0.7048204941457863 \t -0.4641319096590556\n",
            "18     \t [ 4.68384527  1.49985833 10.          0.81523271 14.          0.8945648 ]. \t  -0.4703046715523559 \t -0.4641319096590556\n",
            "19     \t [ 9.51378666  7.51596488 11.          0.92606042 19.          0.78246721]. \t  -0.5394207848307663 \t -0.4641319096590556\n",
            "20     \t [ 9.8049394   5.64638797 12.          0.54117047 14.          0.36652185]. \t  -0.638525088069505 \t -0.4641319096590556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.641237803563213"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXelbcAVVCqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6b2e44-1293-401e-9149-42afe5cbd94e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 17\n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_approx_17 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_17 = GPGO(surrogate_approx_17, Acquisition(util_approx), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
        "approx_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_17 = approx_17.getResult()[0]\n",
        "params_approx_17['max_depth'] = int(params_approx_17['max_depth'])\n",
        "params_approx_17['min_child_weight'] = int(params_approx_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_approx_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_approx_17 = xgb.train(params_approx_17, dX_approx_train17)\n",
        "pred_approx_17 = model_approx_17.predict(dX_approx_test17)\n",
        "\n",
        "rmse_approx_17 = np.sqrt(mean_squared_error(pred_approx_17, y_test17))\n",
        "rmse_approx_17"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.94665003  5.30586756 11.          0.94443241 14.          0.80828691]. \t  -0.48092361225642916 \t -0.48092361225642916\n",
            "init   \t [ 6.56333522  6.37520896 12.          0.81487881 18.          0.42203224]. \t  -0.634455605137701 \t -0.48092361225642916\n",
            "init   \t [ 9.45683187  0.6004468  11.          0.5171566  10.          0.53881211]. \t  -0.6046684392629649 \t -0.48092361225642916\n",
            "init   \t [2.72705857 1.19063434 6.         0.74176431 6.         0.10101151]. \t  -0.6562801618178493 \t -0.48092361225642916\n",
            "init   \t [ 4.77631812  5.24671297 13.          0.66254476 19.          0.36708086]. \t  -0.6415104035419145 \t -0.48092361225642916\n",
            "1      \t [ 0.65702322  5.79284078 13.          0.75136902  1.          0.30306068]. \t  -0.6424064809093688 \t -0.48092361225642916\n",
            "2      \t [ 6.93446178  8.68032298 13.          0.78195789  7.          0.91906958]. \t  \u001b[92m-0.47529593119031155\u001b[0m \t -0.47529593119031155\n",
            "3      \t [9.72843652 3.88893279 9.         0.6901555  1.         0.31608219]. \t  -0.6407338037654788 \t -0.47529593119031155\n",
            "4      \t [ 9.65057736  8.52725784  5.          0.68420234 13.          0.40008732]. \t  -0.6446152251367371 \t -0.47529593119031155\n",
            "5      \t [ 4.97204887  2.40072226  5.          0.54268748 19.          0.30995407]. \t  -0.6473150344064219 \t -0.47529593119031155\n",
            "6      \t [0.12174033 8.73496008 5.         0.89827646 5.         0.85354798]. \t  -0.4993728869548043 \t -0.47529593119031155\n",
            "7      \t [ 2.91443079  0.16723755 13.          0.598201    6.          0.91729605]. \t  -0.48363736808522184 \t -0.47529593119031155\n",
            "8      \t [7.20615247 9.36901627 6.         0.85465034 7.         0.6878262 ]. \t  -0.5223074457605629 \t -0.47529593119031155\n",
            "9      \t [ 9.02586164  0.59354638 10.          0.86038693 18.          0.90794111]. \t  -0.48184301693459747 \t -0.47529593119031155\n",
            "10     \t [ 0.          5.95090281  5.          0.5        11.3592988   0.1       ]. \t  -0.6636626772067994 \t -0.47529593119031155\n",
            "11     \t [ 0.12410542  7.60180472 13.          0.97425003  8.          0.76245421]. \t  \u001b[92m-0.47195842108145214\u001b[0m \t -0.47195842108145214\n",
            "12     \t [ 2.44282715  9.71851747  7.          0.86792183 17.          0.93015556]. \t  -0.48228384573163224 \t -0.47195842108145214\n",
            "13     \t [9.59190042 1.26233772 5.         0.55398722 6.         0.46259714]. \t  -0.6127976173878007 \t -0.47195842108145214\n",
            "14     \t [ 9.06002681  9.03779351 14.          0.60614154  1.          0.16421461]. \t  -0.6627557424336108 \t -0.47195842108145214\n",
            "15     \t [ 0.59590325  0.30071901  8.          0.9148403  16.          0.62633871]. \t  -0.507330476870976 \t -0.47195842108145214\n",
            "16     \t [ 8.69529605  0.27226865 14.          0.90636352  1.          0.39638219]. \t  -0.6352506435607296 \t -0.47195842108145214\n",
            "17     \t [ 5.5997101   2.80089542 11.          0.91397635  4.          0.58148876]. \t  -0.49267256921662134 \t -0.47195842108145214\n",
            "18     \t [ 7.37738963  2.47849041  7.          0.56824573 14.          0.14404512]. \t  -0.663364864665205 \t -0.47195842108145214\n",
            "19     \t [ 9.24004771  6.36004976 11.          0.81644504 12.          0.66235401]. \t  -0.5080582866060617 \t -0.47195842108145214\n",
            "20     \t [10. 10.  5.  1.  1.  1.]. \t  -0.4876581060509898 \t -0.47195842108145214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.758964442352727"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJG2fAtAVFDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82b6572-4d34-499d-f2ff-9690d6db4bef"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 18\n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_approx_18 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_18 = GPGO(surrogate_approx_18, Acquisition(util_approx), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
        "approx_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_18 = approx_18.getResult()[0]\n",
        "params_approx_18['max_depth'] = int(params_approx_18['max_depth'])\n",
        "params_approx_18['min_child_weight'] = int(params_approx_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_approx_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_approx_18 = xgb.train(params_approx_18, dX_approx_train18)\n",
        "pred_approx_18 = model_approx_18.predict(dX_approx_test18)\n",
        "\n",
        "rmse_approx_18 = np.sqrt(mean_squared_error(pred_approx_18, y_test18))\n",
        "rmse_approx_18"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.50374242 5.05453374 6.         0.59092011 3.         0.28357516]. \t  -0.6513740959050699 \t -0.4474949996843899\n",
            "init   \t [0.11506734 4.26891483 9.         0.81785956 5.         0.63489043]. \t  -0.5068571689279089 \t -0.4474949996843899\n",
            "init   \t [ 2.8861259   6.35547834 11.          0.64267955 14.          0.27877092]. \t  -0.6466851980958979 \t -0.4474949996843899\n",
            "init   \t [6.57189031 6.99655629 8.         0.63235896 4.         0.52894035]. \t  -0.5514036168275981 \t -0.4474949996843899\n",
            "init   \t [ 6.66600348  2.11312037 14.          0.74363461  4.          0.73174558]. \t  -0.4474949996843899 \t -0.4474949996843899\n",
            "1      \t [ 8.67093232  0.11649132  5.          0.92962202 15.          0.53672863]. \t  -0.5562589356787108 \t -0.4474949996843899\n",
            "2      \t [ 8.43851229  2.41114508 13.          0.75771586 19.          0.86905071]. \t  \u001b[92m-0.44050164109333145\u001b[0m \t -0.44050164109333145\n",
            "3      \t [ 9.44281001  9.01534322  7.          0.99142432 16.          0.37631199]. \t  -0.549158457418693 \t -0.44050164109333145\n",
            "4      \t [ 3.19538294  9.91737336 14.          0.7976317   5.          0.25704487]. \t  -0.6515598146742108 \t -0.44050164109333145\n",
            "5      \t [ 1.97643014  8.37982471  5.          0.63246176 17.          0.45403539]. \t  -0.5634039832057651 \t -0.44050164109333145\n",
            "6      \t [ 1.26601315  0.31299408  6.          0.95296222 16.          0.13649883]. \t  -0.6466742489441083 \t -0.44050164109333145\n",
            "7      \t [ 1.18347798  2.03195078 14.          0.62549517 10.          0.6517083 ]. \t  -0.511373695923542 \t -0.44050164109333145\n",
            "8      \t [6.72039962 1.0287777  9.         0.62848622 9.         0.70815446]. \t  -0.5159589935554622 \t -0.44050164109333145\n",
            "9      \t [ 7.49192948  9.60283663 14.          0.93718151 19.          0.24625933]. \t  -0.6476549496904568 \t -0.44050164109333145\n",
            "10     \t [ 3.63870552  9.73349763  7.          0.73625258 11.          0.87968363]. \t  -0.4484432027321388 \t -0.44050164109333145\n",
            "11     \t [9.98653758 8.80568206 9.         0.86984433 9.         0.78984159]. \t  -0.45607631534129245 \t -0.44050164109333145\n",
            "12     \t [ 1.68019344  0.20490035 13.          0.89332378 19.          0.17761947]. \t  -0.6471945108281247 \t -0.44050164109333145\n",
            "13     \t [0.58655573 8.89860432 5.         0.62593909 1.         0.46819727]. \t  -0.5622837222481027 \t -0.44050164109333145\n",
            "14     \t [ 4.25762222  4.02301922  9.          0.6129246  19.          0.59981465]. \t  -0.5142020783326252 \t -0.44050164109333145\n",
            "15     \t [ 5.93963174  0.35038192 14.          0.99474968 14.          0.46057279]. \t  -0.5351521207701755 \t -0.44050164109333145\n",
            "16     \t [2.5556895  0.91787864 5.         0.57129972 6.         0.40466558]. \t  -0.5665188856299738 \t -0.44050164109333145\n",
            "17     \t [ 8.60576508  8.57930928 14.          0.78656855  7.          0.59728424]. \t  -0.5071486743154364 \t -0.44050164109333145\n",
            "18     \t [ 9.56072091  9.1368036  14.          0.68050527  1.          0.47597207]. \t  -0.5560730708083954 \t -0.44050164109333145\n",
            "19     \t [ 7.60556784  0.42682492 10.          0.97994323  1.          0.15105913]. \t  -0.6491267657624828 \t -0.44050164109333145\n",
            "20     \t [ 2.08722767  3.98919754  5.          0.76742571 11.          0.93277603]. \t  -0.46014482327322215 \t -0.44050164109333145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.746067780922308"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHidSEGcVHvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3d49bf-87fd-4af4-83db-187e7c896d29"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 19\n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_approx_19 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_19 = GPGO(surrogate_approx_19, Acquisition(util_approx), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
        "approx_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_19 = approx_19.getResult()[0]\n",
        "params_approx_19['max_depth'] = int(params_approx_19['max_depth'])\n",
        "params_approx_19['min_child_weight'] = int(params_approx_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_approx_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_approx_19 = xgb.train(params_approx_19, dX_approx_train19)\n",
        "pred_approx_19 = model_approx_19.predict(dX_approx_test19)\n",
        "\n",
        "rmse_approx_19 = np.sqrt(mean_squared_error(pred_approx_19, y_test19))\n",
        "rmse_approx_19"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.97533602  7.61249717 13.          0.85765469 11.          0.39830191]. \t  -0.5851081958447377 \t -0.4870287725699859\n",
            "init   \t [ 0.82999565  6.71977081  6.          0.50407413 19.          0.67209466]. \t  -0.5245729886945251 \t -0.4870287725699859\n",
            "init   \t [ 2.15923256  5.49027432 12.          0.52588686 10.          0.20235326]. \t  -0.677999989539271 \t -0.4870287725699859\n",
            "init   \t [4.99659267 1.52108422 6.         0.73481085 4.         0.71949465]. \t  -0.4940235599803803 \t -0.4870287725699859\n",
            "init   \t [ 3.72927156  9.46160045  5.          0.80554614 18.          0.97708466]. \t  -0.4870287725699859 \t -0.4870287725699859\n",
            "1      \t [ 8.33060043  1.42030563  8.          0.92863724 14.          0.78606141]. \t  \u001b[92m-0.48699206254226385\u001b[0m \t -0.48699206254226385\n",
            "2      \t [ 4.70068371  4.9755295  14.          0.98901029  1.          0.96039614]. \t  \u001b[92m-0.454763872590455\u001b[0m \t -0.454763872590455\n",
            "3      \t [9.07948237 9.55063617 7.         0.80962147 4.         0.34142584]. \t  -0.5905722937491956 \t -0.454763872590455\n",
            "4      \t [ 3.65000245  2.90359952 13.          0.98940034 19.          0.29019455]. \t  -0.5803493338026307 \t -0.454763872590455\n",
            "5      \t [0.25768796 8.33414072 6.         0.99948369 4.         0.66680619]. \t  -0.5074502274890225 \t -0.454763872590455\n",
            "6      \t [ 8.89243569  4.2136102  10.          0.88870164  8.          0.77683659]. \t  -0.4793573657831819 \t -0.454763872590455\n",
            "7      \t [ 9.20734788  9.40702602 11.          0.68264617 14.          0.68723167]. \t  -0.5088698943836598 \t -0.454763872590455\n",
            "8      \t [1.60895472e-01 8.27074864e-03 1.30000000e+01 6.91893018e-01\n",
            " 5.00000000e+00 4.60327119e-01]. \t  -0.5676975299111877 \t -0.454763872590455\n",
            "9      \t [ 0.32873959  7.7155114   5.          0.56871173 11.          0.34471029]. \t  -0.5850396812677789 \t -0.454763872590455\n",
            "10     \t [ 6.55489773  0.06438745 14.          0.53351105 11.          0.76391016]. \t  -0.4835963309582638 \t -0.454763872590455\n",
            "11     \t [ 9.32390837  9.51845123 14.          0.71072327  2.          0.55726506]. \t  -0.5721136978716921 \t -0.454763872590455\n",
            "12     \t [ 4.31405249  0.92741236  7.          0.67333637 19.          0.97097788]. \t  -0.47827249796188526 \t -0.454763872590455\n",
            "13     \t [ 4.44431879  9.49168646 13.          0.64581011 19.          0.4702202 ]. \t  -0.5592489367596285 \t -0.454763872590455\n",
            "14     \t [ 5.91102876  9.59864462 12.          0.78362129  7.          0.97877384]. \t  -0.4591791053241341 \t -0.454763872590455\n",
            "15     \t [0.22611985 0.1170465  7.         0.55795985 9.         0.86758401]. \t  -0.47279581874539717 \t -0.454763872590455\n",
            "16     \t [ 9.0780433   8.49155787  5.          0.8925855  13.          0.38273904]. \t  -0.5938832064204094 \t -0.454763872590455\n",
            "17     \t [ 9.89996921  0.59570014 13.          0.67906413  1.          0.37793763]. \t  -0.5890751717161915 \t -0.454763872590455\n",
            "18     \t [ 0.65734769  9.79668398 13.          0.57710479  1.          0.50646708]. \t  -0.5771756215263869 \t -0.454763872590455\n",
            "19     \t [ 0.17032432  0.5511349   8.          0.8772351  15.          0.24483614]. \t  -0.6798814652826352 \t -0.454763872590455\n",
            "20     \t [ 3.82670531  7.06245628  8.          0.7747773  14.          0.11384763]. \t  -0.67893464439286 \t -0.454763872590455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.63424117920896"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWGPYRJhVKsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c2c4e3-2230-4225-c856-c66d4ea73b5f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'approx' Acquisition Function run number = 20\n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_approx_20 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "approx_20 = GPGO(surrogate_approx_20, Acquisition(util_approx), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
        "approx_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_approx_20 = approx_20.getResult()[0]\n",
        "params_approx_20['max_depth'] = int(params_approx_20['max_depth'])\n",
        "params_approx_20['min_child_weight'] = int(params_approx_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_approx_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_approx_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_approx_20 = xgb.train(params_approx_20, dX_approx_train20)\n",
        "pred_approx_20 = model_approx_20.predict(dX_approx_test20)\n",
        "\n",
        "rmse_approx_20 = np.sqrt(mean_squared_error(pred_approx_20, y_test20))\n",
        "rmse_approx_20"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.88130801  8.97713728 14.          0.81074445  8.          0.95540649]. \t  -0.4485352768858121 \t -0.4485352768858121\n",
            "init   \t [6.72865655 0.41173329 8.         0.6361582  7.         0.76174061]. \t  -0.47208091450542966 \t -0.4485352768858121\n",
            "init   \t [ 4.77387703  8.66202323 10.          0.51833215  7.          0.10123387]. \t  -0.7316473600840852 \t -0.4485352768858121\n",
            "init   \t [ 5.75489985  4.74524381  8.          0.78084343 15.          0.26643049]. \t  -0.7314226542252507 \t -0.4485352768858121\n",
            "init   \t [ 4.53444     4.47342833  8.          0.91974896 18.          0.35997552]. \t  -0.6494230116583573 \t -0.4485352768858121\n",
            "1      \t [ 7.96566073  7.15509535  7.          0.79906691 11.          0.34132075]. \t  -0.6502222800629637 \t -0.4485352768858121\n",
            "2      \t [ 1.98667885  1.35773177 13.          0.57199118  2.          0.39498908]. \t  -0.6661970064789862 \t -0.4485352768858121\n",
            "3      \t [ 3.00704909  2.42524876 14.          0.95062509 15.          0.83595087]. \t  -0.4577111911042998 \t -0.4485352768858121\n",
            "4      \t [0.52053109 3.24156501 6.         0.62694491 9.         0.62944167]. \t  -0.5198752102757443 \t -0.4485352768858121\n",
            "5      \t [8.0846212  5.99993376 6.         0.83941375 1.         0.46362124]. \t  -0.5681524533474447 \t -0.4485352768858121\n",
            "6      \t [0.61316554 1.36115087 5.         0.87944956 1.         0.21740227]. \t  -0.7332228664111685 \t -0.4485352768858121\n",
            "7      \t [ 0.72788527  2.26655356 10.          0.97273032 15.          0.85843758]. \t  -0.45084600130879665 \t -0.4485352768858121\n",
            "8      \t [4.62702633 9.6876243  5.         0.9923406  1.         0.38132515]. \t  -0.6522172440468724 \t -0.4485352768858121\n",
            "9      \t [ 1.44692101  9.96202174 12.          0.76092884 18.          0.10523817]. \t  -0.731303133317551 \t -0.4485352768858121\n",
            "10     \t [ 8.07574107  9.26938534 14.          0.9315973  14.          0.78345994]. \t  -0.46040647481050156 \t -0.4485352768858121\n",
            "11     \t [ 9.05255372  2.37894322 12.          0.61307055 13.          0.78275789]. \t  -0.47723885637506297 \t -0.4485352768858121\n",
            "12     \t [ 9.62878188  0.47045758 13.          0.73408624  1.          0.43054124]. \t  -0.5746597113890792 \t -0.4485352768858121\n",
            "13     \t [ 7.68006659  9.23429763  8.          0.96940014 15.          0.65722021]. \t  -0.5065278873092665 \t -0.4485352768858121\n",
            "14     \t [ 7.47036597  0.87634126 13.          0.90366023 19.          0.17017133]. \t  -0.732072189688284 \t -0.4485352768858121\n",
            "15     \t [ 5.37444991  0.3056877   9.          0.99777328 15.          0.5903115 ]. \t  -0.5008636074552522 \t -0.4485352768858121\n",
            "16     \t [ 8.81579675  8.3667984  12.          0.69895873  2.          0.28897175]. \t  -0.656636562688776 \t -0.4485352768858121\n",
            "17     \t [ 0.          8.27513644 11.57681414  0.51060077  2.08354002  0.72017381]. \t  -0.4676006014176982 \t -0.4485352768858121\n",
            "18     \t [ 0.         10.          5.          1.         10.92944753  1.        ]. \t  -0.454293248500303 \t -0.4485352768858121\n",
            "19     \t [ 0.31013291  2.98527396 14.          0.72603658 10.          0.35868226]. \t  -0.6550415944110137 \t -0.4485352768858121\n",
            "20     \t [ 2.13718188  8.35527669 14.          0.98978746 13.          0.26405248]. \t  -0.7317225765444565 \t -0.4485352768858121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.454763173494824"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1d_1LyydIfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda54264-d5eb-4231-aeb7-a97c91e4b26b"
      },
      "source": [
        "end_approx = time.time()\n",
        "end_approx\n",
        "\n",
        "time_approx = end_approx - start_approx\n",
        "time_approx\n",
        "\n",
        "start_exact = time.time()\n",
        "start_exact"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1629368425.1950397"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyOw7XYVwAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae37395-326f-4486-938e-cc4b9e082489"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 1 \n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_exact_1 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_1 = dGPGO(surrogate_exact_1, Acquisition_new(util_exact), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_1 = exact_1.getResult()[0]\n",
        "params_exact_1['max_depth'] = int(params_exact_1['max_depth'])\n",
        "params_exact_1['min_child_weight'] = int(params_exact_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_exact_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_exact_1 = xgb.train(params_exact_1, dX_exact_train1)\n",
        "pred_exact_1 = model_exact_1.predict(dX_exact_test1)\n",
        "\n",
        "rmse_exact_1 = np.sqrt(mean_squared_error(pred_exact_1, y_test1))\n",
        "rmse_exact_1"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [5.48813504 7.15189366 8.         0.92897281 8.         0.48128932]. \t  -0.5626915313589341 \t -0.46143572360276275\n",
            "init   \t [ 6.45894113  4.37587211 11.          0.52835649 13.          0.44509737]. \t  -0.5772881365468763 \t -0.46143572360276275\n",
            "init   \t [ 7.91725038  5.2889492  13.          0.6963924  14.          0.40365654]. \t  -0.5870544636272766 \t -0.46143572360276275\n",
            "init   \t [ 6.48171872  3.6824154  10.          0.88907838 16.          0.88307853]. \t  -0.46143572360276275 \t -0.46143572360276275\n",
            "init   \t [4.73608045 8.00910752 8.         0.83943977 8.         0.67592892]. \t  -0.5051288806760134 \t -0.46143572360276275\n",
            "1      \t [ 0.96098408  9.76459465  7.          0.75481219 17.          0.64436097]. \t  -0.5149059967458438 \t -0.46143572360276275\n",
            "2      \t [ 5.13759733  2.22657933 12.          0.58106013  2.          0.92007745]. \t  -0.46868154430393166 \t -0.46143572360276275\n",
            "3      \t [9.58067178 9.65734278 7.         0.88193436 1.         0.38223155]. \t  -0.5888715285109799 \t -0.46143572360276275\n",
            "4      \t [ 0.90969339  9.80979401 14.          0.8665633   3.          0.55460209]. \t  -0.5632471108749298 \t -0.46143572360276275\n",
            "5      \t [0.3028841  4.069464   5.         0.51500749 1.         0.27359263]. \t  -0.6787735406244566 \t -0.46143572360276275\n",
            "6      \t [ 8.87166351  9.3367646   5.          0.58200219 19.          0.4055524 ]. \t  -0.6087566280502396 \t -0.46143572360276275\n",
            "7      \t [ 0.51228404  8.90605177 14.          0.7949699  11.          0.73913262]. \t  \u001b[92m-0.46104762612970374\u001b[0m \t -0.46104762612970374\n",
            "8      \t [ 2.05150398  0.53599727  5.          0.71551734 11.          0.37377971]. \t  -0.60261611749113 \t -0.46104762612970374\n",
            "9      \t [8.38797278 0.6003286  6.         0.6181346  1.         0.60370114]. \t  -0.526164863483156 \t -0.46104762612970374\n",
            "10     \t [ 9.06530919  9.40796401 14.          0.73452132  4.          0.22638202]. \t  -0.6737273666958219 \t -0.46104762612970374\n",
            "11     \t [ 2.92761431  0.02841708 13.          0.97950295  9.          0.31091424]. \t  -0.5822275926277118 \t -0.46104762612970374\n",
            "12     \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.4302853773034897\u001b[0m \t -0.4302853773034897\n",
            "13     \t [ 1.99180311  1.76156949  6.          0.52474573 18.          0.69397695]. \t  -0.5292680491699555 \t -0.4302853773034897\n",
            "14     \t [10.         10.         15.          1.         11.03652682  1.        ]. \t  \u001b[92m-0.42514855917982713\u001b[0m \t -0.42514855917982713\n",
            "15     \t [ 3.33998723  9.61997442 13.          0.87678219 17.          0.47112812]. \t  -0.562089749118206 \t -0.42514855917982713\n",
            "16     \t [ 0.89800437  1.82654173 13.          0.73602129 17.          0.1041576 ]. \t  -0.6740368890718571 \t -0.42514855917982713\n",
            "17     \t [ 7.96942817  9.76181769  7.          0.91046857 13.          0.32455735]. \t  -0.5870001390492389 \t -0.42514855917982713\n",
            "18     \t [0.45095418 9.96977628 8.         0.5330212  1.         0.30363698]. \t  -0.5984194428816372 \t -0.42514855917982713\n",
            "19     \t [ 8.55155756  0.36726655  8.          0.94666955 10.          0.23257137]. \t  -0.6706122625441684 \t -0.42514855917982713\n",
            "20     \t [3.69919707 0.85322265 6.         0.75904625 5.         0.97222422]. \t  -0.4807633478348844 \t -0.42514855917982713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.607787340872903"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDQbChpZ48F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ad1d70-2699-4fad-90ad-7658c1279167"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 2 \n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_exact_2 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_2 = dGPGO(surrogate_exact_2, Acquisition_new(util_exact), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_2 = exact_2.getResult()[0]\n",
        "params_exact_2['max_depth'] = int(params_exact_2['max_depth'])\n",
        "params_exact_2['min_child_weight'] = int(params_exact_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_exact_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_exact_2 = xgb.train(params_exact_2, dX_exact_train2)\n",
        "pred_exact_2 = model_exact_2.predict(dX_exact_test2)\n",
        "\n",
        "rmse_exact_2 = np.sqrt(mean_squared_error(pred_exact_2, y_test2))\n",
        "rmse_exact_2"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 4.35994902  0.25926232 11.          0.97386531 12.          0.47833102]. \t  -0.5147867600449748 \t -0.4765694615523879\n",
            "init   \t [ 3.30334821  2.04648634 10.          0.55997527  6.          0.71472339]. \t  -0.4765694615523879 \t -0.4765694615523879\n",
            "init   \t [ 4.9856117   5.86796978  8.          0.89266757 11.          0.59158659]. \t  -0.4946399399702889 \t -0.4765694615523879\n",
            "init   \t [ 4.07307832  1.76984624 13.          0.75262305  7.          0.35908193]. \t  -0.5884370816585467 \t -0.4765694615523879\n",
            "init   \t [ 1.16193318  1.81727038  9.          0.79837265 19.          0.29965165]. \t  -0.584982798458911 \t -0.4765694615523879\n",
            "1      \t [10.         10.         13.31650102  1.         20.          1.        ]. \t  \u001b[92m-0.3809881978047184\u001b[0m \t -0.3809881978047184\n",
            "2      \t [1.25559631 9.8394609  9.         0.52015567 2.         0.32958416]. \t  -0.5945813795974784 \t -0.3809881978047184\n",
            "3      \t [ 9.14946201  2.43697872  6.          0.997805   19.          0.45949208]. \t  -0.5305156688763031 \t -0.3809881978047184\n",
            "4      \t [9.94733027 1.06305306 6.         0.53557792 2.         0.26251983]. \t  -0.6747986207670527 \t -0.3809881978047184\n",
            "5      \t [ 9.49925654  7.97200083  8.          0.53193588 18.          0.45705681]. \t  -0.5269444956120974 \t -0.3809881978047184\n",
            "6      \t [ 9.61885664  9.78589131 14.          0.8903706   1.          0.14094172]. \t  -0.6776999663731892 \t -0.3809881978047184\n",
            "7      \t [ 0.05725091  9.56919109 14.          0.73355327 17.          0.65868771]. \t  -0.4840368839655496 \t -0.3809881978047184\n",
            "8      \t [7.36877801 8.87815651 5.         0.56972286 4.         0.80625064]. \t  -0.5008693406902159 \t -0.3809881978047184\n",
            "9      \t [ 5.76886466  6.30636441 11.          0.85075313  6.          0.94564556]. \t  -0.39766690308447894 \t -0.3809881978047184\n",
            "10     \t [ 9.532292    9.50049573 13.          0.6020865  12.          0.69858036]. \t  -0.4949335744560372 \t -0.3809881978047184\n",
            "11     \t [ 0.44071499  7.73764551  6.          0.95035779 15.          0.59534933]. \t  -0.5150284970246458 \t -0.3809881978047184\n",
            "12     \t [ 8.21799102  0.75840382 13.          0.74503318 19.          0.27131427]. \t  -0.6754902476470062 \t -0.3809881978047184\n",
            "13     \t [ 0.59751708  9.63364991 11.          0.90897061  9.          0.44731047]. \t  -0.5150962154652547 \t -0.3809881978047184\n",
            "14     \t [ 2.61078484  8.48438058 14.          0.79784401 12.          0.37480602]. \t  -0.5873563000820609 \t -0.3809881978047184\n",
            "15     \t [ 8.35914343  2.39349698 13.          0.9820231   1.          0.65048694]. \t  -0.4939446347578816 \t -0.3809881978047184\n",
            "16     \t [ 8.39253634  0.41975132  5.          0.53583299 10.          0.93462258]. \t  -0.4369912210415411 \t -0.3809881978047184\n",
            "17     \t [1.70838606 1.7073017  5.         0.64676695 1.         0.90384668]. \t  -0.4303260861905837 \t -0.3809881978047184\n",
            "18     \t [ 4.81917386  5.39014745 11.41344907  1.         16.41344907  1.        ]. \t  \u001b[92m-0.3777094511302258\u001b[0m \t -0.3777094511302258\n",
            "19     \t [10.          4.0050117  15.          1.          6.09854109  1.        ]. \t  \u001b[92m-0.3763198412022126\u001b[0m \t -0.3763198412022126\n",
            "20     \t [ 0.40164348  1.66439141 14.          0.96476321  1.          0.14148109]. \t  -0.6802555077917066 \t -0.3763198412022126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.556046943199982"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUPyXRfZ95Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c284ba90-718f-40d4-87a8-ae3a447a14b8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 3 \n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_exact_3 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_3 = dGPGO(surrogate_exact_3, Acquisition_new(util_exact), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_3 = exact_3.getResult()[0]\n",
        "params_exact_3['max_depth'] = int(params_exact_3['max_depth'])\n",
        "params_exact_3['min_child_weight'] = int(params_exact_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_exact_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_exact_3 = xgb.train(params_exact_3, dX_exact_train3)\n",
        "pred_exact_3 = model_exact_3.predict(dX_exact_test3)\n",
        "\n",
        "rmse_exact_3 = np.sqrt(mean_squared_error(pred_exact_3, y_test3))\n",
        "rmse_exact_3"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.50797903  7.08147823 13.          0.56066429 11.          0.11687321]. \t  -0.7165783188757435 \t -0.6409647951145182\n",
            "init   \t [ 0.40630737  2.47888297 11.          0.72040492 13.          0.23083313]. \t  -0.7204431346766296 \t -0.6409647951145182\n",
            "init   \t [ 4.53172301  2.15577008 11.          0.74631796  2.          0.60296868]. \t  -0.6409647951145182 \t -0.6409647951145182\n",
            "init   \t [ 2.59252447  4.15101197 13.          0.79330998  8.          0.24118096]. \t  -0.7214290072967551 \t -0.6409647951145182\n",
            "init   \t [ 5.44649018  7.80314765 10.          0.62879264 18.          0.44917413]. \t  -0.6558401549443297 \t -0.6409647951145182\n",
            "1      \t [4.88873245 9.27936348 6.         0.94344906 8.         0.25949204]. \t  -0.7188700813687741 \t -0.6409647951145182\n",
            "2      \t [ 8.93142368  1.52910591 13.          0.84039318 17.          0.60846833]. \t  \u001b[92m-0.6320347814043803\u001b[0m \t -0.6320347814043803\n",
            "3      \t [ 6.38594331  1.19109066  5.          0.81189053 13.          0.59164768]. \t  \u001b[92m-0.6289274956252369\u001b[0m \t -0.6289274956252369\n",
            "4      \t [ 1.02918863  9.32189805 13.          0.88333707  1.          0.86998588]. \t  \u001b[92m-0.45190108244647254\u001b[0m \t -0.45190108244647254\n",
            "5      \t [ 9.74929058  1.51205926 11.          0.50025602  8.          0.46132437]. \t  -0.6593087000926661 \t -0.45190108244647254\n",
            "6      \t [ 9.45052852  8.62641484  7.          0.79615518 14.          0.32790361]. \t  -0.708730491334048 \t -0.45190108244647254\n",
            "7      \t [ 8.92744991  9.09956287 12.          0.74944313  3.          0.11030352]. \t  -0.7194264138705575 \t -0.45190108244647254\n",
            "8      \t [0.28002919 1.86471402 5.         0.90893429 3.         0.44808833]. \t  -0.653236808211726 \t -0.45190108244647254\n",
            "9      \t [ 9.02893081  7.64399312 14.          0.72894099 15.          0.70609928]. \t  -0.6325306398208942 \t -0.45190108244647254\n",
            "10     \t [4.37264513 7.83931591 6.         0.65864599 1.         0.34090671]. \t  -0.7108079171350814 \t -0.45190108244647254\n",
            "11     \t [ 2.84857043  0.57472701  5.          0.53705172 19.          0.51858228]. \t  -0.6589951838647294 \t -0.45190108244647254\n",
            "12     \t [ 0.63346059  9.87550877  6.          0.74382195 14.          0.39340576]. \t  -0.7092906536332471 \t -0.45190108244647254\n",
            "13     \t [2.99254427 2.69228882 7.         0.63915731 9.         0.21678027]. \t  -0.7194910621859361 \t -0.45190108244647254\n",
            "14     \t [8.8237369  0.04240955 5.         0.7158964  1.         0.99950517]. \t  -0.48215926032694006 \t -0.45190108244647254\n",
            "15     \t [ 5.7586577   1.42812945 10.          0.74022438  8.          0.79053147]. \t  -0.5205954164321709 \t -0.45190108244647254\n",
            "16     \t [9.25339855 3.80341124 5.         0.92679998 9.         0.56391072]. \t  -0.6536743943093699 \t -0.45190108244647254\n",
            "17     \t [ 0.94336997  1.29293405 12.          0.62917786 19.          0.81056172]. \t  -0.5192284078227264 \t -0.45190108244647254\n",
            "18     \t [ 0.31516471  7.97295608 10.          0.77489537  9.          0.25855149]. \t  -0.7193805891172754 \t -0.45190108244647254\n",
            "19     \t [ 9.8542409   3.34207335 14.          0.75035548  3.          0.6035725 ]. \t  -0.6317346576420496 \t -0.45190108244647254\n",
            "20     \t [ 3.60954441  5.81173439  5.          0.60415615 17.          0.56274483]. \t  -0.6561590588771492 \t -0.45190108244647254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.649527586702978"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKX_nfEaaAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136b44a6-517e-4a8f-9542-0a3b7500d5a7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 4 \n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_exact_4 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_4 = dGPGO(surrogate_exact_4, Acquisition_new(util_exact), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_4 = exact_4.getResult()[0]\n",
        "params_exact_4['max_depth'] = int(params_exact_4['max_depth'])\n",
        "params_exact_4['min_child_weight'] = int(params_exact_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_exact_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_exact_4 = xgb.train(params_exact_4, dX_exact_train4)\n",
        "pred_exact_4 = model_exact_4.predict(dX_exact_test4)\n",
        "\n",
        "rmse_exact_4 = np.sqrt(mean_squared_error(pred_exact_4, y_test4))\n",
        "rmse_exact_4"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.67029839 5.47232249 6.         0.92781047 9.         0.72795594]. \t  -0.5993772224326677 \t -0.4983304913999733\n",
            "init   \t [ 2.16089496  9.76274455 12.          0.62649118  9.          0.66966679]. \t  -0.6069567430422909 \t -0.4983304913999733\n",
            "init   \t [ 0.05159149  5.72356491  9.          0.99170034 10.          0.10808749]. \t  -0.7139334307278753 \t -0.4983304913999733\n",
            "init   \t [ 3.86571283  0.44160058 10.          0.90553105 18.          0.95407958]. \t  -0.4983304913999733 \t -0.4983304913999733\n",
            "init   \t [ 7.86305986  8.66289299  6.          0.53285477 14.          0.25117497]. \t  -0.7091576633146701 \t -0.4983304913999733\n",
            "1      \t [ 8.45443649  8.61014312 11.          0.83475494  1.          0.14018305]. \t  -0.7189305559932541 \t -0.4983304913999733\n",
            "2      \t [ 0.77431146  1.96668116 12.          0.50723361  3.          0.74768925]. \t  -0.6140173026050956 \t -0.4983304913999733\n",
            "3      \t [2.27858743 6.23199766 5.         0.58705984 2.         0.80794289]. \t  -0.5986031600147258 \t -0.4983304913999733\n",
            "4      \t [ 6.832625    9.87635293 14.          0.84450885 19.          0.20389666]. \t  -0.7153744507380464 \t -0.4983304913999733\n",
            "5      \t [ 7.37255369  2.03491596 13.          0.8921741   9.          0.46934318]. \t  -0.625550615237801 \t -0.4983304913999733\n",
            "6      \t [ 0.05992751  6.06320143 14.          0.89322475 17.          0.32211096]. \t  -0.6540035079472759 \t -0.4983304913999733\n",
            "7      \t [ 0.79250634  6.36332745  6.          0.84703891 17.          0.8628914 ]. \t  -0.5176542387032834 \t -0.4983304913999733\n",
            "8      \t [9.26767626 0.09691703 5.         0.59554562 3.         0.95249041]. \t  -0.524958204283032 \t -0.4983304913999733\n",
            "9      \t [ 9.93824172  2.34876498  7.          0.94210707 16.          0.6947317 ]. \t  -0.6069344861897712 \t -0.4983304913999733\n",
            "10     \t [3.43076773 0.51115291 6.         0.80398076 7.         0.163561  ]. \t  -0.7152347423610873 \t -0.4983304913999733\n",
            "11     \t [8.12039932 9.82838311 5.         0.6851891  3.         0.87462757]. \t  -0.5209212303565759 \t -0.4983304913999733\n",
            "12     \t [ 6.03647398  5.02077859  7.          0.51964174 12.          0.90396407]. \t  -0.5142832795776289 \t -0.4983304913999733\n",
            "13     \t [ 9.03174101  2.34471053 13.          0.52042845  3.          0.20136175]. \t  -0.7105427027924398 \t -0.4983304913999733\n",
            "14     \t [10.         10.         13.57473549  1.          9.57473549  1.        ]. \t  \u001b[92m-0.4791672055007812\u001b[0m \t -0.4791672055007812\n",
            "15     \t [ 6.05918846  4.34302105 13.          0.69548235 15.          0.51487907]. \t  -0.6222861048031127 \t -0.4791672055007812\n",
            "16     \t [ 2.02212851  9.9407933  10.          0.71009222  3.          0.59925198]. \t  -0.6118231860074616 \t -0.4791672055007812\n",
            "17     \t [ 9.31090633  0.03922389 13.          0.89687279 18.          0.65712575]. \t  -0.6054491997792859 \t -0.4791672055007812\n",
            "18     \t [ 0.84701744  1.60683479 14.          0.52191349  9.          0.13284287]. \t  -0.7118778609540446 \t -0.4791672055007812\n",
            "19     \t [0.43927529 9.60376826 5.         0.58040652 9.         0.31646261]. \t  -0.6626762857296217 \t -0.4791672055007812\n",
            "20     \t [ 0.65787882  1.52536753  7.          0.582989   13.          0.8348692 ]. \t  -0.5926194249900745 \t -0.4791672055007812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.742748709233146"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJmI9saAaEG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae83493-88c7-4f60-ec68-96ab32dd4c98"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 5 \n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_exact_5 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_5 = dGPGO(surrogate_exact_5, Acquisition_new(util_exact), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_5 = exact_5.getResult()[0]\n",
        "params_exact_5['max_depth'] = int(params_exact_5['max_depth'])\n",
        "params_exact_5['min_child_weight'] = int(params_exact_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_exact_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_exact_5 = xgb.train(params_exact_5, dX_exact_train5)\n",
        "pred_exact_5 = model_exact_5.predict(dX_exact_test5)\n",
        "\n",
        "rmse_exact_5 = np.sqrt(mean_squared_error(pred_exact_5, y_test5))\n",
        "rmse_exact_5"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.21993171  8.70732306 11.          0.68186845 10.          0.53957007]. \t  -0.5323233521622429 \t -0.48683475961332984\n",
            "init   \t [ 6.11743863  7.65907856  5.          0.64840025 16.          0.82745351]. \t  -0.48683475961332984 \t -0.48683475961332984\n",
            "init   \t [ 6.49458883  8.19472793  6.          0.93996852 19.          0.36647194]. \t  -0.586475577548452 \t -0.48683475961332984\n",
            "init   \t [ 6.28787909  5.7983781   6.          0.63290956 17.          0.18402673]. \t  -0.633609225551848 \t -0.48683475961332984\n",
            "init   \t [8.26554249 8.33492742 9.         0.97900675 3.         0.26957319]. \t  -0.6326249738244012 \t -0.48683475961332984\n",
            "1      \t [1.95474956 1.21548467 5.         0.65548996 6.         0.3261206 ]. \t  -0.5882279171378358 \t -0.48683475961332984\n",
            "2      \t [ 8.68915106  0.84881749 13.          0.9945373   7.          0.34624572]. \t  -0.5633370634302679 \t -0.48683475961332984\n",
            "3      \t [ 6.12310163  2.21013771 14.          0.74740665 18.          0.42989776]. \t  -0.5333068626906653 \t -0.48683475961332984\n",
            "4      \t [ 9.64635884  9.52633265 14.          0.67611826 10.          0.87506739]. \t  \u001b[92m-0.4423357727617705\u001b[0m \t -0.4423357727617705\n",
            "5      \t [ 0.42801231  0.53056997 14.          0.88001393  3.          0.60870677]. \t  -0.4956565139336144 \t -0.4423357727617705\n",
            "6      \t [10.         10.         14.76488616  1.         20.          1.        ]. \t  \u001b[92m-0.4315329340187998\u001b[0m \t -0.4315329340187998\n",
            "7      \t [9.67414353 2.68949571 5.         0.62350468 9.         0.83649418]. \t  -0.4893925661153425 \t -0.4315329340187998\n",
            "8      \t [ 0.5259471   9.7567347  14.          0.91097263  1.          0.64281486]. \t  -0.49657614671134453 \t -0.4315329340187998\n",
            "9      \t [9.64880583 0.85455793 9.         0.80366346 1.         0.54097439]. \t  -0.5442200644862888 \t -0.4315329340187998\n",
            "10     \t [1.24717977 9.40645683 5.         0.65383928 3.         0.68852908]. \t  -0.5172211356155465 \t -0.4315329340187998\n",
            "11     \t [ 1.7633545   9.18687735 14.          0.62483521 19.          0.67316164]. \t  -0.4977308231633463 \t -0.4315329340187998\n",
            "12     \t [ 1.13915888  1.42227157  5.          0.93948287 15.          0.24386626]. \t  -0.6356899457971215 \t -0.4315329340187998\n",
            "13     \t [ 1.33802527  0.56317443 13.          0.62013582 12.          0.977958  ]. \t  -0.4363186570405474 \t -0.4315329340187998\n",
            "14     \t [ 2.36281749  5.54578862 10.          0.61695313  4.          0.17816121]. \t  -0.6325214388686458 \t -0.4315329340187998\n",
            "15     \t [9.30969345 8.84356664 5.         0.89369445 8.         0.11148942]. \t  -0.6362192215439146 \t -0.4315329340187998\n",
            "16     \t [ 9.7108222   4.76509396 12.          0.6344947  14.          0.80559269]. \t  -0.4759315199959574 \t -0.4315329340187998\n",
            "17     \t [ 6.80615583  6.45556443 14.          0.61036314  1.          0.8555849 ]. \t  -0.4679937193324773 \t -0.4315329340187998\n",
            "18     \t [ 0.39655743  9.88537948  7.          0.8395494  15.          0.83639512]. \t  -0.46556996770739073 \t -0.4315329340187998\n",
            "19     \t [ 0.04353364  3.80292085 10.          0.99695718 18.          0.799674  ]. \t  -0.4560426797696765 \t -0.4315329340187998\n",
            "20     \t [ 9.94984248  2.37676311  9.          0.96621815 19.          0.48539202]. \t  -0.531856562647136 \t -0.4315329340187998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.742742703974025"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulhEolsxaG4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711f5d5d-0af8-4af2-f739-e989d4e08f96"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 6 \n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_exact_6 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror', eval_metric = 'rmse')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_6 = dGPGO(surrogate_exact_6, Acquisition_new(util_exact), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_6 = exact_6.getResult()[0]\n",
        "params_exact_6['max_depth'] = int(params_exact_6['max_depth'])\n",
        "params_exact_6['min_child_weight'] = int(params_exact_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_exact_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_exact_6 = xgb.train(params_exact_6, dX_exact_train6)\n",
        "pred_exact_6 = model_exact_6.predict(dX_exact_test6)\n",
        "\n",
        "rmse_exact_6 = np.sqrt(mean_squared_error(pred_exact_6, y_test6))\n",
        "rmse_exact_6"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5719256944003751 \t -0.5405445954433028\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.6047098118480896 \t -0.5405445954433028\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5405445954433028 \t -0.5405445954433028\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5678490550489279 \t -0.5405445954433028\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6145131146454834 \t -0.5405445954433028\n",
            "1      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.45961383649052767\u001b[0m \t -0.45961383649052767\n",
            "2      \t [ 9.24343066  2.50578958  7.          0.73023742 13.          0.6940436 ]. \t  -0.5767008292506094 \t -0.45961383649052767\n",
            "3      \t [8.18334854 9.97104849 8.         0.91869016 8.         0.37558961]. \t  -0.6144590591119746 \t -0.45961383649052767\n",
            "4      \t [ 2.04733214  5.62395529 10.          0.79098076 10.          0.19218812]. \t  -0.6920160120895364 \t -0.45961383649052767\n",
            "5      \t [ 1.35461816  3.68867636  7.          0.97358458 19.          0.99760691]. \t  -0.4813823942223731 \t -0.45961383649052767\n",
            "6      \t [ 9.79809506  4.05954978 14.          0.57601486  3.          0.61048927]. \t  -0.5747329707889864 \t -0.45961383649052767\n",
            "7      \t [ 1.7609747   9.34733292 14.          0.94873694  3.          0.31587639]. \t  -0.6202575508979221 \t -0.45961383649052767\n",
            "8      \t [ 9.32420466  6.39616005 13.          0.93300527 17.          0.34904443]. \t  -0.618513635024325 \t -0.45961383649052767\n",
            "9      \t [ 8.18088231  9.44781209  7.          0.95869221 19.          0.77401414]. \t  -0.5389010350952639 \t -0.45961383649052767\n",
            "10     \t [ 1.77214312  9.25420942  5.          0.67440912 15.          0.38002389]. \t  -0.6272073900971591 \t -0.45961383649052767\n",
            "11     \t [1.74386723 0.44633377 5.         0.62204812 9.         0.13189777]. \t  -0.6931813502859878 \t -0.45961383649052767\n",
            "12     \t [ 2.79340377  7.98121034 11.78897397  1.         20.          1.        ]. \t  \u001b[92m-0.459161351790203\u001b[0m \t -0.459161351790203\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5742354992220393 \t -0.459161351790203\n",
            "14     \t [ 0.47139237  0.24747681 14.          0.53291734 14.          0.24490793]. \t  -0.6947949784703452 \t -0.459161351790203\n",
            "15     \t [0.34300109 9.68403349 8.         0.54781067 3.         0.98074554]. \t  -0.483546606850085 \t -0.459161351790203\n",
            "16     \t [ 8.38603255  0.66031556 14.          0.89601822  8.          0.63597949]. \t  -0.5563153105609959 \t -0.459161351790203\n",
            "17     \t [ 8.4425376  10.         14.79626801  1.          6.79626801  1.        ]. \t  \u001b[92m-0.4526708734791033\u001b[0m \t -0.4526708734791033\n",
            "18     \t [ 0.1424991   9.83224829 14.          0.82001413 10.          0.36444993]. \t  -0.6201751120416013 \t -0.4526708734791033\n",
            "19     \t [ 8.21396627  2.65706382  6.          0.52082555 19.          0.80555848]. \t  -0.5578395133551297 \t -0.4526708734791033\n",
            "20     \t [ 0.99977857  3.51872667 14.          0.87616868  4.          0.76711714]. \t  -0.5299939150665656 \t -0.4526708734791033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.565813167045083"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYebx3RVaJ1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b3833e-e55e-47fd-fb19-0305f0b8bea9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 7 \n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_exact_7 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_7 = dGPGO(surrogate_exact_7, Acquisition_new(util_exact), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_7 = exact_7.getResult()[0]\n",
        "params_exact_7['max_depth'] = int(params_exact_7['max_depth'])\n",
        "params_exact_7['min_child_weight'] = int(params_exact_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_exact_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_exact_7 = xgb.train(params_exact_7, dX_exact_train7)\n",
        "pred_exact_7 = model_exact_7.predict(dX_exact_test7)\n",
        "\n",
        "rmse_exact_7 = np.sqrt(mean_squared_error(pred_exact_7, y_test7))\n",
        "rmse_exact_7"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.76308289 7.79918792 8.         0.98911145 8.         0.98019056]. \t  -0.44500885348659536 \t -0.44173641078261416\n",
            "init   \t [ 5.3849587   5.01120464 13.          0.74994125  5.          0.88192131]. \t  -0.4488374676936292 \t -0.44173641078261416\n",
            "init   \t [ 3.30839249  3.9294231  12.          0.6440728  13.          0.41137564]. \t  -0.5669653799025498 \t -0.44173641078261416\n",
            "init   \t [9.29528191 2.6258377  5.         0.80027446 1.         0.86616513]. \t  -0.4661284696195417 \t -0.44173641078261416\n",
            "init   \t [ 1.74052764  7.90763512 14.          0.7244129   4.          0.77536887]. \t  -0.44173641078261416 \t -0.44173641078261416\n",
            "1      \t [3.43305102 3.00339076 8.         0.71322679 4.         0.33322219]. \t  -0.5720117286339644 \t -0.44173641078261416\n",
            "2      \t [ 7.6343627   1.31181598  5.          0.5769645  12.          0.84874959]. \t  -0.48924755963876193 \t -0.44173641078261416\n",
            "3      \t [ 9.12127254  9.64651695 14.          0.53624962  1.          0.38247449]. \t  -0.5784219709012162 \t -0.44173641078261416\n",
            "4      \t [ 4.51243396  9.79601217  8.          0.69773915 19.          0.69687222]. \t  -0.48670271968935647 \t -0.44173641078261416\n",
            "5      \t [ 8.06748781  9.6311716   5.          0.89165168 11.          0.90769253]. \t  -0.47467849504450826 \t -0.44173641078261416\n",
            "6      \t [ 9.84853722  9.76587477 12.          0.66808012 15.          0.83895675]. \t  -0.464300180546447 \t -0.44173641078261416\n",
            "7      \t [6.4915356  8.69600226 5.         0.66481282 2.         0.54976375]. \t  -0.5200045453863744 \t -0.44173641078261416\n",
            "8      \t [ 0.86712134  2.53401598  5.          0.75823741 18.          0.7884932 ]. \t  -0.4803186810834383 \t -0.44173641078261416\n",
            "9      \t [ 1.29932493  8.0055142  14.          0.51325501 19.          0.83654101]. \t  -0.4650670910068424 \t -0.44173641078261416\n",
            "10     \t [ 8.667653    2.8973594  14.          0.53825633 18.          0.10643456]. \t  -0.6861760180900373 \t -0.44173641078261416\n",
            "11     \t [ 9.89504759  2.29066357 10.          0.72562459  8.          0.65324681]. \t  -0.4791774660247169 \t -0.44173641078261416\n",
            "12     \t [ 9.94824081  0.39149062 12.          0.59113076  1.          0.49709751]. \t  -0.5081268521688408 \t -0.44173641078261416\n",
            "13     \t [ 9.60964399  8.52338044  5.          0.92796141 17.          0.69277707]. \t  -0.49365990657681785 \t -0.44173641078261416\n",
            "14     \t [ 3.34596156  0.67230605 11.          0.7366642  19.          0.29949997]. \t  -0.5628067831028366 \t -0.44173641078261416\n",
            "15     \t [ 9.60816545  8.33912452 11.          0.58307044  8.          0.36972424]. \t  -0.5786370523858145 \t -0.44173641078261416\n",
            "16     \t [ 0.47345619  1.31956961  7.          0.68987164 11.          0.9514247 ]. \t  -0.466265051270997 \t -0.44173641078261416\n",
            "17     \t [ 8.96081021  0.38413512 14.          0.90624871 12.          0.41560009]. \t  -0.565007973216504 \t -0.44173641078261416\n",
            "18     \t [ 9.71411962  1.71380043  5.          0.50726337 19.          0.71982498]. \t  -0.4912391351239423 \t -0.44173641078261416\n",
            "19     \t [ 0.18764716  0.58029521 13.          0.75995865  6.          0.69134836]. \t  -0.46855437085127594 \t -0.44173641078261416\n",
            "20     \t [ 1.77048754  9.0838881   6.          0.98375355 14.          0.13025417]. \t  -0.6815881776266557 \t -0.44173641078261416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.499924380514474"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0IPTSTbIl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05ce424-cf9c-4833-b36f-7ea66f31c2c3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 8 \n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_exact_8 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_8 = dGPGO(surrogate_exact_8, Acquisition_new(util_exact), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_8 = exact_8.getResult()[0]\n",
        "params_exact_8['max_depth'] = int(params_exact_8['max_depth'])\n",
        "params_exact_8['min_child_weight'] = int(params_exact_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_exact_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_exact_8 = xgb.train(params_exact_8, dX_exact_train8)\n",
        "pred_exact_8 = model_exact_8.predict(dX_exact_test8)\n",
        "\n",
        "rmse_exact_8 = np.sqrt(mean_squared_error(pred_exact_8, y_test8))\n",
        "rmse_exact_8"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.5450023023990902 \t -0.47785117417083445\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.48415390639601685 \t -0.47785117417083445\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5514374023096014 \t -0.47785117417083445\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.509390123714371 \t -0.47785117417083445\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.47785117417083445 \t -0.47785117417083445\n",
            "1      \t [4.42022545 5.48487111 9.         0.97165909 3.         0.63617522]. \t  -0.4933129789895931 \t -0.47785117417083445\n",
            "2      \t [ 9.3432851   3.80536023 13.          0.82203895 19.          0.99569116]. \t  -0.4825289719254064 \t -0.47785117417083445\n",
            "3      \t [ 2.52429836  9.02824683 14.          0.59641093 17.          0.61934886]. \t  -0.5047477495526641 \t -0.47785117417083445\n",
            "4      \t [ 9.53473907  5.08424998 11.          0.50652828 18.          0.67121466]. \t  -0.5164144636241208 \t -0.47785117417083445\n",
            "5      \t [6.89072012 1.88822945 5.         0.9252956  8.         0.40577637]. \t  -0.5624845374478475 \t -0.47785117417083445\n",
            "6      \t [ 2.42575645  9.87357367  5.          0.61143882 19.          0.11833201]. \t  -0.6358778400175291 \t -0.47785117417083445\n",
            "7      \t [ 0.9931658   1.40870497 14.          0.50614818  6.          0.71944448]. \t  -0.48485914970894306 \t -0.47785117417083445\n",
            "8      \t [ 9.09148899  1.42093493  5.          0.88801001 17.          0.53231573]. \t  -0.5546570042431702 \t -0.47785117417083445\n",
            "9      \t [9.69554908 3.70013633 5.         0.72727157 1.         0.49530336]. \t  -0.5595421515943506 \t -0.47785117417083445\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.558869771245836 \t -0.47785117417083445\n",
            "11     \t [1.66316873 0.47469495 7.         0.60820298 1.         0.50689458]. \t  -0.5436576355709689 \t -0.47785117417083445\n",
            "12     \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.46870913378803636\u001b[0m \t -0.46870913378803636\n",
            "13     \t [5.63420638 9.7026496  5.         0.64869539 8.         0.22475566]. \t  -0.6389332388103492 \t -0.46870913378803636\n",
            "14     \t [ 0.24957112  3.26102415  5.          0.59065773 17.          0.91920206]. \t  -0.49962809668085056 \t -0.46870913378803636\n",
            "15     \t [ 1.12264609  9.56425304 13.          0.79308235  1.          0.93526753]. \t  -0.4740633204616465 \t -0.46870913378803636\n",
            "16     \t [10.         10.         15.          1.         12.51199706  1.        ]. \t  \u001b[92m-0.4678615693129947\u001b[0m \t -0.4678615693129947\n",
            "17     \t [ 9.3183355   3.29262794 14.          0.86350712  1.          0.18562794]. \t  -0.6359402471994396 \t -0.4678615693129947\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.6380286876071068 \t -0.4678615693129947\n",
            "19     \t [ 7.2771035   6.97077181  6.          0.96280365 18.          0.64814702]. \t  -0.5204979904417385 \t -0.4678615693129947\n",
            "20     \t [ 6.68744311  8.81728204 13.          0.79205386  1.          0.27645475]. \t  -0.6373135148611248 \t -0.4678615693129947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.487971093090443"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UroEj_RbLSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c20c6b4-6e5d-45d9-8ff0-be29fc62ed99"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 9 \n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_exact_9 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_9 = dGPGO(surrogate_exact_9, Acquisition_new(util_exact), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_9 = exact_9.getResult()[0]\n",
        "params_exact_9['max_depth'] = int(params_exact_9['max_depth'])\n",
        "params_exact_9['min_child_weight'] = int(params_exact_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_exact_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_exact_9 = xgb.train(params_exact_9, dX_exact_train9)\n",
        "pred_exact_9 = model_exact_9.predict(dX_exact_test9)\n",
        "\n",
        "rmse_exact_9 = np.sqrt(mean_squared_error(pred_exact_9, y_test9))\n",
        "rmse_exact_9"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.10374154  5.01874592 11.          0.50377155  2.          0.29670281]. \t  -0.6545930802207814 \t -0.4584168030068045\n",
            "init   \t [ 4.18508181  2.48101168 13.          0.69794293  2.          0.25009871]. \t  -0.7166132091943936 \t -0.4584168030068045\n",
            "init   \t [ 8.78559086  9.50964032 13.          0.98395204 11.          0.90820641]. \t  -0.4584168030068045 \t -0.4584168030068045\n",
            "init   \t [ 6.66898973  5.47837783  6.          0.97165345 12.          0.72499481]. \t  -0.48839211091816903 \t -0.4584168030068045\n",
            "init   \t [ 8.24870465  4.65668475 13.          0.68760467  9.          0.98502332]. \t  -0.46354466019784824 \t -0.4584168030068045\n",
            "1      \t [6.73714319 2.39608167 5.         0.58130302 3.         0.163077  ]. \t  -0.7145926373770018 \t -0.4584168030068045\n",
            "2      \t [ 4.24955662  9.67331527 12.          0.64012695  7.          0.96617478]. \t  -0.4599552738922936 \t -0.4584168030068045\n",
            "3      \t [ 3.60566534  9.79805332 11.          0.62032576 16.          0.3578496 ]. \t  -0.6301914521165797 \t -0.4584168030068045\n",
            "4      \t [ 4.86601509  0.61279594  8.          0.72162785 18.          0.68911833]. \t  -0.4897803511428914 \t -0.4584168030068045\n",
            "5      \t [ 0.42678797  0.40430921 14.          0.96202194 10.          0.10119674]. \t  -0.7201378581044144 \t -0.4584168030068045\n",
            "6      \t [1.13863488 5.86180669 5.         0.9953054  3.         0.88639082]. \t  -0.4613511892865009 \t -0.4584168030068045\n",
            "7      \t [9.20185355 9.40235017 8.         0.60433558 1.         0.94540222]. \t  -0.4678077722812364 \t -0.4584168030068045\n",
            "8      \t [ 7.07313313  8.94084339  5.          0.99439529 19.          0.33798274]. \t  -0.6320298567588886 \t -0.4584168030068045\n",
            "9      \t [ 0.32747031  4.15373137 11.          0.86073055 16.          0.59344779]. \t  -0.4847822875702831 \t -0.4584168030068045\n",
            "10     \t [ 9.89680111  6.16985579 13.          0.58428363 17.          0.77251451]. \t  -0.4901785694142925 \t -0.4584168030068045\n",
            "11     \t [ 0.89628785  3.68529458  7.          0.8923295  10.          0.50355901]. \t  -0.4926976357635584 \t -0.4584168030068045\n",
            "12     \t [ 8.88256183  1.86768316 10.          0.88865081 13.          0.70876109]. \t  -0.4907324671255294 \t -0.4584168030068045\n",
            "13     \t [ 4.04453993  0.29784113 14.          0.62598795 17.          0.38851381]. \t  -0.6317392704296504 \t -0.4584168030068045\n",
            "14     \t [ 0.80844375  8.9121894   5.          0.56134557 18.          0.84901701]. \t  -0.496220520490058 \t -0.4584168030068045\n",
            "15     \t [4.62485155 9.65182222 5.         0.58896427 8.         0.36022331]. \t  -0.6322698857670199 \t -0.4584168030068045\n",
            "16     \t [ 8.77945628  5.51694736 12.          0.84613589  4.          0.64741755]. \t  -0.4798840189273464 \t -0.4584168030068045\n",
            "17     \t [ 3.89448326  9.77793126 11.          0.87312755  1.          0.25420157]. \t  -0.7204829959803113 \t -0.4584168030068045\n",
            "18     \t [9.96461831 1.05025842 5.         0.85654396 9.         0.88077949]. \t  -0.47633394276149926 \t -0.4584168030068045\n",
            "19     \t [ 2.87706677  6.16957719 14.          0.55246385 12.          0.92866915]. \t  -0.4653617556003965 \t -0.4584168030068045\n",
            "20     \t [ 2.2888102   0.47311819 10.          0.52676226  6.          0.86310876]. \t  -0.46328602169921157 \t -0.4584168030068045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6575353004618085"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VgaJOoJbOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6cc2b8-9870-4dd8-aa40-8410b530aa3f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 10 \n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_exact_10 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_10 = dGPGO(surrogate_exact_10, Acquisition_new(util_exact), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_10 = exact_10.getResult()[0]\n",
        "params_exact_10['max_depth'] = int(params_exact_10['max_depth'])\n",
        "params_exact_10['min_child_weight'] = int(params_exact_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_exact_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_exact_10 = xgb.train(params_exact_10, dX_exact_train10)\n",
        "pred_exact_10 = model_exact_10.predict(dX_exact_test10)\n",
        "\n",
        "rmse_exact_10 = np.sqrt(mean_squared_error(pred_exact_10, y_test10))\n",
        "rmse_exact_10"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.71320643  0.20751949  5.          0.72150747 17.          0.12265456]. \t  -0.7090674967614334 \t -0.4737745634473992\n",
            "init   \t [ 7.0920801   2.65566127 13.          0.57518893 17.          0.83494165]. \t  -0.4737745634473992 \t -0.4737745634473992\n",
            "init   \t [ 3.36071584  8.90816531  6.          0.86087766 15.          0.75469196]. \t  -0.4755277191484213 \t -0.4737745634473992\n",
            "init   \t [ 5.40880931  1.31458152  8.          0.57108502 14.          0.62551123]. \t  -0.48811859212530173 \t -0.4737745634473992\n",
            "init   \t [1.82631436 8.26082248 6.         0.80888349 5.         0.15900694]. \t  -0.7057210222477256 \t -0.4737745634473992\n",
            "1      \t [8.31989768 3.09778055 7.         0.64798085 3.         0.98471878]. \t  \u001b[92m-0.46336171949490257\u001b[0m \t -0.46336171949490257\n",
            "2      \t [ 1.51483713  6.46720195 14.          0.87676044  8.          0.10934204]. \t  -0.7062921047588426 \t -0.46336171949490257\n",
            "3      \t [ 0.44494294  2.20797313 10.          0.76097539  2.          0.34290111]. \t  -0.6180922534763184 \t -0.46336171949490257\n",
            "4      \t [ 6.47425096  8.4482791  10.          0.69239539 11.          0.47622913]. \t  -0.5685254207537167 \t -0.46336171949490257\n",
            "5      \t [ 0.20896963  7.17600684 13.          0.63836859 16.          0.77244006]. \t  \u001b[92m-0.45403228727774253\u001b[0m \t -0.45403228727774253\n",
            "6      \t [ 9.38854854  7.91087361  8.          0.57475286 19.          0.33969987]. \t  -0.6147666257838221 \t -0.45403228727774253\n",
            "7      \t [ 9.16520307  0.72602801 12.          0.91999471  9.          0.54336218]. \t  -0.5572402945816037 \t -0.45403228727774253\n",
            "8      \t [ 8.33810851  9.8990204  14.          0.61893039  5.          0.69227045]. \t  -0.4851208634097359 \t -0.45403228727774253\n",
            "9      \t [ 0.12250572  0.33829451 12.          0.86512188 11.          0.46912604]. \t  -0.5583881911050124 \t -0.45403228727774253\n",
            "10     \t [ 0.8556149   1.34495008  5.          0.88655293 18.          0.15072152]. \t  -0.7048793212420641 \t -0.45403228727774253\n",
            "11     \t [9.53733075 9.92797623 5.         0.78642342 2.         0.9933049 ]. \t  -0.4656240229085077 \t -0.45403228727774253\n",
            "12     \t [1.69575137 2.28628662 5.         0.76046389 8.         0.60065268]. \t  -0.5134648983420089 \t -0.45403228727774253\n",
            "13     \t [ 6.15733989  0.33396443 13.          0.69796716  1.          0.92875212]. \t  \u001b[92m-0.4397628730770955\u001b[0m \t -0.4397628730770955\n",
            "14     \t [ 9.08727548  7.95071568  5.          0.91034544 13.          0.55647706]. \t  -0.5735578706821028 \t -0.4397628730770955\n",
            "15     \t [ 1.45996834  0.59889716 12.          0.82815203 19.          0.24922559]. \t  -0.7069273813967614 \t -0.4397628730770955\n",
            "16     \t [ 9.42421731  1.45800675  5.          0.89810349 11.          0.91467377]. \t  -0.4712218735320602 \t -0.4397628730770955\n",
            "17     \t [ 2.80210285  9.24943163 14.          0.85818339  2.          0.85428456]. \t  \u001b[92m-0.4387844304056118\u001b[0m \t -0.4387844304056118\n",
            "18     \t [7.60796195 7.8170794  5.         0.85145938 8.         0.57832563]. \t  -0.5106716993417335 \t -0.4387844304056118\n",
            "19     \t [ 8.10151611  9.78827602 14.          0.68048433 19.          0.19392154]. \t  -0.7116333725331636 \t -0.4387844304056118\n",
            "20     \t [ 1.11903492  9.93958695  9.          0.79898321 10.          0.44087381]. \t  -0.5625838427005065 \t -0.4387844304056118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.521324504618526"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51z87uHWbRGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412841ab-76a3-4463-f8cd-1768bbb0e791"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 11 \n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_exact_11 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_11 = dGPGO(surrogate_exact_11, Acquisition_new(util_exact), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_11 = exact_11.getResult()[0]\n",
        "params_exact_11['max_depth'] = int(params_exact_11['max_depth'])\n",
        "params_exact_11['min_child_weight'] = int(params_exact_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_exact_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_exact_11 = xgb.train(params_exact_11, dX_exact_train11)\n",
        "pred_exact_11 = model_exact_11.predict(dX_exact_test11)\n",
        "\n",
        "rmse_exact_11 = np.sqrt(mean_squared_error(pred_exact_11, y_test11))\n",
        "rmse_exact_11"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.80269689  0.19475241  6.          0.59705781 13.          0.47818324]. \t  -0.5922349044250168 \t -0.49898623219170346\n",
            "init   \t [ 4.85427098  0.12780815  5.          0.91309068 14.          0.86571558]. \t  -0.49898623219170346 \t -0.49898623219170346\n",
            "init   \t [ 7.2996447   1.08736072 10.          0.92857712 18.          0.66910061]. \t  -0.5404544349803458 \t -0.49898623219170346\n",
            "init   \t [ 0.20483613  1.16737269  7.          0.57895615 16.          0.83644782]. \t  -0.5108833748715963 \t -0.49898623219170346\n",
            "init   \t [ 3.44624491  3.18798797 14.          0.54197657 15.          0.63958906]. \t  -0.5545314939891337 \t -0.49898623219170346\n",
            "1      \t [9.77136617 6.6548802  7.         0.51036649 9.         0.81011527]. \t  -0.5252491147925742 \t -0.49898623219170346\n",
            "2      \t [0.5279662  8.15331655 5.         0.83127487 9.         0.53242685]. \t  -0.5931601503405173 \t -0.49898623219170346\n",
            "3      \t [8.62555756 1.5478147  8.         0.99964468 2.         0.74874718]. \t  \u001b[92m-0.490360342610192\u001b[0m \t -0.490360342610192\n",
            "4      \t [ 0.90299561  9.42808632 14.          0.71344248  9.          0.5250902 ]. \t  -0.5778138976577691 \t -0.490360342610192\n",
            "5      \t [ 5.37271973  6.74878506 12.          0.69507758  3.          0.34109729]. \t  -0.5771395586534209 \t -0.490360342610192\n",
            "6      \t [10.         10.         14.84107387  1.          8.84107387  1.        ]. \t  \u001b[92m-0.4589029793726005\u001b[0m \t -0.4589029793726005\n",
            "7      \t [ 5.0402397   9.42026535  8.          0.71577    16.          0.52045581]. \t  -0.5873554197790632 \t -0.4589029793726005\n",
            "8      \t [10.          7.25651488 15.          1.         20.          1.        ]. \t  -0.46481753339047127 \t -0.4589029793726005\n",
            "9      \t [0.71954842 4.00816482 5.         0.76183414 1.         0.87022597]. \t  -0.49940652224028204 \t -0.4589029793726005\n",
            "10     \t [ 8.55410258  0.21556026 13.          0.58321534  8.          0.98283741]. \t  -0.4890790617970084 \t -0.4589029793726005\n",
            "11     \t [ 8.61765552  0.94349031 10.          0.85641829  4.          0.91576259]. \t  -0.47852786064513103 \t -0.4589029793726005\n",
            "12     \t [ 9.84847079  6.56311229  5.          0.73494874 19.          0.14302895]. \t  -0.6907115385769984 \t -0.4589029793726005\n",
            "13     \t [5.71344477 9.31745465 5.         0.62710446 1.         0.57350664]. \t  -0.56563042311184 \t -0.4589029793726005\n",
            "14     \t [ 8.40730244  5.3235921  14.          0.57824885  9.          0.96689527]. \t  -0.4867986869106529 \t -0.4589029793726005\n",
            "15     \t [ 0.23352034  0.69446734 12.          0.5389699   3.          0.65080983]. \t  -0.5571304657462128 \t -0.4589029793726005\n",
            "16     \t [ 3.71272268  4.57338191 11.          0.7134411  10.          0.83979859]. \t  -0.4880121906758131 \t -0.4589029793726005\n",
            "17     \t [ 0.80632707  5.39920626 11.          0.71823589 19.          0.37976587]. \t  -0.5836273896307068 \t -0.4589029793726005\n",
            "18     \t [ 6.19006075  3.41925871 11.          0.95286331  6.          0.39133224]. \t  -0.5745107435758626 \t -0.4589029793726005\n",
            "19     \t [0.77256475 1.22732493 7.         0.97692145 7.         0.9071396 ]. \t  -0.4728689341077363 \t -0.4589029793726005\n",
            "20     \t [7.46566309 0.73949694 7.         0.58224414 9.         0.25972328]. \t  -0.6904564119816002 \t -0.4589029793726005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.679812501694444"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8jZUeoWbTvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4faaa9d0-a666-4760-c8aa-d2f2f6fc3a73"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_exact_12 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_12 = dGPGO(surrogate_exact_12, Acquisition_new(util_exact), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_12 = exact_12.getResult()[0]\n",
        "params_exact_12['max_depth'] = int(params_exact_12['max_depth'])\n",
        "params_exact_12['min_child_weight'] = int(params_exact_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_exact_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_exact_12 = xgb.train(params_exact_12, dX_exact_train12)\n",
        "pred_exact_12 = model_exact_12.predict(dX_exact_test12)\n",
        "\n",
        "rmse_exact_12 = np.sqrt(mean_squared_error(pred_exact_12, y_test12))\n",
        "rmse_exact_12"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.54162842 7.40049697 6.         0.54321714 4.         0.11311747]. \t  -0.6840535846029854 \t -0.5032799564384677\n",
            "init   \t [ 9.18747008  9.00714854 14.          0.97847467 11.          0.35544552]. \t  -0.6305456734924068 \t -0.5032799564384677\n",
            "init   \t [ 6.06083184  9.44225136 14.          0.95626942  5.          0.56910342]. \t  -0.6144010633484512 \t -0.5032799564384677\n",
            "init   \t [ 5.52037633  4.85377414  7.          0.97886436 17.          0.78810441]. \t  -0.5032799564384677 \t -0.5032799564384677\n",
            "init   \t [ 0.20809798  1.35210178  5.          0.65494879 16.          0.36062811]. \t  -0.6516977337723864 \t -0.5032799564384677\n",
            "1      \t [9.46555822 8.57190559 5.         0.50164398 5.         0.71992807]. \t  -0.5227768283895782 \t -0.5032799564384677\n",
            "2      \t [9.04256367 2.61736915 8.         0.66026854 8.         0.14510453]. \t  -0.6776847411315782 \t -0.5032799564384677\n",
            "3      \t [6.03751892 2.08855857 8.         0.88966175 1.         0.6215545 ]. \t  -0.537741041779077 \t -0.5032799564384677\n",
            "4      \t [ 0.24796255  2.18203944 14.          0.56497025 17.          0.63132662]. \t  -0.5478949507371567 \t -0.5032799564384677\n",
            "5      \t [ 1.93384153  7.13950146  8.          0.85480597 18.          0.33734734]. \t  -0.6395317281169034 \t -0.5032799564384677\n",
            "6      \t [ 0.40359854  2.22527636 10.          0.60258213 10.          0.38255809]. \t  -0.6389275427144069 \t -0.5032799564384677\n",
            "7      \t [ 0.28427394  4.67296732 14.          0.84909523  4.          0.99176009]. \t  \u001b[92m-0.46369522867609564\u001b[0m \t -0.46369522867609564\n",
            "8      \t [ 7.63658847  0.39719075 14.          0.96199388 14.          0.84093877]. \t  -0.48258324270646796 \t -0.46369522867609564\n",
            "9      \t [ 7.0436136   8.68562161 15.          1.         19.35798404  1.        ]. \t  \u001b[92m-0.46093842220682946\u001b[0m \t -0.46093842220682946\n",
            "10     \t [ 4.27921374  9.2199845   6.          0.67076861 12.          0.56605459]. \t  -0.628155564863435 \t -0.46093842220682946\n",
            "11     \t [ 7.63578483  4.07501457 14.          0.97723751  1.          0.2033266 ]. \t  -0.6773954967709924 \t -0.46093842220682946\n",
            "12     \t [ 1.73522493  9.43858502 13.          0.55982395 15.          0.59746886]. \t  -0.5473582496522056 \t -0.46093842220682946\n",
            "13     \t [ 3.95801979  3.60127695  5.          0.58247512 11.          0.33687332]. \t  -0.6564398452227902 \t -0.46093842220682946\n",
            "14     \t [ 9.68766688  1.03665973  9.          0.58395515 19.          0.19787312]. \t  -0.6796636780038418 \t -0.46093842220682946\n",
            "15     \t [ 6.59943135  1.71178305 14.          0.69283152  8.          0.30459736]. \t  -0.635622313137239 \t -0.46093842220682946\n",
            "16     \t [5.99294447 7.0114806  5.         0.86352024 2.         0.47030879]. \t  -0.629371597372417 \t -0.46093842220682946\n",
            "17     \t [ 9.63630565  6.83547158  8.          0.6195521  13.          0.11712353]. \t  -0.6780822711639433 \t -0.46093842220682946\n",
            "18     \t [0.93465239 0.11560545 5.         0.52904926 2.         0.78190279]. \t  -0.5147104266754287 \t -0.46093842220682946\n",
            "19     \t [ 4.02953989  6.35914994 11.          0.73063178  8.          0.53793685]. \t  -0.6140059235484745 \t -0.46093842220682946\n",
            "20     \t [ 0.49442426  9.20422434 11.          0.70522852  1.          0.50168224]. \t  -0.62709850175482 \t -0.46093842220682946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.743572716730358"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snTrqE2RbWbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf8359b-511a-445c-b3d1-375f46759d9e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 13 \n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_exact_13 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_13 = dGPGO(surrogate_exact_13, Acquisition_new(util_exact), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_13 = exact_13.getResult()[0]\n",
        "params_exact_13['max_depth'] = int(params_exact_13['max_depth'])\n",
        "params_exact_13['min_child_weight'] = int(params_exact_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_exact_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_exact_13 = xgb.train(params_exact_13, dX_exact_train13)\n",
        "pred_exact_13 = model_exact_13.predict(dX_exact_test13)\n",
        "\n",
        "rmse_exact_13 = np.sqrt(mean_squared_error(pred_exact_13, y_test13))\n",
        "rmse_exact_13"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.77702411  2.3754122  11.          0.94649135 13.          0.7827256 ]. \t  -0.5099204187421568 \t -0.5099204187421568\n",
            "init   \t [ 7.51661514  6.07343344 11.          0.69402149 11.          0.13153287]. \t  -0.7086153631136594 \t -0.5099204187421568\n",
            "init   \t [ 2.98449471  0.58512492 10.          0.73579614 12.          0.33065195]. \t  -0.6372461781857162 \t -0.5099204187421568\n",
            "init   \t [ 3.47581215  0.0941277  11.          0.86143432  8.          0.58454932]. \t  -0.5702017504451442 \t -0.5099204187421568\n",
            "init   \t [ 4.70137857  6.24432527 10.          0.8149145  18.          0.10784416]. \t  -0.7101125715665313 \t -0.5099204187421568\n",
            "1      \t [1.1119361  5.43221306 6.         0.56899303 8.         0.32100319]. \t  -0.6493427493033831 \t -0.5099204187421568\n",
            "2      \t [5.39023698 3.80105709 8.         0.54170057 1.         0.56798884]. \t  -0.621866591058205 \t -0.5099204187421568\n",
            "3      \t [ 0.5185863   5.23876151 13.          0.63798348  5.          0.7914799 ]. \t  \u001b[92m-0.5090480969691882\u001b[0m \t -0.5090480969691882\n",
            "4      \t [ 9.65518672  0.13040633  6.          0.63296628 18.          0.44133279]. \t  -0.632314879589983 \t -0.5090480969691882\n",
            "5      \t [ 9.80722669  7.22571611  7.          0.5026908  19.          0.92519376]. \t  -0.5113851690128051 \t -0.5090480969691882\n",
            "6      \t [ 6.75965929  9.42320667 14.          0.75932127  4.          0.51195623]. \t  -0.6206028595311828 \t -0.5090480969691882\n",
            "7      \t [9.95671825 0.88335607 5.         0.76593799 7.         0.60049246]. \t  -0.5952873237754599 \t -0.5090480969691882\n",
            "8      \t [ 1.88898055  9.92199995 14.          0.53783103 12.          0.63359705]. \t  -0.5738907345204808 \t -0.5090480969691882\n",
            "9      \t [0.32121091 9.03384774 6.         0.79493663 1.         0.29330571]. \t  -0.6544818879601484 \t -0.5090480969691882\n",
            "10     \t [ 9.64211232  3.05396831 11.          0.80784352  5.          0.67910498]. \t  -0.5747201434527256 \t -0.5090480969691882\n",
            "11     \t [ 1.60018805  0.54211528  7.          0.80910607 18.          0.55232873]. \t  -0.6209454018308087 \t -0.5090480969691882\n",
            "12     \t [10.        10.        15.         1.        14.9316955  1.       ]. \t  \u001b[92m-0.454006170940959\u001b[0m \t -0.454006170940959\n",
            "13     \t [7.99022981 8.23715587 5.         0.79172469 5.         0.39852784]. \t  -0.6543343020941466 \t -0.454006170940959\n",
            "14     \t [ 8.15066897  1.98276445 13.          0.64083395 19.          0.16153982]. \t  -0.7124294948180114 \t -0.454006170940959\n",
            "15     \t [ 2.27251819  8.81728667  7.          0.92054476 14.          0.74899609]. \t  -0.5198071138714226 \t -0.454006170940959\n",
            "16     \t [ 5.96896727  4.39812948  6.          0.6032279  13.          0.97913959]. \t  -0.5111397995838916 \t -0.454006170940959\n",
            "17     \t [ 9.08002101  9.03807456  5.          0.92565737 13.          0.80618835]. \t  -0.5365673692460949 \t -0.454006170940959\n",
            "18     \t [ 5.1333583   3.10658752 14.          0.74854361  1.          0.92539511]. \t  -0.4793091157473782 \t -0.454006170940959\n",
            "19     \t [ 0.65009462  9.91088996 13.          0.92448902  2.          0.37838494]. \t  -0.6501362550952002 \t -0.454006170940959\n",
            "20     \t [ 2.24744249  1.78720106 13.          0.8729686  19.          0.90941823]. \t  -0.47112056719100737 \t -0.454006170940959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.586816132339627"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAuEsXYbtOnC",
        "outputId": "d5d36c81-a889-482a-d476-6a5fac67b3db"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 14 \n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_exact_14 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_14 = dGPGO(surrogate_exact_14, Acquisition_new(util_exact), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_14 = exact_14.getResult()[0]\n",
        "params_exact_14['max_depth'] = int(params_exact_14['max_depth'])\n",
        "params_exact_14['min_child_weight'] = int(params_exact_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_exact_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_exact_14 = xgb.train(params_exact_14, dX_exact_train14)\n",
        "pred_exact_14 = model_exact_14.predict(dX_exact_test14)\n",
        "\n",
        "rmse_exact_14 = np.sqrt(mean_squared_error(pred_exact_14, y_test14))\n",
        "rmse_exact_14"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.13943344  7.73165052 12.          0.6831412  11.          0.37876233]. \t  -0.558794499921046 \t -0.4448140077853998\n",
            "init   \t [ 9.57603739  5.13116712 14.          0.76959997 12.          0.71328228]. \t  -0.49979629433789113 \t -0.4448140077853998\n",
            "init   \t [5.34950319 2.47493539 5.         0.50293689 6.         0.29706373]. \t  -0.5741697988899073 \t -0.4448140077853998\n",
            "init   \t [ 2.94506579  3.45329697  8.          0.87620946 14.          0.9783044 ]. \t  -0.4448140077853998 \t -0.4448140077853998\n",
            "init   \t [ 1.11811929  1.73004086  5.          0.73745288 12.          0.20586008]. \t  -0.6214151152359092 \t -0.4448140077853998\n",
            "1      \t [ 6.50637223  2.67617722 14.          0.53562507  1.          0.16862152]. \t  -0.6294794339238933 \t -0.4448140077853998\n",
            "2      \t [ 9.97732733  0.9008687  13.          0.65397817 19.          0.96533011]. \t  -0.46274298441446626 \t -0.4448140077853998\n",
            "3      \t [ 0.28409124  4.13353348 13.          0.96339983  6.          0.90100709]. \t  \u001b[92m-0.42439805248956936\u001b[0m \t -0.42439805248956936\n",
            "4      \t [9.32373648 9.05676215 9.         0.53064322 3.         0.70657534]. \t  -0.5066999638361388 \t -0.42439805248956936\n",
            "5      \t [ 9.52454394  8.82757271  9.          0.90064956 19.          0.16022914]. \t  -0.6201852747998959 \t -0.42439805248956936\n",
            "6      \t [ 5.21920054  9.35580917 14.          0.81835368  4.          0.54800317]. \t  -0.4907240663685837 \t -0.42439805248956936\n",
            "7      \t [ 0.9687803   2.15143442 14.          0.51651811 18.          0.56051657]. \t  -0.5069463513331254 \t -0.42439805248956936\n",
            "8      \t [ 9.40430013  0.15700131  7.          0.99940272 12.          0.43771306]. \t  -0.50114980828739 \t -0.42439805248956936\n",
            "9      \t [0.38652312 9.54840602 9.         0.51100563 1.         0.36720837]. \t  -0.5713884526711975 \t -0.42439805248956936\n",
            "10     \t [ 2.08494663  9.70581169 14.          0.64307382  3.          0.10613693]. \t  -0.6314022013205511 \t -0.42439805248956936\n",
            "11     \t [ 1.55781918  0.46142569 10.          0.80240433 16.          0.22604037]. \t  -0.6223833972775541 \t -0.42439805248956936\n",
            "12     \t [ 8.32471637  8.64868248  5.          0.80719895 12.          0.64183859]. \t  -0.5064508572748014 \t -0.42439805248956936\n",
            "13     \t [ 1.40044045  8.51657075 10.          0.9376679  19.          0.10378975]. \t  -0.619787592651992 \t -0.42439805248956936\n",
            "14     \t [ 6.67907222  1.16181929 11.          0.57892751  8.          0.43787261]. \t  -0.5089510029010434 \t -0.42439805248956936\n",
            "15     \t [ 7.88588096  7.12943097 15.          1.         18.42182682  1.        ]. \t  -0.4295249225468953 \t -0.42439805248956936\n",
            "16     \t [3.08911491 9.05396381 5.         0.56731196 7.         0.30286489]. \t  -0.563723404842588 \t -0.42439805248956936\n",
            "17     \t [10.         10.         15.          1.          7.70471767  1.        ]. \t  \u001b[92m-0.42254397418764056\u001b[0m \t -0.42254397418764056\n",
            "18     \t [9.95902868 1.23916696 7.         0.60336262 2.         0.41962125]. \t  -0.5646831606854809 \t -0.42254397418764056\n",
            "19     \t [1.20508963 0.14448649 8.         0.50927505 2.         0.71549909]. \t  -0.48572060865355005 \t -0.42254397418764056\n",
            "20     \t [ 3.38602001  5.52665599  5.          0.6443898  19.          0.93037694]. \t  -0.4749460845017488 \t -0.42254397418764056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.567023503468326"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxvE7Irbbj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ff646a-a0b8-42b0-f708-0ee5319daea6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 15 \n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_exact_15 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_15 = dGPGO(surrogate_exact_15, Acquisition_new(util_exact), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_15 = exact_15.getResult()[0]\n",
        "params_exact_15['max_depth'] = int(params_exact_15['max_depth'])\n",
        "params_exact_15['min_child_weight'] = int(params_exact_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_exact_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_exact_15 = xgb.train(params_exact_15, dX_exact_train15)\n",
        "pred_exact_15 = model_exact_15.predict(dX_exact_test15)\n",
        "\n",
        "rmse_exact_15 = np.sqrt(mean_squared_error(pred_exact_15, y_test15))\n",
        "rmse_exact_15"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.48817697  1.78895925 12.          0.55549316  8.          0.93397854]. \t  -0.48943791400638287 \t -0.48943791400638287\n",
            "init   \t [ 0.24953032  8.22298097 12.          0.62494951 11.          0.12924598]. \t  -0.6992441679399787 \t -0.48943791400638287\n",
            "init   \t [ 5.02017228  5.50882771 11.          0.85295832 19.          0.13548008]. \t  -0.69945028775584 \t -0.48943791400638287\n",
            "init   \t [2.0023081  9.98543403 7.         0.6295772  2.         0.526127  ]. \t  -0.6264981748624211 \t -0.48943791400638287\n",
            "init   \t [ 5.09715306  9.45038417 11.          0.7388277  16.          0.22739973]. \t  -0.6964615643692806 \t -0.48943791400638287\n",
            "1      \t [ 0.29158961  4.9949242  12.          0.89124583  3.          0.67554049]. \t  -0.56524595824364 \t -0.48943791400638287\n",
            "2      \t [3.68214008 4.55748717 6.         0.60488381 8.         0.88973248]. \t  -0.494310985641296 \t -0.48943791400638287\n",
            "3      \t [9.75991344 6.15203198 6.         0.65490407 1.         0.73816291]. \t  -0.49903240523735964 \t -0.48943791400638287\n",
            "4      \t [ 0.04347405  0.16019908 11.          0.96902942  9.          0.40185268]. \t  -0.6876830293606538 \t -0.48943791400638287\n",
            "5      \t [ 8.02289731 10.         15.          1.          8.7668462   1.        ]. \t  \u001b[92m-0.4556272723840594\u001b[0m \t -0.4556272723840594\n",
            "6      \t [1.13261559 9.28349969 7.         0.98964629 7.         0.37032959]. \t  -0.6828004414762805 \t -0.4556272723840594\n",
            "7      \t [ 5.30770728  0.52044454  5.          0.75000665 15.          0.36204331]. \t  -0.683131978328503 \t -0.4556272723840594\n",
            "8      \t [ 7.76649344  1.163194   14.          0.89685392  1.          0.90591323]. \t  -0.46131653887574997 \t -0.4556272723840594\n",
            "9      \t [ 0.53269319  3.10786722 14.          0.59617872 16.          0.1338107 ]. \t  -0.700992505050063 \t -0.4556272723840594\n",
            "10     \t [ 0.40185461  0.03468168  9.          0.58775784 15.          0.13532287]. \t  -0.6983586430175102 \t -0.4556272723840594\n",
            "11     \t [ 2.97928851  7.73905112  6.          0.87870551 18.          0.36796416]. \t  -0.6830838005681892 \t -0.4556272723840594\n",
            "12     \t [3.02150875 0.49910736 5.         0.91633249 1.         0.11262301]. \t  -0.6990999780460975 \t -0.4556272723840594\n",
            "13     \t [ 9.52301108  9.56759639  5.          0.87397797 11.          0.23091707]. \t  -0.6978364801116691 \t -0.4556272723840594\n",
            "14     \t [ 8.83554548  9.97893931 14.          0.88835544  1.          0.8298738 ]. \t  -0.47714205578923014 \t -0.4556272723840594\n",
            "15     \t [ 7.70971055  5.9598343  10.          0.83735899 18.          0.36826156]. \t  -0.6853727999168309 \t -0.4556272723840594\n",
            "16     \t [10. 10. 15.  1. 20.  1.]. \t  -0.4662835424155933 \t -0.4556272723840594\n",
            "17     \t [ 8.79093169  0.86787487 14.          0.80222436 14.          0.17525565]. \t  -0.6977535247740003 \t -0.4556272723840594\n",
            "18     \t [ 3.51479622  9.74194901 13.          0.90442852  5.          0.62459375]. \t  -0.5589827061205567 \t -0.4556272723840594\n",
            "19     \t [ 5.64647885  9.07152022 10.          0.83229506 10.          0.27661589]. \t  -0.6978237750995275 \t -0.4556272723840594\n",
            "20     \t [ 3.70936727  2.056093   14.          0.59327467  6.          0.45934375]. \t  -0.6241376584484061 \t -0.4556272723840594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.564641872479156"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TaP6RoGuiNT",
        "outputId": "8d38ab80-216d-4e63-fe3f-94f500511e1e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 16 \n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_exact_16 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_16 = dGPGO(surrogate_exact_16, Acquisition_new(util_exact), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_16 = exact_16.getResult()[0]\n",
        "params_exact_16['max_depth'] = int(params_exact_16['max_depth'])\n",
        "params_exact_16['min_child_weight'] = int(params_exact_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_exact_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_exact_16 = xgb.train(params_exact_16, dX_exact_train16)\n",
        "pred_exact_16 = model_exact_16.predict(dX_exact_test16)\n",
        "\n",
        "rmse_exact_16 = np.sqrt(mean_squared_error(pred_exact_16, y_test16))\n",
        "rmse_exact_16"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.23291079 5.23163341 6.         0.65430839 5.         0.30077285]. \t  -0.6357813258069683 \t -0.6345701590947206\n",
            "init   \t [6.88726162 1.63731425 7.         0.97050543 2.         0.25392012]. \t  -0.7029752724132097 \t -0.6345701590947206\n",
            "init   \t [ 5.94328983  5.6393473   5.          0.67602695 19.          0.42538144]. \t  -0.6345701590947206 \t -0.6345701590947206\n",
            "init   \t [ 0.88741148  3.08148142 14.          0.56043938  9.          0.27515386]. \t  -0.7076230970293895 \t -0.6345701590947206\n",
            "init   \t [ 2.74631586  1.30996118 11.          0.52160786  8.          0.27956463]. \t  -0.7061563820165734 \t -0.6345701590947206\n",
            "1      \t [ 7.8937256   1.5972923  14.          0.61610774 17.          0.78739284]. \t  \u001b[92m-0.5498317738591506\u001b[0m \t -0.5498317738591506\n",
            "2      \t [ 9.01655783  8.21383177  9.          0.60772965 10.          0.9401803 ]. \t  \u001b[92m-0.4830822737254163\u001b[0m \t -0.4830822737254163\n",
            "3      \t [ 4.35132073  9.89698316 12.          0.94137984 16.          0.57741056]. \t  -0.5389659972108385 \t -0.4830822737254163\n",
            "4      \t [ 3.38377852  9.31285251 11.          0.88244942  1.          0.67627774]. \t  -0.5404220213450739 \t -0.4830822737254163\n",
            "5      \t [ 9.63904847  1.13624975 14.          0.67706869  2.          0.95236673]. \t  \u001b[92m-0.4641319096590556\u001b[0m \t -0.4641319096590556\n",
            "6      \t [ 0.19317903  0.6596816   6.          0.86233301 15.          0.41906313]. \t  -0.6308098904949025 \t -0.4641319096590556\n",
            "7      \t [ 3.86147645  9.68905939  5.          0.65198689 12.          0.68426055]. \t  -0.5557826482727706 \t -0.4641319096590556\n",
            "8      \t [ 1.22130867  0.64008351 12.          0.59515137 19.          0.65193522]. \t  -0.5486769508942775 \t -0.4641319096590556\n",
            "9      \t [8.31596139 8.08775071 5.         0.5698323  1.         0.11769544]. \t  -0.708927036050009 \t -0.4641319096590556\n",
            "10     \t [ 9.39421065  1.15238895 10.          0.86257926 10.          0.82976642]. \t  -0.5380808068994449 \t -0.4641319096590556\n",
            "11     \t [ 8.63952355  8.87914214 14.          0.57943185  3.          0.83429859]. \t  -0.5495434658820855 \t -0.4641319096590556\n",
            "12     \t [ 2.23523952  3.62733473 12.          0.51556206  2.          0.2036675 ]. \t  -0.7124687886931153 \t -0.4641319096590556\n",
            "13     \t [10.         10.         15.          1.         11.03041881  1.        ]. \t  \u001b[92m-0.4538384944380411\u001b[0m \t -0.4538384944380411\n",
            "14     \t [ 5.94853197  4.92024227  5.          0.75769119 13.          0.52349427]. \t  -0.5762899773358159 \t -0.4538384944380411\n",
            "15     \t [10. 10. 15.  1. 20.  1.]. \t  -0.46362391476067477 \t -0.4538384944380411\n",
            "16     \t [ 9.28336661  6.57187836 11.          0.54297007 17.          0.56210295]. \t  -0.5785906968518683 \t -0.4538384944380411\n",
            "17     \t [ 0.03255143  5.62206767 10.          0.695628   16.          0.90161277]. \t  -0.4734497149203408 \t -0.4538384944380411\n",
            "18     \t [ 3.32190637  9.36212837 13.          0.54407927 10.          0.12468989]. \t  -0.7062938512175606 \t -0.4538384944380411\n",
            "19     \t [ 9.76289081  0.88879743  6.          0.50453768 15.          0.73005629]. \t  -0.5522738715689526 \t -0.4538384944380411\n",
            "20     \t [ 7.43573851  4.55544477 14.          0.86207767  8.          0.45183313]. \t  -0.5660380755246425 \t -0.4538384944380411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.568091354119152"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiOaMUmgulbx",
        "outputId": "6ae3d26d-b3c1-4c65-86b4-9641696fe243"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 17 \n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_exact_17 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_17 = dGPGO(surrogate_exact_17, Acquisition_new(util_exact), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_17 = exact_17.getResult()[0]\n",
        "params_exact_17['max_depth'] = int(params_exact_17['max_depth'])\n",
        "params_exact_17['min_child_weight'] = int(params_exact_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_exact_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_exact_17 = xgb.train(params_exact_17, dX_exact_train17)\n",
        "pred_exact_17 = model_exact_17.predict(dX_exact_test17)\n",
        "\n",
        "rmse_exact_17 = np.sqrt(mean_squared_error(pred_exact_17, y_test17))\n",
        "rmse_exact_17"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.94665003  5.30586756 11.          0.94443241 14.          0.80828691]. \t  -0.48092361225642916 \t -0.48092361225642916\n",
            "init   \t [ 6.56333522  6.37520896 12.          0.81487881 18.          0.42203224]. \t  -0.634455605137701 \t -0.48092361225642916\n",
            "init   \t [ 9.45683187  0.6004468  11.          0.5171566  10.          0.53881211]. \t  -0.6046684392629649 \t -0.48092361225642916\n",
            "init   \t [2.72705857 1.19063434 6.         0.74176431 6.         0.10101151]. \t  -0.6562801618178493 \t -0.48092361225642916\n",
            "init   \t [ 4.77631812  5.24671297 13.          0.66254476 19.          0.36708086]. \t  -0.6415104035419145 \t -0.48092361225642916\n",
            "1      \t [ 0.65702322  5.79284078 13.          0.75136902  1.          0.30306068]. \t  -0.6424064809093688 \t -0.48092361225642916\n",
            "2      \t [ 6.93446178  8.68032298 13.          0.78195789  7.          0.91906958]. \t  \u001b[92m-0.47529593119031155\u001b[0m \t -0.47529593119031155\n",
            "3      \t [9.72843652 3.88893279 9.         0.6901555  1.         0.31608219]. \t  -0.6407338037654788 \t -0.47529593119031155\n",
            "4      \t [ 9.65057736  8.52725784  5.          0.68420234 13.          0.40008732]. \t  -0.6446152251367371 \t -0.47529593119031155\n",
            "5      \t [ 4.97204887  2.40072226  5.          0.54268748 19.          0.30995407]. \t  -0.6473150344064219 \t -0.47529593119031155\n",
            "6      \t [0.12174033 8.73496008 5.         0.89827646 5.         0.85354798]. \t  -0.4993728869548043 \t -0.47529593119031155\n",
            "7      \t [ 2.91443079  0.16723755 13.          0.598201    6.          0.91729605]. \t  -0.48363736808522184 \t -0.47529593119031155\n",
            "8      \t [7.20615247 9.36901627 6.         0.85465034 7.         0.6878262 ]. \t  -0.5223074457605629 \t -0.47529593119031155\n",
            "9      \t [ 9.02586164  0.59354638 10.          0.86038693 18.          0.90794111]. \t  -0.48184301693459747 \t -0.47529593119031155\n",
            "10     \t [ 1.66641474  7.47633023  5.          0.68203645 17.          0.15512069]. \t  -0.6612773320145335 \t -0.47529593119031155\n",
            "11     \t [ 0.12410542  7.60180472 13.          0.97425003  8.          0.76245421]. \t  \u001b[92m-0.47195842108145214\u001b[0m \t -0.47195842108145214\n",
            "12     \t [ 8.39240649 10.         15.          1.         13.24506875  1.        ]. \t  \u001b[92m-0.47173619960497104\u001b[0m \t -0.47173619960497104\n",
            "13     \t [9.59190042 1.26233772 5.         0.55398722 6.         0.46259714]. \t  -0.6127976173878007 \t -0.47173619960497104\n",
            "14     \t [ 9.06002681  9.03779351 14.          0.60614154  1.          0.16421461]. \t  -0.6627557424336108 \t -0.47173619960497104\n",
            "15     \t [ 0.59590325  0.30071901  8.          0.9148403  16.          0.62633871]. \t  -0.507330476870976 \t -0.47173619960497104\n",
            "16     \t [ 8.69529605  0.27226865 14.          0.90636352  1.          0.39638219]. \t  -0.6352506435607296 \t -0.47173619960497104\n",
            "17     \t [ 5.5997101   2.80089542 11.          0.91397635  4.          0.58148876]. \t  -0.49267256921662134 \t -0.47173619960497104\n",
            "18     \t [10. 10. 15.  1. 20.  1.]. \t  -0.478716870694923 \t -0.47173619960497104\n",
            "19     \t [ 2.76368647  8.96162764  6.          0.57935111 11.          0.22903211]. \t  -0.6599396664254092 \t -0.47173619960497104\n",
            "20     \t [ 7.9822505   0.1930237   5.          0.88184327 12.          0.30139498]. \t  -0.6375373096353469 \t -0.47173619960497104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.661205621669708"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H4MWSXFcZjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e29c4d-e42b-42ab-c182-bffb03eabc87"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 18 \n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_exact_18 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_18 = dGPGO(surrogate_exact_18, Acquisition_new(util_exact), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_18 = exact_18.getResult()[0]\n",
        "params_exact_18['max_depth'] = int(params_exact_18['max_depth'])\n",
        "params_exact_18['min_child_weight'] = int(params_exact_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_exact_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_exact_18 = xgb.train(params_exact_18, dX_exact_train18)\n",
        "pred_exact_18 = model_exact_18.predict(dX_exact_test18)\n",
        "\n",
        "rmse_exact_18 = np.sqrt(mean_squared_error(pred_exact_18, y_test18))\n",
        "rmse_exact_18"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.50374242 5.05453374 6.         0.59092011 3.         0.28357516]. \t  -0.6408812889056655 \t -0.45563901799042095\n",
            "init   \t [0.11506734 4.26891483 9.         0.81785956 5.         0.63489043]. \t  -0.5015618704267137 \t -0.45563901799042095\n",
            "init   \t [ 2.8861259   6.35547834 11.          0.64267955 14.          0.27877092]. \t  -0.6438813467576416 \t -0.45563901799042095\n",
            "init   \t [6.57189031 6.99655629 8.         0.63235896 4.         0.52894035]. \t  -0.5434165841486521 \t -0.45563901799042095\n",
            "init   \t [ 6.66600348  2.11312037 14.          0.74363461  4.          0.73174558]. \t  -0.45563901799042095 \t -0.45563901799042095\n",
            "1      \t [ 8.67093232  0.11649132  5.          0.92962202 15.          0.53672863]. \t  -0.5446894741691042 \t -0.45563901799042095\n",
            "2      \t [ 8.43851229  2.41114508 13.          0.75771586 19.          0.86905071]. \t  \u001b[92m-0.4388122674991237\u001b[0m \t -0.4388122674991237\n",
            "3      \t [ 9.44281001  9.01534322  7.          0.99142432 16.          0.37631199]. \t  -0.6085057891370991 \t -0.4388122674991237\n",
            "4      \t [10.         10.         13.95119006  1.          8.95119006  1.        ]. \t  \u001b[92m-0.4266957287400768\u001b[0m \t -0.4266957287400768\n",
            "5      \t [ 1.97643014  8.37982471  5.          0.63246176 17.          0.45403539]. \t  -0.5418357136590688 \t -0.4266957287400768\n",
            "6      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.42588161300140354\u001b[0m \t -0.42588161300140354\n",
            "7      \t [ 0.02961082  2.38062589  5.          0.60457904 13.          0.70655364]. \t  -0.5083627676422857 \t -0.42588161300140354\n",
            "8      \t [ 6.63763513  1.21594617 14.          0.92753462 12.          0.25002233]. \t  -0.6447227465520562 \t -0.42588161300140354\n",
            "9      \t [ 4.32027405  9.96851926 14.          0.70719552  1.          0.80495431]. \t  -0.44551025370789715 \t -0.42588161300140354\n",
            "10     \t [ 0.89534385  1.39878838 10.          0.85395723 19.          0.70708679]. \t  -0.49227382562309224 \t -0.42588161300140354\n",
            "11     \t [ 2.69640701  8.7583803   5.          0.72267254 10.          0.8528077 ]. \t  -0.4752181963523114 \t -0.42588161300140354\n",
            "12     \t [9.0137211  3.37251846 8.         0.99529789 9.         0.14915462]. \t  -0.6435423820932967 \t -0.42588161300140354\n",
            "13     \t [0.58655573 8.89860432 5.         0.62593909 1.         0.46819727]. \t  -0.5449038923555584 \t -0.42588161300140354\n",
            "14     \t [ 1.64238572  0.62580745 11.          0.79203735 11.          0.21517242]. \t  -0.643614453864466 \t -0.42588161300140354\n",
            "15     \t [ 2.72240994  9.88599464 12.          0.79869012  8.          0.52915179]. \t  -0.5404306975471715 \t -0.42588161300140354\n",
            "16     \t [2.5556895  0.91787864 5.         0.57129972 6.         0.40466558]. \t  -0.6123895871923584 \t -0.42588161300140354\n",
            "17     \t [ 2.88988266  8.62215245 13.          0.75211659 19.          0.58952824]. \t  -0.4917333227604216 \t -0.42588161300140354\n",
            "18     \t [ 7.16872281  3.36587781  7.          0.60098211 19.          0.72044147]. \t  -0.46380371702866086 \t -0.42588161300140354\n",
            "19     \t [ 7.53809313  9.75652362 13.          0.61919632 14.          0.91992568]. \t  -0.4388210663948877 \t -0.42588161300140354\n",
            "20     \t [ 1.47346798  3.42280397 13.          0.87173303  1.          0.71719115]. \t  -0.4411780295132647 \t -0.42588161300140354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.787850327309379"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-zaPbk2uuzH",
        "outputId": "252747c3-9ad8-45cf-b7e4-8b81b816fe08"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 19 \n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_exact_19 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_19 = dGPGO(surrogate_exact_19, Acquisition_new(util_exact), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_19 = exact_19.getResult()[0]\n",
        "params_exact_19['max_depth'] = int(params_exact_19['max_depth'])\n",
        "params_exact_19['min_child_weight'] = int(params_exact_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_exact_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_exact_19 = xgb.train(params_exact_19, dX_exact_train19)\n",
        "pred_exact_19 = model_exact_19.predict(dX_exact_test19)\n",
        "\n",
        "rmse_exact_19 = np.sqrt(mean_squared_error(pred_exact_19, y_test19))\n",
        "rmse_exact_19"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.97533602  7.61249717 13.          0.85765469 11.          0.39830191]. \t  -0.5851081958447377 \t -0.4870287725699859\n",
            "init   \t [ 0.82999565  6.71977081  6.          0.50407413 19.          0.67209466]. \t  -0.5245729886945251 \t -0.4870287725699859\n",
            "init   \t [ 2.15923256  5.49027432 12.          0.52588686 10.          0.20235326]. \t  -0.677999989539271 \t -0.4870287725699859\n",
            "init   \t [4.99659267 1.52108422 6.         0.73481085 4.         0.71949465]. \t  -0.4940235599803803 \t -0.4870287725699859\n",
            "init   \t [ 3.72927156  9.46160045  5.          0.80554614 18.          0.97708466]. \t  -0.4870287725699859 \t -0.4870287725699859\n",
            "1      \t [ 8.33060043  1.42030563  8.          0.92863724 14.          0.78606141]. \t  \u001b[92m-0.48699206254226385\u001b[0m \t -0.48699206254226385\n",
            "2      \t [ 7.89674065  9.31460122 11.          0.96158687  5.          0.67754853]. \t  -0.49785049104905016 \t -0.48699206254226385\n",
            "3      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.45072062300002613\u001b[0m \t -0.45072062300002613\n",
            "4      \t [ 2.48547521  0.16821047 14.          0.50102007  1.          0.10104193]. \t  -0.6865863454987299 \t -0.45072062300002613\n",
            "5      \t [0.25768796 8.33414072 6.         0.99948369 4.         0.66680619]. \t  -0.5074502274890225 \t -0.45072062300002613\n",
            "6      \t [ 1.10650842  9.77537077 14.          0.83431584  1.          0.5884296 ]. \t  -0.5035756891739214 \t -0.45072062300002613\n",
            "7      \t [ 9.97057985  1.83320304 14.          0.52757417  6.          0.12389204]. \t  -0.6775116303533313 \t -0.45072062300002613\n",
            "8      \t [ 8.56732982  8.91986104 14.          0.62397749 13.          0.90657477]. \t  -0.4704331127149441 \t -0.45072062300002613\n",
            "9      \t [7.08488952 8.05904216 5.         0.93088713 1.         0.30662699]. \t  -0.5910777842699757 \t -0.45072062300002613\n",
            "10     \t [ 0.60329519  1.30823827 14.          0.81928223 17.          0.54931634]. \t  -0.5614358389874498 \t -0.45072062300002613\n",
            "11     \t [ 9.47453996  8.80927889  6.          0.60975678 10.          0.22761105]. \t  -0.6786997262456852 \t -0.45072062300002613\n",
            "12     \t [ 7.79929955  2.13050742 14.          0.83877886 17.          0.7783859 ]. \t  -0.48184598472537166 \t -0.45072062300002613\n",
            "13     \t [ 2.37824388  4.47272578  5.          0.97943263 10.          0.125395  ]. \t  -0.6793722154876207 \t -0.45072062300002613\n",
            "14     \t [ 0.11534005  0.08986117  8.          0.95341643 19.          0.48014014]. \t  -0.5639400804347694 \t -0.45072062300002613\n",
            "15     \t [ 2.45146849  8.32898472 14.          0.63227537 18.          0.3341326 ]. \t  -0.5774093941622171 \t -0.45072062300002613\n",
            "16     \t [ 8.69943878  7.36890408  7.          0.86891549 19.          0.36276071]. \t  -0.5930096394832391 \t -0.45072062300002613\n",
            "17     \t [ 1.9120179   0.52119504 12.          0.71043482  7.          0.33341372]. \t  -0.5854472398210584 \t -0.45072062300002613\n",
            "18     \t [9.32488714 1.90315632 6.         0.7513195  8.         0.80108048]. \t  -0.49835441465198543 \t -0.45072062300002613\n",
            "19     \t [ 1.50133196  1.19414112  9.          0.60450761 13.          0.34743418]. \t  -0.5811044554703819 \t -0.45072062300002613\n",
            "20     \t [ 3.82670531  7.06245628  8.          0.7747773  14.          0.11384763]. \t  -0.67893464439286 \t -0.45072062300002613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.72073900839424"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvkuHKlQuxRy",
        "outputId": "7631315b-fa0c-4d41-bbf7-e7dff74fb398"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'exact' Acquisition Function run number = 20 \n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_exact_20 = dGaussianProcess(cov_func, optimize=opt)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "exact_20 = dGPGO(surrogate_exact_20, Acquisition_new(util_exact), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
        "exact_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_exact_20 = exact_20.getResult()[0]\n",
        "params_exact_20['max_depth'] = int(params_exact_20['max_depth'])\n",
        "params_exact_20['min_child_weight'] = int(params_exact_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_exact_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_exact_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_exact_20 = xgb.train(params_exact_20, dX_exact_train20)\n",
        "pred_exact_20 = model_exact_20.predict(dX_exact_test20)\n",
        "\n",
        "rmse_exact_20 = np.sqrt(mean_squared_error(pred_exact_20, y_test20))\n",
        "rmse_exact_20"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.88130801  8.97713728 14.          0.81074445  8.          0.95540649]. \t  -0.4485352768858121 \t -0.4485352768858121\n",
            "init   \t [6.72865655 0.41173329 8.         0.6361582  7.         0.76174061]. \t  -0.47208091450542966 \t -0.4485352768858121\n",
            "init   \t [ 4.77387703  8.66202323 10.          0.51833215  7.          0.10123387]. \t  -0.7316473600840852 \t -0.4485352768858121\n",
            "init   \t [ 5.75489985  4.74524381  8.          0.78084343 15.          0.26643049]. \t  -0.7314226542252507 \t -0.4485352768858121\n",
            "init   \t [ 4.53444     4.47342833  8.          0.91974896 18.          0.35997552]. \t  -0.6494230116583573 \t -0.4485352768858121\n",
            "1      \t [ 7.96566073  7.15509535  7.          0.79906691 11.          0.34132075]. \t  -0.6502222800629637 \t -0.4485352768858121\n",
            "2      \t [ 1.98667885  1.35773177 13.          0.57199118  2.          0.39498908]. \t  -0.6661970064789862 \t -0.4485352768858121\n",
            "3      \t [ 3.00704909  2.42524876 14.          0.95062509 15.          0.83595087]. \t  -0.4577111911042998 \t -0.4485352768858121\n",
            "4      \t [ 8.94683774  7.07743462 15.          1.         17.10232747  1.        ]. \t  \u001b[92m-0.43833021580133885\u001b[0m \t -0.43833021580133885\n",
            "5      \t [8.0846212  5.99993376 6.         0.83941375 1.         0.46362124]. \t  -0.5681524533474447 \t -0.43833021580133885\n",
            "6      \t [0.61316554 1.36115087 5.         0.87944956 1.         0.21740227]. \t  -0.7332228664111685 \t -0.43833021580133885\n",
            "7      \t [ 0.72788527  2.26655356 10.          0.97273032 15.          0.85843758]. \t  -0.45084600130879665 \t -0.43833021580133885\n",
            "8      \t [4.62702633 9.6876243  5.         0.9923406  1.         0.38132515]. \t  -0.6522172440468724 \t -0.43833021580133885\n",
            "9      \t [ 1.44692101  9.96202174 12.          0.76092884 18.          0.10523817]. \t  -0.731303133317551 \t -0.43833021580133885\n",
            "10     \t [ 1.01814405  9.81807131 14.          0.52908047  1.          0.89345697]. \t  -0.4570018264326601 \t -0.43833021580133885\n",
            "11     \t [ 0.81648933  6.00906202 12.          0.6106182   6.          0.17068603]. \t  -0.7370223446090218 \t -0.43833021580133885\n",
            "12     \t [ 9.62878188  0.47045758 13.          0.73408624  1.          0.43054124]. \t  -0.5746597113890792 \t -0.43833021580133885\n",
            "13     \t [ 8.98143836  8.7693897  12.          0.89468403 12.          0.66552576]. \t  -0.49793969514395464 \t -0.43833021580133885\n",
            "14     \t [ 9.55715055  0.18843871 11.          0.63478984 17.          0.93511154]. \t  -0.4708818143147438 \t -0.43833021580133885\n",
            "15     \t [ 5.37444991  0.3056877   9.          0.99777328 15.          0.5903115 ]. \t  -0.5008636074552522 \t -0.43833021580133885\n",
            "16     \t [ 8.81579675  8.3667984  12.          0.69895873  2.          0.28897175]. \t  -0.656636562688776 \t -0.43833021580133885\n",
            "17     \t [ 8.2503326   1.61920179 14.          0.80641713 11.          0.5764476 ]. \t  -0.4970739671390835 \t -0.43833021580133885\n",
            "18     \t [ 4.4224035   9.83430007  5.          0.89681877 17.          0.40799802]. \t  -0.6495318911805994 \t -0.43833021580133885\n",
            "19     \t [ 9.64898386  6.20494423  6.          0.8469719  19.          0.57533849]. \t  -0.5209730469573338 \t -0.43833021580133885\n",
            "20     \t [ 1.39513001  1.37149056  5.          0.56200696 12.          0.29219492]. \t  -0.6531158385825743 \t -0.43833021580133885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.532516410062584"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFKuwvS3uzrs",
        "outputId": "d9878202-fbd1-46f6-d051-e4cff7c9704b"
      },
      "source": [
        "end_exact = time.time()\n",
        "end_exact\n",
        "\n",
        "time_exact = end_exact - start_exact\n",
        "time_exact"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1606.0069267749786"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2FlhY4vHUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9abe65-ebe5-4056-e253-0767288a4d9b"
      },
      "source": [
        "rmse_approx = [rmse_approx_1,\n",
        "rmse_approx_2,\n",
        "rmse_approx_3,\n",
        "rmse_approx_4,\n",
        "rmse_approx_5,\n",
        "rmse_approx_6,\n",
        "rmse_approx_7,\n",
        "rmse_approx_8,\n",
        "rmse_approx_9,\n",
        "rmse_approx_10,\n",
        "rmse_approx_11,\n",
        "rmse_approx_12,\n",
        "rmse_approx_13,\n",
        "rmse_approx_14,\n",
        "rmse_approx_15,\n",
        "rmse_approx_16,\n",
        "rmse_approx_17,\n",
        "rmse_approx_18,\n",
        "rmse_approx_19,\n",
        "rmse_approx_20]\n",
        "\n",
        "np.mean(rmse_approx)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.69288976992484"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ53FsWXu3J1",
        "outputId": "c9d38632-2dd7-477f-bc7b-fc5dbc096538"
      },
      "source": [
        "rmse_exact = [rmse_exact_1,\n",
        "rmse_exact_2,\n",
        "rmse_exact_3,\n",
        "rmse_exact_4,\n",
        "rmse_exact_5,\n",
        "rmse_exact_6,\n",
        "rmse_exact_7,\n",
        "rmse_exact_8,\n",
        "rmse_exact_9,\n",
        "rmse_exact_10,\n",
        "rmse_exact_11,\n",
        "rmse_exact_12,\n",
        "rmse_exact_13,\n",
        "rmse_exact_14,\n",
        "rmse_exact_15,\n",
        "rmse_exact_16,\n",
        "rmse_exact_17,\n",
        "rmse_exact_18,\n",
        "rmse_exact_19,\n",
        "rmse_exact_20]\n",
        "\n",
        "np.mean(rmse_exact)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.622184558899018"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9FOyoH8u5Wx",
        "outputId": "65ec187b-4f52-4c03-d079-bc63f2ac9e2b"
      },
      "source": [
        "min_rmse_approx = min_max_array(rmse_approx)\n",
        "min_rmse_approx, len(min_rmse_approx)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.667222167746296,\n",
              "  4.600295852493361,\n",
              "  4.600295852493361,\n",
              "  4.600295852493361,\n",
              "  4.600295852493361,\n",
              "  4.600295852493361,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.499924380514474,\n",
              "  4.454763173494824],\n",
              " 20)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXOpKHcvO15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d038beba-6018-4e66-e104-5042332c769a"
      },
      "source": [
        "min_rmse_exact = min_max_array(rmse_exact)\n",
        "min_rmse_exact, len(min_rmse_exact)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.607787340872903,\n",
              "  4.556046943199982,\n",
              "  4.556046943199982,\n",
              "  4.556046943199982,\n",
              "  4.556046943199982,\n",
              "  4.556046943199982,\n",
              "  4.499924380514474,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443,\n",
              "  4.487971093090443],\n",
              " 20)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxo85-HEvRPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "7f2cd5fc-9c4d-47e1-b930-4b4fbaab3117"
      },
      "source": [
        "### Visualise!\n",
        "\n",
        "title = obj_func\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(min_rmse_approx, color = 'Yellow', label='RMSE: GP EI ApproxGrad')\n",
        "plt.plot(min_rmse_exact, color = 'Green', label='RMSE: GP EI ExactGrad', ls='--')# r'($\\nu$' ' = {})'.format(df))\n",
        "\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\n",
        "plt.xlabel('Experiment(s)', weight = 'bold', family = 'Arial') # x-axis label\n",
        "plt.ylabel('RMSE (US Dollars $)', weight = 'bold', family = 'Arial') # y-axis label\n",
        "plt.legend(loc=0) # add plot legend\n",
        "\n",
        "### Make the x-ticks integers, not floats:\n",
        "count = len(min_rmse_approx)\n",
        "plt.xticks(np.arange(count), np.arange(1, count + 1))\n",
        "plt.grid(b=None)\n",
        "plt.show() #visualize!\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwyO7_iZvT7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb07ee6-039f-4455-8333-b6ece93272f9"
      },
      "source": [
        "time_approx, time_exact\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(791.539687871933, 1606.0069267749786)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHLA-0DnVXxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab85774-c3ce-45bd-aed3-8b2f158256c0"
      },
      "source": [
        "min(min_rmse_exact), min(min_rmse_approx)\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.487971093090443, 4.454763173494824)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iUNBRy3W0GY"
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    }
  ]
}